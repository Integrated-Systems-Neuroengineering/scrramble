{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d8761571",
   "metadata": {},
   "source": [
    "# Baseline Model with Partial Sums\n",
    "- Construct a baseline model with partial sums to compare against ScRRAMBLe.\n",
    "- Refer to older scrips.\n",
    "\n",
    "_Created on: 08/06/2025_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "11707e92",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 19:02:29.423407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754532149.435797 1922137 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754532149.439827 1922137 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1754532149.450166 1922137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754532149.450178 1922137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754532149.450180 1922137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1754532149.450182 1922137 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import math\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax\n",
    "from flax import nnx\n",
    "from flax.nnx.nn import initializers\n",
    "from typing import Callable\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from functools import partial\n",
    "from tqdm import tqdm\n",
    "from datetime import date\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import seaborn as sns\n",
    "# mpl.use('Agg')  # Use a non-interactive backend for matplotlib.\n",
    "\n",
    "from models import ScRRAMBLeCapsLayer\n",
    "\n",
    "from utils.activation_functions import quantized_relu_ste, squash, qrelu\n",
    "# from utils.loss_functions import margin_loss\n",
    "from utils import ScRRAMBLe_routing, intercore_connectivity, load_and_augment_mnist\n",
    "\n",
    "\n",
    "import tensorflow_datasets as tfds  # TFDS to download MNIST.\n",
    "import tensorflow as tf  # TensorFlow / `tf.data` operations.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5b3a2ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PartialSumsLayer(nnx.Module):\n",
    "    \"\"\"\n",
    "    Module for single partial sums layer.\n",
    "    Takes in size of a feedforwardf layer.\n",
    "    Initializes appropriate number of cores as trainable parameters.\n",
    "    Accumulates partial sums across the cores\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_size: int,\n",
    "                 out_size: int,\n",
    "                 rngs: nnx.Rngs,\n",
    "                 activation_function: Callable,\n",
    "                 columns_per_core: int = 256\n",
    "                 ):\n",
    "        \n",
    "        self.in_size = in_size\n",
    "        self.out_size = out_size\n",
    "        self.activation_function = activation_function\n",
    "        self.columns_per_core = columns_per_core\n",
    "        self.rngs = rngs\n",
    "\n",
    "        # number of cores requires\n",
    "        self.in_blocks = math.ceil(in_size / columns_per_core)\n",
    "        self.out_blocks = math.ceil(out_size / columns_per_core)\n",
    "\n",
    "        # initialize parameters\n",
    "        initializer = initializers.glorot_normal()\n",
    "        self.W = nnx.Param(\n",
    "            initializer(self.rngs.params(), (self.out_blocks, self.in_blocks, self.columns_per_core, self.columns_per_core))\n",
    "        )\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        \"\"\"\n",
    "        Forward pass. No batch dimension. use vmap\n",
    "        Assume the x is flat\n",
    "        \"\"\"\n",
    "\n",
    "        # # pad the input if in_size is not a multiple of 256\n",
    "        # x_padded = jnp.pad(x, pad_width=((0, 0), (0, self.columns_per_core * self.in_blocks - self.in_size)))\n",
    "\n",
    "        x_reshape = x.reshape(self.in_blocks, self.columns_per_core)\n",
    "\n",
    "        # compute the partial sums\n",
    "        y = jnp.einsum('ijkl,jl->ik', self.W, x_reshape)\n",
    "\n",
    "        # apply activation function\n",
    "        y = jax.vmap(self.activation_function, in_axes=(0,))(y)\n",
    "\n",
    "        return y\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "42ed2787",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (10, 2, 256)\n",
      "Output: [0.         0.7686344  0.         0.         0.33890092 0.12463952\n",
      " 0.         0.03248509 0.26785067 0.18344072]\n"
     ]
    }
   ],
   "source": [
    "# testing forward pass\n",
    "rngs = nnx.Rngs(params=0, activations=1, permute=5, default=345)\n",
    "x_test = jax.random.normal(rngs.default(), (10, 1024))\n",
    "activation_function = nnx.relu\n",
    "# activation_function = partial(qrelu, bits=4)\n",
    "test_layer = PartialSumsLayer(\n",
    "    in_size=x_test.shape[-1],\n",
    "    out_size=512,\n",
    "    rngs=rngs,\n",
    "    activation_function=activation_function,\n",
    ")\n",
    "\n",
    "# nnx.display(test_layer)\n",
    "y_test = jax.vmap(test_layer, in_axes=(0,))(x_test)\n",
    "print(f\"Output shape: {y_test.shape}\")\n",
    "print(f\"Output: {y_test[0, 0, :10]}\")\n",
    "\n",
    "del test_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9af24f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a network with partial sums\n",
    "class PartialSumsNetwork(nnx.Module):\n",
    "    \"\"\"\n",
    "    Network with partial sums layers.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 layer_sizes: list,\n",
    "                 rngs: nnx.Rngs,\n",
    "                 activation_function: Callable,\n",
    "                 columns_per_core: int = 256\n",
    "                ):\n",
    "        \n",
    "        self.layer_sizes = layer_sizes\n",
    "        self.activation_function = activation_function\n",
    "        self.columns_per_core = columns_per_core\n",
    "        self.rngs = rngs\n",
    "\n",
    "        # initialize the layers\n",
    "        self.layers = [\n",
    "            PartialSumsLayer(\n",
    "                in_size=i,\n",
    "                out_size=o,\n",
    "                rngs=rngs,\n",
    "                activation_function=activation_function,\n",
    "                columns_per_core=columns_per_core\n",
    "            )\n",
    "            for i, o in zip(layer_sizes[:-1], layer_sizes[1:])\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x: jax.Array) -> jax.Array:\n",
    "        \"\"\"\n",
    "        Forward pass through the network. Assume that x has a batch dimension!\n",
    "        \"\"\"\n",
    "\n",
    "        # resize the image to be (32, 32) for MNIST\n",
    "        x = jax.image.resize(x, (x.shape[0], 32, 32, 1), method='nearest')\n",
    "\n",
    "        # flatten the first two dimensions\n",
    "        x = jnp.reshape(x, (x.shape[0], -1))\n",
    "\n",
    "        for layer in self.layers:\n",
    "            x = jax.vmap(layer, in_axes=(0,))(x)\n",
    "\n",
    "        # at the final layer apply population code\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        x = x[:, :250]\n",
    "        x = x.reshape(x.shape[0], 10, -1)\n",
    "        x = jnp.mean(x, axis=-1)\n",
    "\n",
    "        return x\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eebfd64b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (10, 10)\n",
      "Output: [0.02932509 0.06052509 0.05852911 0.05839609 0.04864245 0.05797828\n",
      " 0.05379442 0.01932956 0.04287183 0.08704461]\n"
     ]
    }
   ],
   "source": [
    "# testing the network\n",
    "layer_sizes = [1024, 2048, 512, 256]\n",
    "activation_function = nnx.relu\n",
    "test_network = PartialSumsNetwork(\n",
    "    layer_sizes=layer_sizes,\n",
    "    rngs=rngs,\n",
    "    activation_function=activation_function,\n",
    "    columns_per_core=256\n",
    ")\n",
    "\n",
    "x_test = jax.random.normal(rngs.default(), (10, 32, 32, 1))\n",
    "y_test = test_network(x_test)\n",
    "print(f\"Output shape: {y_test.shape}\")\n",
    "print(f\"Output: {y_test[0, :]}\")\n",
    "\n",
    "# nnx.display(test_network)\n",
    "del test_network  # Clean up the test network to free memory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee74c8c",
   "metadata": {},
   "source": [
    "## Setting up a training pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "de823cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1754532170.340388 1922137 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "# dataset loading\n",
    "data_dir = \"/local_disk/vikrant/datasets\"\n",
    "dataset_dict = {\n",
    "    'batch_size': 64, # 64 is a good batch size for MNIST\n",
    "    'train_steps': int(2e4), # run for longer, 20000 is good!\n",
    "    'binarize': True, \n",
    "    'greyscale': True,\n",
    "    'data_dir': data_dir,\n",
    "    'seed': 101,\n",
    "    'shuffle_buffer': 1024,\n",
    "    'threshold' : 0.5, # binarization threshold, not to be confused with the threshold in the model\n",
    "    'eval_every': 1000,\n",
    "}\n",
    "\n",
    "# loading the dataset\n",
    "train_ds, valid_ds, test_ds = load_and_augment_mnist(\n",
    "    batch_size=dataset_dict['batch_size'],\n",
    "    train_steps=dataset_dict['train_steps'],\n",
    "    data_dir=dataset_dict['data_dir'],\n",
    "    seed=dataset_dict['seed'],\n",
    "    shuffle_buffer=dataset_dict['shuffle_buffer'],\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "915fb48c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training step functions\n",
    "def loss_fn(model: PartialSumsNetwork, batch):\n",
    "    logits = model(batch['image'])\n",
    "    loss = optax.softmax_cross_entropy_with_integer_labels(\n",
    "        logits=logits, labels=batch['label']\n",
    "    ).mean()\n",
    "\n",
    "    return loss, logits\n",
    "\n",
    "@nnx.jit\n",
    "@nnx.jit\n",
    "def train_step(model: PartialSumsNetwork, optimizer: nnx.Optimizer, metrics: nnx.MultiMetric, batch, loss_fn: Callable = loss_fn):\n",
    "  \"\"\"Train for a single step.\"\"\"\n",
    "  grad_fn = nnx.value_and_grad(loss_fn, has_aux=True)\n",
    "  (loss, logits), grads = grad_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch['label'])  # In-place updates.\n",
    "  optimizer.update(grads)  # In-place updates.\n",
    "\n",
    "@nnx.jit\n",
    "def eval_step(model: PartialSumsNetwork, metrics: nnx.MultiMetric, batch, loss_fn: Callable = loss_fn):\n",
    "  loss, logits = loss_fn(model, batch)\n",
    "  metrics.update(loss=loss, logits=logits, labels=batch['label'])  # In-place updates.\n",
    "\n",
    "@nnx.jit\n",
    "def pred_step(model: PartialSumsNetwork, batch):\n",
    "  logits = model(batch['image'])\n",
    "  return logits.argmax(axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40658526",
   "metadata": {},
   "outputs": [],
   "source": [
    "key = jax.random.key(10)\n",
    "key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "rngs = nnx.Rngs(params=key1, activations=key2, permute=key3, default=key4)\n",
    "\n",
    "# Initialize the model\n",
    "model = PartialSumsNetwork(\n",
    "    layer_sizes=[1024, 2048, 512, 256],\n",
    "    rngs=rngs,\n",
    "    activation_function=nnx.relu,\n",
    "    columns_per_core=256\n",
    ")\n",
    "\n",
    "# optimizers\n",
    "hyperparameters = {\n",
    "    'learning_rate': 0.8e-4, # 1e-3 seems to work well\n",
    "    'momentum': 0.9, \n",
    "    'weight_decay': 1e-4\n",
    "}\n",
    "\n",
    "optimizer = nnx.Optimizer(\n",
    "    model,\n",
    "    optax.adamw(learning_rate=hyperparameters['learning_rate'], weight_decay=hyperparameters['weight_decay'])\n",
    ")\n",
    "\n",
    "metrics = nnx.MultiMetric(\n",
    "    accuracy=nnx.metrics.Accuracy(),\n",
    "    loss=nnx.metrics.Average('loss')\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c64db229",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_history = {\n",
    "'train_loss': [],\n",
    "'train_accuracy': [],\n",
    "'test_loss': [],\n",
    "'valid_loss': [],\n",
    "'valid_accuracy': [],\n",
    "'test_accuracy': [],\n",
    "'step': []\n",
    "}\n",
    "\n",
    "def train_scrramble_capsnet_mnist(\n",
    "        model: PartialSumsNetwork = model,\n",
    "        optimizer: nnx.Optimizer = optimizer,\n",
    "        train_ds: tf.data.Dataset = train_ds,\n",
    "        valid_ds: tf.data.Dataset = valid_ds,\n",
    "        dataset_dict: dict = dataset_dict,\n",
    "        save_model_flag: bool = False,\n",
    "        save_metrics_flag: bool = False,\n",
    "):\n",
    "    \n",
    "    eval_every = dataset_dict['eval_every']\n",
    "    train_steps = dataset_dict['train_steps']\n",
    "\n",
    "    for step, batch in enumerate(train_ds.as_numpy_iterator()):\n",
    "        # Run the optimization for one step and make a stateful update to the following:\n",
    "        # - The train state's model parameters\n",
    "        # - The optimizer state\n",
    "        # - The training loss and accuracy batch metrics\n",
    "\n",
    "        train_step(model, optimizer, metrics, batch)\n",
    "\n",
    "        if step > 0 and (step % eval_every == 0 or step == train_steps - 1):  # One training epoch has passed.\n",
    "            metrics_history['step'].append(step)  # Record the step.\n",
    "            # Log the training metrics.\n",
    "            for metric, value in metrics.compute().items():  # Compute the metrics.\n",
    "                metrics_history[f'train_{metric}'].append(float(value))  # Record the metrics.\n",
    "            metrics.reset()  # Reset the metrics for the test set.\n",
    "\n",
    "            # Compute the metrics on the validation set after each training epoch.\n",
    "            for valid_batch in valid_ds.as_numpy_iterator():\n",
    "                eval_step(model, metrics, valid_batch)\n",
    "\n",
    "            # Log the validation metrics.\n",
    "            for metric, value in metrics.compute().items():\n",
    "                metrics_history[f'valid_{metric}'].append(float(value))\n",
    "            metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "            print(f\"Step {step}: Valid loss: {metrics_history['valid_loss'][-1]}, Accuracy: {metrics_history['valid_accuracy'][-1]}\")\n",
    "\n",
    "    best_accuracy = max(metrics_history['valid_accuracy'])\n",
    "    print(f\"Best accuracy: {best_accuracy}\")\n",
    "\n",
    "    # find the test set accuracy\n",
    "    for test_batch in test_ds.as_numpy_iterator():\n",
    "        eval_step(model, metrics, test_batch)\n",
    "        # print the metrics\n",
    "    for metric, value in metrics.compute().items():\n",
    "        metrics_history[f'test_{metric}'].append(float(value))\n",
    "    metrics.reset()  # Reset the metrics for the next training epoch.\n",
    "\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Test loss: {metrics_history['test_loss'][-1]}, Test accuracy: {metrics_history['test_accuracy'][-1]}\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "    # if save_model_flag:\n",
    "    #     today = date.today().isoformat()\n",
    "    #     filename = f\"sscamble_mnist_capsnet_recon_capsules{(sum(model.layer_sizes)-model.input_eff_capsules):d}_acc_{metrics_history['test_accuracy'][-1]*100:.0f}_{today}.pkl\"\n",
    "    #     graphdef, state = nnx.split(model)\n",
    "    #     save_model(state, filename)\n",
    "\n",
    "    # if save_metrics_flag:\n",
    "    #     today = date.today().isoformat()\n",
    "    #     filename = f\"sscamble_mnist_capsnet_recon_capsules{(sum(model.layer_sizes)-model.input_eff_capsules):d}_acc_{metrics_history['test_accuracy'][-1]*100:.0f}_{today}.pkl\"\n",
    "    #     save_metrics(metrics_history, filename)\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f318eef4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-06 19:02:51.381742: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-08-06 19:02:51.381770: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-08-06 19:02:51.381835: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n",
      "2025-08-06 19:02:51.381846: W external/xla/xla/service/gpu/autotuning/dot_search_space.cc:200] All configs were filtered out because none of them sufficiently match the hints. Maybe the hints set does not contain a good representative set of valid configs?Working around this by using the full hints set instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1000: Valid loss: 0.5676942467689514, Accuracy: 0.8343870043754578\n",
      "Step 2000: Valid loss: 0.33052974939346313, Accuracy: 0.9018086194992065\n",
      "Step 3000: Valid loss: 0.2367028146982193, Accuracy: 0.9331986308097839\n",
      "Step 4000: Valid loss: 0.19924293458461761, Accuracy: 0.9418213963508606\n",
      "Step 5000: Valid loss: 0.17907705903053284, Accuracy: 0.9481034278869629\n",
      "Step 6000: Valid loss: 0.15969376266002655, Accuracy: 0.9534050822257996\n",
      "Step 7000: Valid loss: 0.15318016707897186, Accuracy: 0.9547855257987976\n",
      "Step 8000: Valid loss: 0.13666708767414093, Accuracy: 0.9592269659042358\n",
      "Step 9000: Valid loss: 0.12477154284715652, Accuracy: 0.9638484716415405\n",
      "Step 10000: Valid loss: 0.12560532987117767, Accuracy: 0.9622079133987427\n",
      "Step 11000: Valid loss: 0.1087472140789032, Accuracy: 0.9675696492195129\n",
      "Step 12000: Valid loss: 0.1167786493897438, Accuracy: 0.9648887515068054\n",
      "Step 13000: Valid loss: 0.11031541973352432, Accuracy: 0.967409610748291\n",
      "Step 14000: Valid loss: 0.0993165448307991, Accuracy: 0.9702705144882202\n",
      "Step 15000: Valid loss: 0.09352857619524002, Accuracy: 0.9722911715507507\n",
      "Step 16000: Valid loss: 0.09546074271202087, Accuracy: 0.9715508818626404\n",
      "Step 17000: Valid loss: 0.09416969865560532, Accuracy: 0.9718310236930847\n",
      "Step 18000: Valid loss: 0.09069770574569702, Accuracy: 0.9725712537765503\n",
      "Step 19000: Valid loss: 0.08946891129016876, Accuracy: 0.9735515713691711\n",
      "Step 19999: Valid loss: 0.08295625448226929, Accuracy: 0.9755321741104126\n",
      "Best accuracy: 0.9755321741104126\n",
      "==================================================\n",
      "Test loss: 0.04903031140565872, Test accuracy: 0.9842748641967773\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "trained_model = train_scrramble_capsnet_mnist(\n",
    "    model=model,\n",
    "    optimizer=optimizer,\n",
    "    train_ds=train_ds,\n",
    "    valid_ds=valid_ds,\n",
    "    dataset_dict=dataset_dict,\n",
    "    save_model_flag=False,  # Set to True if you want to save the model.\n",
    "    save_metrics_flag=False,  # Set to True if you want to save the metrics.\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6ee63314",
   "metadata": {},
   "outputs": [],
   "source": [
    "del model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b9d651ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model test accuracy: 0.6991\n"
     ]
    }
   ],
   "source": [
    "graphdef, trained_state = nnx.split(trained_model)\n",
    "\n",
    "qmodel = PartialSumsNetwork(\n",
    "    layer_sizes=[1024, 2048, 512, 256],\n",
    "    rngs=nnx.Rngs(params=0, activations=1, permute=5, default=345),\n",
    "    activation_function=partial(qrelu, bits=32, max_value=2.0),\n",
    "    columns_per_core=256\n",
    ")\n",
    "\n",
    "qgraphdef, _ = nnx.split(qmodel)\n",
    "qmodel = nnx.merge(qgraphdef, trained_state)\n",
    "\n",
    "qmodel.eval() # Switch to evaluation mode.\n",
    "\n",
    "accuracies = []\n",
    "\n",
    "# evaluate the quantized model\n",
    "for test_batch in test_ds.as_numpy_iterator():\n",
    "    preds = pred_step(qmodel, test_batch)\n",
    "    true_labels = test_batch['label']\n",
    "    accuracies.append(jnp.mean(preds == true_labels))\n",
    "\n",
    "accuracy = jnp.mean(jnp.array(accuracies))\n",
    "print(f\"Quantized model test accuracy: {accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f175f09",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_gpu_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

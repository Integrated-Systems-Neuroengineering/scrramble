{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the working of the blocks\n",
    "- Copy over all the blocks.\n",
    "- Simulate them imdependently.\n",
    "- Vefity `vmap`\n",
    "- Verify activation.\n",
    "- Implement quantization.\n",
    "\n",
    "\n",
    "## Progress till 1/15/25\n",
    "- Weights quantized ✅ (caveat: not quantized within some rails but the rails are min/max values of parameters)\n",
    "- Blocks running  ✅\n",
    "- Training works ~92% with `relu`  ✅\n",
    "- Rommani also has data showing training works for `silu`\n",
    "- Training does NOT work with binary case ❌\n",
    "- Inference works with `relu` and `silu`  ✅\n",
    "- Inference does NOT work with binary case ❌"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "# from EICDense import *\n",
    "# from ShuffleBlock import *\n",
    "# from Accumulator import *\n",
    "# from PseudoFFNet import *\n",
    "# from EICNet import *\n",
    "from HelperFunctions.binary_trident_helper_functions import *\n",
    "from HelperFunctions.binary_mnist_dataloader import *\n",
    "from HelperFunctions.metric_functions import *\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': Array([ 1.000e-03,  2.345e-01,  2.100e-01, -9.000e-01,  1.320e+00,\n",
      "       -7.234e+00], dtype=float32)}\n",
      "{'weights': Array([ 0.        ,  0.23622048,  0.21259843, -0.8976378 ,  1.3228346 ,\n",
      "       -7.2362204 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def quantize_params(params, bits = 8):\n",
    "    \"\"\"\n",
    "    Quantizes the parameters of the model to given number of bits.\n",
    "    Args:\n",
    "        params: flax model parameters\n",
    "        bits: number of bits to quantize to\n",
    "    Returns:\n",
    "        quantized_params: quantized flax model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 2**(bits - 1) - 1\n",
    "    params = jax.tree.map(\n",
    "        lambda p: jnp.round(p * scale) / scale, params\n",
    "    )\n",
    "\n",
    "    return params\n",
    "\n",
    "## testing\n",
    "params = {\n",
    "    'weights': jnp.array([0.001, 0.2345, 0.21, -0.9, 1.32, -7.234])\n",
    "}\n",
    "print(params)\n",
    "quantized_params = quantize_params(params, bits = 8)\n",
    "print(quantized_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EICDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EICDense(nn.Module):\n",
    "    \"\"\"\n",
    "    Pseudo-dense layer using EIC Cores.\n",
    "    Args:\n",
    "    in_size: int, number of input neurons\n",
    "    out_size: int, number of output neurons\n",
    "    threshold: float, threshold for binary activation\n",
    "    noise_sd: flaat, standard deviation of noise for binary activation\n",
    "    key: jax.random.PRNGKey, random key\n",
    "\n",
    "    Returns:\n",
    "    x: jnp.ndarray, output of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    in_size: int\n",
    "    out_size: int\n",
    "    # threshold: float\n",
    "    # noise_sd: float\n",
    "    # activation: callable\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up dependent parameters\n",
    "        \"\"\"\n",
    "        self.out_blocks = max(self.out_size//256, 1) # number of blocks required at the output \n",
    "        self.in_blocks = max(self.in_size//256, 1) # number of bloacks required at the input\n",
    "\n",
    "\n",
    "        self.num_cores = self.out_blocks * self.in_blocks # number of cores required\n",
    "        self.W = self.param(\n",
    "            \"weights\",\n",
    "            lambda key, shape: nn.initializers.xavier_normal()(key, shape),\n",
    "            (self.out_blocks, self.in_blocks, 256, 256)\n",
    "        )\n",
    "\n",
    "\n",
    "    # @staticmethod\n",
    "    # def linear_map(x, threshold = 0., noise_sd = 0.1, key = None):\n",
    "    #     \"\"\"\n",
    "    #     Linear map\n",
    "    #     \"\"\"\n",
    "    #     return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def sigmoid_fn(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0)):\n",
    "    #     \"\"\"\n",
    "    #     Simple sigmoid.\n",
    "    #     \"\"\"\n",
    "    #     return jax.nn.sigmoid(x)\n",
    "    \n",
    "    # def relu_fn(self, x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0)):\n",
    "    #     \"\"\"\n",
    "    #     Simple ReLU.\n",
    "    #     \"\"\"\n",
    "    #     return jax.nn.relu(x)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the layer\n",
    "        Args:\n",
    "        x: jnp.ndarray (batch_size, in_size), input to the layer\n",
    "        \n",
    "        Returns:\n",
    "        x: jnp.ndarray, output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[-1] == self.in_size, f\"Input shape is incorrect. Got {x.shape[-1]}, expected {self.in_size}\"\n",
    "\n",
    "        x_reshaped = x.reshape(x.shape[0], self.in_blocks, 256) # organize x into blocks of 256 for every batch\n",
    "\n",
    "        # make sure that the weights are positive\n",
    "        W_pos= jax.nn.softplus(self.W)\n",
    "\n",
    "        # quantize weights\n",
    "        W_pos = quantize_params(W_pos, bits = 8)\n",
    "\n",
    "        y = jnp.einsum(\"ijkl,bjl->bijk\", W_pos, x_reshaped)\n",
    "\n",
    "        # activation_fn = self.activation if self.activation is not None else self.linear_map\n",
    " \n",
    "        # key = self.make_rng(\"activation\")\n",
    "        # y = activation_fn(y, threshold = self.threshold, noise_sd = self.noise_sd, key = key)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    \n",
    "# testing...\n",
    "\n",
    "# rng = jax.random.key(0)\n",
    "# key, subkey = jax.random.split(rng)\n",
    "# x = jax.random.normal(key, (1024,))\n",
    "# eic = EICDense(in_size = 1024, out_size = 2048, threshold=0., activation = custom_binary_gradient, noise_sd = 0.1)\n",
    "# params_eic = eic.init(key, x)\n",
    "# print(\"Initialized EICDense parameters\")\n",
    "# print(f\"Params: {params_eic}\")\n",
    "# print(f\"Params shape: {params_eic['params']['weights'].shape}\")\n",
    "# y = eic.apply(params_eic, x, rngs = {\"activation\": subkey})\n",
    "# print(f\"Output: {y}\")\n",
    "# print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the accumulator module\n",
    "class Accumulator(nn.Module):\n",
    "    \"\"\"\n",
    "    Accumulating the EICDense outputs. \n",
    "    Since the EICDense generates pseudo-feedforward outputs, we use a learnable accumulation matrix that minimizes error\n",
    "    between the true feedforward output and the EIC output.\n",
    "\n",
    "    Args:\n",
    "        in_block_size: int, number of 256-sized blocks. This should be the .shape[0] of the EICDense output\n",
    "    \"\"\"\n",
    "\n",
    "    in_block_size: int\n",
    "    # threshold: float\n",
    "    # noise_sd: float\n",
    "    # activation: callable = None\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up the weights for the accumulator\n",
    "        \"\"\"\n",
    "\n",
    "        self.W = self.param(\n",
    "            \"weights\",\n",
    "            nn.initializers.xavier_normal(),\n",
    "            (self.in_block_size, 256, 256)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_map(x, threshold = 0., noise_sd = 0.1, key = None):\n",
    "        \"\"\"\n",
    "        Linear map\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the accumulator\n",
    "        Args:\n",
    "        x: jnp.ndarray, input to the accumulator\n",
    "        \n",
    "        Returns:\n",
    "        x: jnp.ndarray, output of the accumulator\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[1] == self.in_block_size, \"Input shape is incorrect\"\n",
    "        # assert x.shape[1] == self.out_block_size, \"Input shape is incorrect\"\n",
    "\n",
    "        # ensure positive \n",
    "        W_pos = jax.nn.softplus(self.W)\n",
    "        W_pos = quantize_params(W_pos, bits = 8)\n",
    "        \n",
    "        x = jnp.einsum(\"bijk->bik\", x)\n",
    "        y = jnp.einsum(\"ijk,bik->bik\", W_pos, x) \n",
    "\n",
    "\n",
    "        # y = jnp.einsum('ijk,imk->ij', W_pos, x) #jnp.einsum('imk,ijk->im', W_pos, x) #\n",
    "        # key = self.make_rng(\"activation\")\n",
    "\n",
    "        # activation_fn = self.activation if self.activation is not None else self.linear_map\n",
    "        # y = activation_fn(y, threshold = self.threshold, noise_sd = self.noise_sd, key = key)\n",
    "\n",
    "        # flatten y before returning\n",
    "        y = y.reshape((y.shape[0], -1)) # (batch_size, out_size)\n",
    "\n",
    "        return y\n",
    "\n",
    "# testing...\n",
    "# def __main__():\n",
    "#     rng = jax.random.key(42)\n",
    "#     x = jax.random.normal(rng, (8, 4, 256))\n",
    "#     acc = Accumulator(\n",
    "#         in_block_size = x.shape[0],\n",
    "#         threshold = 0.0,\n",
    "#         noise_sd = 1.0,\n",
    "#         activation = custom_binary_gradient\n",
    "#     )\n",
    "\n",
    "#     params = acc.init(rng, x)\n",
    "#     print(\"Initialized accumulator parameters\")\n",
    "#     print(\"----------------------------------------\")\n",
    "#     print(f\"Output shape: {params[\"params\"][\"weights\"].shape}\")\n",
    "#     y = acc.apply(params, x, rngs = {\"activation\": rng})\n",
    "#     print(\"----------------------------------------\")\n",
    "#     print(f\"Accumulator output {y}, \\n Output shape: {y.shape}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __main__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simplified accumulator\n",
    "- Simply accumulates over contracting dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the accumulator module\n",
    "class SimpleAccumulator(nn.Module):\n",
    "    \"\"\"\n",
    "    Accumulating the EICDense outputs. \n",
    "    Since the EICDense generates pseudo-feedforward outputs, we use a learnable accumulation matrix that minimizes error\n",
    "    between the true feedforward output and the EIC output.\n",
    "\n",
    "    Args:\n",
    "        in_block_size: int, number of 256-sized blocks. This should be the .shape[0] of the EICDense output\n",
    "    \"\"\"\n",
    "\n",
    "    in_block_size: int\n",
    "    # threshold: float\n",
    "    # noise_sd: float\n",
    "    # activation: callable = None\n",
    "\n",
    "    # def setup(self):\n",
    "    #     \"\"\"\n",
    "    #     Set up the weights for the accumulator\n",
    "    #     \"\"\"\n",
    "\n",
    "    #     self.W = self.param(\n",
    "    #         \"weights\",\n",
    "    #         nn.initializers.xavier_normal(),\n",
    "    #         (self.in_block_size, 256, 256)\n",
    "    #     )\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the accumulator\n",
    "        Args:\n",
    "        x: jnp.ndarray, input to the accumulator\n",
    "        \n",
    "        Returns:\n",
    "        x: jnp.ndarray, output of the accumulator\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[1] == self.in_block_size, \"Input shape is incorrect\"\n",
    "        # assert x.shape[1] == self.out_block_size, \"Input shape is incorrect\"\n",
    "\n",
    "        \n",
    "        y = jnp.einsum(\"bijk->bik\", x)\n",
    "        \n",
    "\n",
    "        # flatten y before returning\n",
    "        y = y.reshape((y.shape[0], -1)) # (batch_size, out_size)\n",
    "\n",
    "        return y\n",
    "\n",
    "# testing...\n",
    "# def __main__():\n",
    "#     rng = jax.random.key(42)\n",
    "#     x = jax.random.normal(rng, (8, 4, 256))\n",
    "#     acc = Accumulator(\n",
    "#         in_block_size = x.shape[0],\n",
    "#         threshold = 0.0,\n",
    "#         noise_sd = 1.0,\n",
    "#         activation = custom_binary_gradient\n",
    "#     )\n",
    "\n",
    "#     params = acc.init(rng, x)\n",
    "#     print(\"Initialized accumulator parameters\")\n",
    "#     print(\"----------------------------------------\")\n",
    "#     print(f\"Output shape: {params[\"params\"][\"weights\"].shape}\")\n",
    "#     y = acc.apply(params, x, rngs = {\"activation\": rng})\n",
    "#     print(\"----------------------------------------\")\n",
    "#     print(f\"Accumulator output {y}, \\n Output shape: {y.shape}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __main__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (10, 256)\n",
      "Output: [[ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " ...\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]]\n"
     ]
    }
   ],
   "source": [
    "## define a module to split the input and shuffle it in blocks of 64\n",
    "\n",
    "class ShuffleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a trainable permutation matrix with a temperature parameter.\n",
    "    This is a proxy for the lookup table. In some sense it describes an idealized LUT.\n",
    "    Temperature parameter can be very loosely interpreted as \"release probability\"\n",
    "\n",
    "    Args:\n",
    "        input_size: int, size of the input (e.g. 2048)\n",
    "    Defines:\n",
    "        A: jnp.ndarray, trainable permutation matrix\n",
    "        tau: float, temperature parameter\n",
    "    Returns:\n",
    "        y: jnp.ndarray, shuffled input. Basically, y = Ax\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    tau: float \n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up trainable permutation matrix\n",
    "        \"\"\"\n",
    "\n",
    "        self.Zpos = self.param(\n",
    "            'Zpos',\n",
    "            nn.initializers.xavier_normal(),\n",
    "            (self.input_size, self.input_size)\n",
    "        )\n",
    "\n",
    "        self.Zneg = self.param(\n",
    "            'Zneg',\n",
    "            nn.initializers.xavier_normal(),\n",
    "            (self.input_size, self.input_size)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Soft shuffle the input\n",
    "        \"\"\"\n",
    "\n",
    "        Ppos = jax.nn.softmax(self.Zpos/self.tau, axis = -1)\n",
    "        Pneg = jax.nn.softmax(self.Zneg/self.tau, axis = -1)\n",
    "\n",
    "        xpos = jnp.einsum('ij,bj->bi', Ppos, x)\n",
    "        xneg = jnp.einsum('ij,bj->bi', Pneg, x)\n",
    "\n",
    "        # P = jax.nn.softmax(self.Z/self.tau, axis = -1) * jnp.sign(self.Z)\n",
    "        y = xpos - xneg\n",
    "\n",
    "        return y\n",
    "    \n",
    "# testing...\n",
    "sh1 = ShuffleBlock(input_size = 256, tau = 0.5)\n",
    "key = jax.random.key(1)\n",
    "x = jnp.ones((10, 256))\n",
    "params = sh1.init(key, x)\n",
    "out = sh1.apply(params, x)\n",
    "\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Output: {out}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Permute Block\n",
    "- Contains two fixed non-trainable matrices to permute the inputs in blocks of M (= 16 or 64)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 256)\n",
      "[[ 192.  192.  192.  192.  192.  192.  192.  192.  192.  192.  192.  192.\n",
      "   192.  192.  192.  192. -240. -240. -240. -240. -240. -240. -240. -240.\n",
      "  -240. -240. -240. -240. -240. -240. -240. -240.  224.  224.  224.  224.\n",
      "   224.  224.  224.  224.  224.  224.  224.  224.  224.  224.  224.  224.\n",
      "   -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.\n",
      "   -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.\n",
      "   -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.   80.   80.   80.   80.\n",
      "    80.   80.   80.   80.   80.   80.   80.   80.   80.   80.   80.   80.\n",
      "   -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.\n",
      "   -32.  -32.  -32.  -32.  128.  128.  128.  128.  128.  128.  128.  128.\n",
      "   128.  128.  128.  128.  128.  128.  128.  128. -144. -144. -144. -144.\n",
      "  -144. -144. -144. -144. -144. -144. -144. -144. -144. -144. -144. -144.\n",
      "    32.   32.   32.   32.   32.   32.   32.   32.   32.   32.   32.   32.\n",
      "    32.   32.   32.   32.   64.   64.   64.   64.   64.   64.   64.   64.\n",
      "    64.   64.   64.   64.   64.   64.   64.   64.  -16.  -16.  -16.  -16.\n",
      "   -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.\n",
      "    48.   48.   48.   48.   48.   48.   48.   48.   48.   48.   48.   48.\n",
      "    48.   48.   48.   48.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.\n",
      "   -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16. -160. -160. -160. -160.\n",
      "  -160. -160. -160. -160. -160. -160. -160. -160. -160. -160. -160. -160.\n",
      "    32.   32.   32.   32.   32.   32.   32.   32.   32.   32.   32.   32.\n",
      "    32.   32.   32.   32.]\n",
      " [ 192.  192.  192.  192.  192.  192.  192.  192.  192.  192.  192.  192.\n",
      "   192.  192.  192.  192. -240. -240. -240. -240. -240. -240. -240. -240.\n",
      "  -240. -240. -240. -240. -240. -240. -240. -240.  224.  224.  224.  224.\n",
      "   224.  224.  224.  224.  224.  224.  224.  224.  224.  224.  224.  224.\n",
      "   -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.\n",
      "   -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.\n",
      "   -96.  -96.  -96.  -96.  -96.  -96.  -96.  -96.   80.   80.   80.   80.\n",
      "    80.   80.   80.   80.   80.   80.   80.   80.   80.   80.   80.   80.\n",
      "   -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.  -32.\n",
      "   -32.  -32.  -32.  -32.  128.  128.  128.  128.  128.  128.  128.  128.\n",
      "   128.  128.  128.  128.  128.  128.  128.  128. -144. -144. -144. -144.\n",
      "  -144. -144. -144. -144. -144. -144. -144. -144. -144. -144. -144. -144.\n",
      "    32.   32.   32.   32.   32.   32.   32.   32.   32.   32.   32.   32.\n",
      "    32.   32.   32.   32.   64.   64.   64.   64.   64.   64.   64.   64.\n",
      "    64.   64.   64.   64.   64.   64.   64.   64.  -16.  -16.  -16.  -16.\n",
      "   -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.\n",
      "    48.   48.   48.   48.   48.   48.   48.   48.   48.   48.   48.   48.\n",
      "    48.   48.   48.   48.  -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16.\n",
      "   -16.  -16.  -16.  -16.  -16.  -16.  -16.  -16. -160. -160. -160. -160.\n",
      "  -160. -160. -160. -160. -160. -160. -160. -160. -160. -160. -160. -160.\n",
      "    32.   32.   32.   32.   32.   32.   32.   32.   32.   32.   32.   32.\n",
      "    32.   32.   32.   32.]]\n"
     ]
    }
   ],
   "source": [
    "class PermuteBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains two fixed permutation matrices (pos and neg) to shuffle the input block-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    permute_block_size: int = 16 # previously 64\n",
    "    core_input_size: int = 256\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up permutation matrices\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.num_slots = self.core_input_size // self.permute_block_size # should be 16 in the latest iteration\n",
    "        self.num_subvectors = self.input_size // self.core_input_size # for input_size = 1024, should be 256\n",
    "\n",
    "        # generate two independent permutation sequences\n",
    "        key = jax.random.key(1245)\n",
    "        key1, key2 = jax.random.split(key)\n",
    "        p1 = jax.random.permutation(key1, self.num_slots)\n",
    "        p2 = jnp.roll(p1, shift = 1) #jax.random.permutation(key2, self.num_slots)\n",
    "\n",
    "        # generate permutation matrices\n",
    "        m1 = jnp.eye(self.num_slots)\n",
    "        m2 = jnp.eye(self.num_slots)\n",
    "\n",
    "        # generate the permutation matrices\n",
    "        self.Ppos = m1[p1]\n",
    "        self.Pneg = m2[p2]\n",
    "\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Apply permutations and return (xpos - xneg)\n",
    "        Args:\n",
    "        x: jnp.ndarray, input vector. Shape: (batch_size, input_size) e.g. (32, 2048)\n",
    "        Returns:\n",
    "        xpos - xneg: jnp.ndarray, difference of permuted inputs. Shape: (batch_size, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[-1] == self.input_size, f\"Input shape is incorrect. Got {x.shape[-1]}, expected {self.input_size}\"\n",
    "        assert self.num_subvectors * self.num_slots * self.permute_block_size == self.input_size, f\"Inconsistent metrics!\"\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.num_subvectors, self.num_slots, self.permute_block_size) # first dimension must be the batch size\n",
    "\n",
    "        xpos = jnp.einsum('ij,bsjp->bsip', self.Ppos, x)\n",
    "        xneg = jnp.einsum('ij,bsjp->bsip', self.Pneg, x)\n",
    "\n",
    "        xout = xpos - xneg\n",
    "\n",
    "        xout = xout.reshape((x.shape[0], self.input_size))\n",
    "\n",
    "        return jax.lax.stop_gradient(xout)\n",
    "    \n",
    "# testing ...\n",
    "key = jax.random.key(0)\n",
    "key1, key2 = jax.random.split(key)\n",
    "\n",
    "ta1 = jnp.concatenate((jnp.arange(256)[:, None], jnp.arange(256)[:, None]), axis = 1).T\n",
    "print(ta1.shape)\n",
    "\n",
    "p1 = PermuteBlock(input_size = 256)\n",
    "_ = p1.init(key, ta1)\n",
    "out = p1.apply(_, ta1)\n",
    "\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 7 8 9 0 1 2 3 4 5]\n"
     ]
    }
   ],
   "source": [
    "# key = jax.random.key(0)\n",
    "# key1, key2 = jax.random.split(key)\n",
    "\n",
    "# p1 = jax.random.permutation(key1, 16)\n",
    "# p2 = jax.random.permutation(key2, 16)\n",
    "\n",
    "# print(f\"{p1} \\n {p2}\")\n",
    "\n",
    "# mat1 = jnp.eye(16)\n",
    "# mat2 = jnp.eye(16)\n",
    "\n",
    "# mat1 = mat1[p1]\n",
    "# mat2 = mat2[p2]\n",
    "\n",
    "# # print(f\"{mat1} \\n {mat2}\")\n",
    "\n",
    "# ta1 = jnp.concatenate((jnp.arange(16)[:, None], jnp.arange(16)[:, None]), axis = 1)\n",
    "# print(ta1)\n",
    "# a1 = jnp.einsum('ij,jk->ik', mat1, ta1)\n",
    "# a2 = jnp.einsum('ij,jk->ik', mat2, ta1)\n",
    "# print(a1 - a2)\n",
    "\n",
    "arr = jnp.arange(10)\n",
    "print(jnp.roll(arr, shift = 4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EICNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EICNetwork -> Might supersede PseudoFFNet... in progress\n",
    "    1024 - 2048 - 256 - 10\n",
    "    \"\"\"\n",
    "    def setup(self):\n",
    "        # self.p1 = PermuteBlock(input_size = 16*16)\n",
    "        self.fc1 = EICDense(in_size = 16*16, out_size = 2048) # (8, 4, 256)\n",
    "        self.ac1 = SimpleAccumulator(in_block_size = 2048//256) # (2048,)\n",
    "        self.p2 = PermuteBlock(input_size = 2048)\n",
    "        self.fc2 = EICDense(in_size = 2048, out_size = 512) # (8, 4, 256)\n",
    "        self.ac2 = SimpleAccumulator(in_block_size = 512//256) # (2048,)\n",
    "        self.p3 = PermuteBlock(input_size = 512)\n",
    "        self.fc3 = EICDense(in_size = 512, out_size = 256) # (8, 4, 256)\n",
    "        self.ac3 = SimpleAccumulator(in_block_size = 256//256) # (2048,)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        nsd = 0.5\n",
    "        x = x.reshape((x.shape[0], 16*16))\n",
    "        # x = self.p1(x)\n",
    "        print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = nsd, key = jax.random.key(0)) #jax.nn.relu(x) # custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0))\n",
    "        x = self.ac1(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = nsd, key = jax.random.key(0))\n",
    "        x = self.p2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = nsd, key = jax.random.key(0))\n",
    "        x = self.ac2(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = nsd, key = jax.random.key(0))\n",
    "        x = self.p3(x)\n",
    "        x = self.fc3(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = nsd, key = jax.random.key(0))\n",
    "        x = self.ac3(x)\n",
    "        x = x[:, :10]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make a binary network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EICNetBinary(nn.Module):\n",
    "    \"\"\"\n",
    "    EICNetwork -> Might supersede PseudoFFNet... in progress\n",
    "    1024 - 2048 - 256 - 10\n",
    "    \"\"\"\n",
    "    def setup(self):\n",
    "        temp = 0.9\n",
    "        # self.sh1 = ShuffleBlock(input_size = 16*16, tau = temp)\n",
    "        self.fc1 = EICDense(in_size = 16*16, out_size = 2048) # (8, 4, 256)\n",
    "        self.ac1 = Accumulator(in_block_size = 2048//256) # (2048,)\n",
    "        self.sh2 = ShuffleBlock(input_size = 2048, tau = temp)\n",
    "        self.fc2 = EICDense(in_size = 2048, out_size = 512) # (8, 4, 256)\n",
    "        self.ac2 = Accumulator(in_block_size = 512//256) # (2048,)\n",
    "        self.sh3 = ShuffleBlock(input_size = 512, tau = temp)\n",
    "        self.fc3 = EICDense(in_size = 512, out_size = 256) # (8, 4, 256)\n",
    "        self.ac3 = Accumulator(in_block_size = 256//256) # (2048,)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape((x.shape[0], 16*16))\n",
    "        # x = self.sh1(x)\n",
    "        key, subkey = jax.random.split(jax.random.PRNGKey(0))\n",
    "        x = self.fc1(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = subkey)\n",
    "        x = self.ac1(x)\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = subkey)\n",
    "        x = self.sh2(x)\n",
    "        x = self.fc2(x)\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = subkey)\n",
    "        x = self.ac2(x)\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = subkey)\n",
    "        x = self.sh3(x)\n",
    "        x = self.fc3(x)\n",
    "        key, subkey = jax.random.split(key)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = subkey)\n",
    "        x = self.ac3(x)\n",
    "        x = x[:, :10]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EICConvNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EICNetwork -> Might supersede PseudoFFNet... in progress\n",
    "    1024 - 2048 - 256 - 10\n",
    "    \"\"\"\n",
    "    def setup(self):\n",
    "        # self.p1 = PermuteBlock(input_size = 16*16)\n",
    "        self.conv1 = nn.Conv(features = 1, kernel_size = (4,4), padding=3, strides=2)\n",
    "        self.fc1 = EICDense(in_size = 16*16, out_size = 2048) # (8, 4, 256)\n",
    "        self.ac1 = SimpleAccumulator(in_block_size = 2048//256) # (2048,)\n",
    "        self.p2 = PermuteBlock(input_size = 2048)\n",
    "        self.fc2 = EICDense(in_size = 2048, out_size = 512) # (8, 4, 256)\n",
    "        self.ac2 = SimpleAccumulator(in_block_size = 512//256) # (2048,)\n",
    "        self.p3 = PermuteBlock(input_size = 512)\n",
    "        self.fc3 = EICDense(in_size = 512, out_size = 256) # (8, 4, 256)\n",
    "        self.ac3 = SimpleAccumulator(in_block_size = 256//256) # (2048,)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0))\n",
    "        x = nn.avg_pool(x, window_shape=(4, 4), strides=(1, 1), padding='SAME')\n",
    "        x = x.reshape((x.shape[0], 16*16))\n",
    "        print(x.shape)\n",
    "        # x = self.p1(x)\n",
    "        print(x.shape)\n",
    "        x = self.fc1(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0)) #jax.nn.relu(x) # custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0))\n",
    "        x = self.ac1(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0))\n",
    "        x = self.p2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0))\n",
    "        x = self.ac2(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0))\n",
    "        x = self.p3(x)\n",
    "        x = self.fc3(x)\n",
    "        x = custom_binary_gradient(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0))\n",
    "        x = self.ac3(x)\n",
    "        x = x[:, :10]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10, 16, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "# testing...\n",
    "conv1 = nn.Conv(features=1, kernel_size=(4, 4), strides=(2, 2), padding=3)\n",
    "x = jnp.ones((10, 28, 28, 1))\n",
    "params = conv1.init(jax.random.PRNGKey(0), x)\n",
    "out = conv1.apply(params, x)\n",
    "out = nn.avg_pool(out, window_shape=(4, 4), strides=(1,1), padding=\"SAME\")\n",
    "print(out.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.690805   -0.48744103 -1.155789    0.12108463 -0.19598432 -0.5078766\n",
      "  0.91568655  1.70968    -0.36749417  0.14315689] 7\n",
      "2.5878873\n"
     ]
    }
   ],
   "source": [
    "# some additional functions\n",
    "def cross_entropy_loss(*, logits, labels):\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes = 10)\n",
    "    loss = optax.softmax_cross_entropy(logits = logits, labels = one_hot_labels).mean()\n",
    "    return loss\n",
    "\n",
    "# testing...\n",
    "key = jax.random.key(1)\n",
    "logits = jax.random.normal(key, (10,))\n",
    "print(logits, jnp.argmax(logits))\n",
    "label = 9\n",
    "loss = cross_entropy_loss(logits = logits, labels = label)\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "DATA_PATH = f\"/Users/vikrantjaltare/OneDrive - UC San Diego/Datasets/\"\n",
    "# (train_inputs, train_labels), (val_inputs, val_labels), (test_inputs, test_labels) = load_and_process_mnist(data_path=DATA_PATH, binarize = True)\n",
    "# train_inputs[0].shape\n",
    "\n",
    "def get_datasets():\n",
    "    \"\"\"Load MNIST train, validation, and test datasets into memory.\"\"\"\n",
    "    # Load MNIST dataset\n",
    "    ds_builder = tfds.builder('mnist', data_dir=DATA_PATH)\n",
    "    ds_builder.download_and_prepare()\n",
    "    \n",
    "    # Load full datasets as numpy arrays\n",
    "    full_train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    full_test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "\n",
    "    # Normalize images\n",
    "    full_train_ds['image'] = jnp.float32(full_train_ds['image']) / 255.\n",
    "    full_test_ds['image'] = jnp.float32(full_test_ds['image']) / 255.\n",
    "\n",
    "    # Concatenate train and test datasets for splitting\n",
    "    all_images = jnp.concatenate([full_train_ds['image'], full_test_ds['image']], axis=0)\n",
    "    all_labels = jnp.concatenate([full_train_ds['label'], full_test_ds['label']], axis=0)\n",
    "\n",
    "    # Train-validation-test split: 50k-10k-10k\n",
    "    train_images, val_images, test_images = all_images[:50000], all_images[50000:60000], all_images[60000:]\n",
    "    train_labels, val_labels, test_labels = all_labels[:50000], all_labels[50000:60000], all_labels[60000:]\n",
    "\n",
    "    # Resize images to 16x16\n",
    "    def resize_images(images, new_size):\n",
    "        resized = tf.image.resize(images, new_size, method='bilinear')\n",
    "        return jnp.array(resized)\n",
    "\n",
    "    train_images = resize_images(train_images, (16, 16))\n",
    "    val_images = resize_images(val_images, (16, 16))\n",
    "    test_images = resize_images(test_images, (16, 16))\n",
    "\n",
    "    # Binarize images\n",
    "    def binarize_images(images, threshold=0.5):\n",
    "        return jnp.where(images < threshold, 0.0, 1.0)\n",
    "\n",
    "    train_images = binarize_images(train_images)\n",
    "    val_images = binarize_images(val_images)\n",
    "    test_images = binarize_images(test_images)\n",
    "\n",
    "    # Prepare datasets in similar structure as original\n",
    "    train_ds = {'image': train_images, 'label': train_labels}\n",
    "    val_ds = {'image': val_images, 'label': val_labels}\n",
    "    test_ds = {'image': test_images, 'label': test_labels}\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n",
    "\n",
    "def get_datasets_28():\n",
    "    \"\"\"Load MNIST train, validation, and test datasets into memory.\"\"\"\n",
    "    # Load MNIST dataset\n",
    "    ds_builder = tfds.builder('mnist', data_dir=DATA_PATH)\n",
    "    ds_builder.download_and_prepare()\n",
    "    \n",
    "    # Load full datasets as numpy arrays\n",
    "    full_train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    full_test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "\n",
    "    # Normalize images\n",
    "    full_train_ds['image'] = jnp.float32(full_train_ds['image']) / 255.\n",
    "    full_test_ds['image'] = jnp.float32(full_test_ds['image']) / 255.\n",
    "\n",
    "    # Concatenate train and test datasets for splitting\n",
    "    all_images = jnp.concatenate([full_train_ds['image'], full_test_ds['image']], axis=0)\n",
    "    all_labels = jnp.concatenate([full_train_ds['label'], full_test_ds['label']], axis=0)\n",
    "\n",
    "    # Train-validation-test split: 50k-10k-10k\n",
    "    train_images, val_images, test_images = all_images[:50000], all_images[50000:60000], all_images[60000:]\n",
    "    train_labels, val_labels, test_labels = all_labels[:50000], all_labels[50000:60000], all_labels[60000:]\n",
    "\n",
    "    # Resize images to 16x16\n",
    "    def resize_images(images, new_size):\n",
    "        resized = tf.image.resize(images, new_size, method='bilinear')\n",
    "        return jnp.array(resized)\n",
    "\n",
    "    # train_images = resize_images(train_images, (16, 16))\n",
    "    # val_images = resize_images(val_images, (16, 16))\n",
    "    # test_images = resize_images(test_images, (16, 16))\n",
    "\n",
    "    # Binarize images\n",
    "    def binarize_images(images, threshold=0.5):\n",
    "        return jnp.where(images < threshold, 0.0, 1.0)\n",
    "\n",
    "    train_images = binarize_images(train_images)\n",
    "    val_images = binarize_images(val_images)\n",
    "    test_images = binarize_images(test_images)\n",
    "\n",
    "    # Prepare datasets in similar structure as original\n",
    "    train_ds = {'image': train_images, 'label': train_labels}\n",
    "    val_ds = {'image': val_images, 'label': val_labels}\n",
    "    test_ds = {'image': test_images, 'label': test_labels}\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 50000\n",
      "Validation dataset size: 10000\n",
      "Test dataset size: 10000\n",
      "Image shape: (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = get_datasets_28()\n",
    "\n",
    "print(f\"Train dataset size: {len(train_ds['image'])}\")\n",
    "print(f\"Validation dataset size: {len(val_ds['image'])}\")\n",
    "print(f\"Test dataset size: {len(test_ds['image'])}\")\n",
    "print(f\"Image shape: {train_ds['image'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ4AAAGbCAYAAAD0sfa8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8hTgPZAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAWy0lEQVR4nO3dbYiVdf748c/Y4MG5ldHQFZ3QLQMbH4S1bhKLLUgm+6ANwaUbs23pZpcNI4JiW2NZQjTELQxaQypqIVk3jIigLZPYXEw3GLexQGaXzRLN23PGnTpp8/0/8N/wO+mo4x6/Z25eL/g+mOtcnvPx4nLeXnOOl3UppRQAkMmYWg8AwOgiPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkVV/rAb7V19cX+/bti+bm5qirq6v1OAAMUkopenp6YsqUKTFmzMDXNUMmPPv27Ytp06bVegwA/kd79+6NqVOnDvj4kPlRW3Nzc61HAKAKzvX9vKrh+eKLL+Lmm2+O8ePHx8SJE2P58uVx8uTJ8/q1frwGMDKc6/t5VcOzZMmSaGpqin379sUHH3wQb7/9dqxdu7aaLwHAcJeqZM+ePSki0ueff96/7ZVXXknt7e3n9euLxWKKCMuyLGuYr2KxeNbv91X7cEFXV1e0tbXFlClT+rfNmjUrPv300zh27FiMHz++Yv9yuRzlcrn/61KpVK1RABjCqvajtp6enmhsbKzY1tDQEBERx48fP23/lStXRmtra//yiTaA0aFq4WlsbIze3t6Kbd9+faZPODz66KNRLBb71969e6s1CgBDWNV+1NbR0RGHDx+OAwcOxKRJkyIiYvfu3TF16tRobW09bf9CoRCFQqFaLw/AMFG1K54rrrgirr/++li+fHn09PTEv//97/j9738fd999d7VeAoARoKofp960aVOcPHkypk+fHnPnzo2FCxfGb3/722q+BADDXF1KKdV6iIhTn2o704/kABheisVitLS0DPj4kLllDgCjg/AAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkFV9rQcAhreU0qB/TV1d3UWYhOHCFQ8AWQkPAFlVNTwbN26M+vr6aGpq6l933HFHNV8CgGGuqu/x7NixI+644454/vnnq/m0AIwgVb3i2bFjR1xzzTXVfEoARpiqhaevry8+/PDDeOONN+Kyyy6LqVOnxj333BNHjx494/7lcjlKpVLFAmDkq1p4Dh48GFdffXUsXrw4Pv7449i2bVvs2bMnbr/99jPuv3Llymhtbe1f06ZNq9YoAAxhdelCPoR/nnbs2BFz586NYrEYzc3NFY+Vy+Uol8v9X5dKJfGBYci/4+G7isVitLS0DPh41a54du3aFY888kjFSVgul2PMmDExduzY0/YvFArR0tJSsQAY+aoWnra2tli3bl08+eSTcfLkyfj000/j4YcfjmXLlkWhUKjWywAwzFUtPFOnTo033ngjNm/eHG1tbXHNNdfEtddeG+vWravWSwAwAlzU93gGo1QqRWtra63HAAbJezx817ne43GTUKDfEPl7KCOce7UBkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJBVfa0HAKovpVTrEWBArngAyEp4AMhKeADISngAyEp4AMhKeADISngAyEp4AMhKeADISngAyEp4AMhKeADIyk1CGbIu5EaXdXV1F2ESzsYxZ7Bc8QCQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJCV8ACQlfAAkJXwAJDVBYfn4MGDcfnll8fWrVv7t23fvj3mzp0bTU1NMX369NiwYUM1ZgRgBLmg8Lz//vtx3XXXRXd3d/+2o0ePxqJFi2Lp0qVx7Nix2LBhQzz44IPxwQcfVG1YAIa/QYfnxRdfjFtvvTWeeOKJiu1/+ctfYsKECfGrX/0q6uvr48c//nHcdttt8cwzz1RtWACGv0GH58Ybb4zu7u5YsmRJxfaurq6YPXt2xbZZs2ZFZ2fnGZ+nXC5HqVSqWACMfIMOz+TJk6O+vv607T09PdHY2FixraGhIY4fP37G51m5cmW0trb2r2nTpg12FACGoap9qq2xsTF6e3srtvX29kZzc/MZ93/00UejWCz2r71791ZrFACGsNMvXS5QR0dHvPXWWxXbdu/eHR0dHWfcv1AoRKFQqNbLAzBMVO2K55Zbbon9+/fHH/7whzhx4kS8++678ac//Sl+/vOfV+slABgBqhaeCRMmxF//+tf485//HBMmTIhf/OIX8fTTT8cNN9xQrZcAYASoSymlWg8REVEqlaK1tbXWYzCEXMipWVdXdxEmGX5y/rF2zPmuYrEYLS0tAz7uljkAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGRVtf+BFLg4hsj/XAJV44oHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArNwklCzc6HJ4qKurq/UIjAKueADISngAyEp4AMhKeADISngAyEp4AMhKeADISngAyEp4AMhKeADISngAyEp4AMjKTUIZUdyMFIY+VzwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABk5SahMALV1dXVegQYkCseALISHgCyuuDwHDx4MC6//PLYunVr/7b7778/CoVCNDU19a/169dXY04ARogLeo/n/fffjzvvvDO6u7srtu/YsSPWr18fd955Z1WGA2DkGfQVz4svvhi33nprPPHEExXby+Vy/POf/4xrrrmmasMBMPIMOjw33nhjdHd3x5IlSyq2d3Z2xokTJ2LFihUxadKkmDlzZqxatSr6+vrO+DzlcjlKpVLFAmDkG3R4Jk+eHPX1p/+Erlgsxvz58+OBBx6Izz77LF5++eV4+umnY82aNWd8npUrV0Zra2v/mjZt2uCnB2DYqUsppQv+xXV18e6778b8+fPP+PiTTz4ZGzdujJ07d572WLlcjnK53P91qVQSnxHsfzjNuAD+HQ+1VCwWo6WlZcDHq/YPSDdv3hwHDhyIe++9t39buVyOcePGnXH/QqEQhUKhWi8PwDBRtX/Hk1KKBx98MN55551IKcXf//73eOqppypCBABVu+L56U9/GmvXro1f/vKX8dlnn8XkyZPjd7/7Xdx+++3VegkARoD/6T2eaiqVStHa2lrrMbhIhshpNmp4j4dayvYeD6PHUI7IUP+GO5SPHeTiXm0AZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZOXu1AzaUL8DNDC0ueIBICvhASAr4QEgK+EBICvhASAr4QEgK+EBICvhASAr4QEgK+EBICvhASAr4QEgKzcJhQuUUsryOm7KykjjigeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArAYVns7OzliwYEG0tbXF5MmTY+nSpXHo0KGIiNi+fXvMnTs3mpqaYvr06bFhw4aLMjAAw9t5h+fLL7+Mm266KebNmxf79++Prq6uOHz4cNx1111x9OjRWLRoUSxdujSOHTsWGzZsiAcffDA++OCDizk7AMNROk+ffPJJWrhwYTp58mT/ttdeey21tLSk5557Ll1xxRUV+993331p6dKl5/v0qVgspoiwrGGzcqn179OyBruKxeJZz+nzvuK58sor480334xLLrmkf9umTZtizpw50dXVFbNnz67Yf9asWdHZ2Tng85XL5SiVShULgJHvgj5ckFKKxx57LF5//fV46qmnoqenJxobGyv2aWhoiOPHjw/4HCtXrozW1tb+NW3atAsZBYBhZtDhKZVKsXjx4nj55Zfjvffei9mzZ0djY2P09vZW7Nfb2xvNzc0DPs+jjz4axWKxf+3du3fw0wMw7NQPZufu7u5YtGhRtLe3x86dO2PixIkREdHR0RFvvfVWxb67d++Ojo6OAZ+rUChEoVC4gJEBGNbO9w3OI0eOpPb29rRs2bL0zTffVDx26NChNH78+LR27dr09ddfpy1btqTm5ua0ZcuW834D1YcLrOG2cqn179OyBrvO9eGC8/7Ts2bNmhQRqaGhITU2NlaslFLasWNHmjdvXmpubk4zZsxIzz///KD+cAmPNdxWLrX+fVrWYNe5wlP3/0/smiuVStHa2lrrMeC85fqjU1dXl+V1oFqKxWK0tLQM+Pig3uOBkWqI/P0LRgX3agMgK+EBICvhASAr4QEgK+EBICvhASAr4QEgK+EBICvhASAr4QEgK+EBICvhASAr4QEgK+EBICvhASAr4QEgK+EBICvhASAr4QEgK+EBICvhASAr4QEgK+EBICvhASAr4QEgK+EBIKv6Wg8Ao0ldXV2tR4Cac8UDQFbCA0BWwgNAVsIDQFbCA0BWwgNAVsIDQFbCA0BWwgNAVsIDQFbCA0BWwgNAVsIDQFbCA0BWwgNAVsIDQFbCA0BWwgNAVsIDQFbCA0BWwgNAVsIDQFbCA0BWwgNAVsIDQFb1tR4AhoK6urpajwCjhiseALISHgCyGlR4Ojs7Y8GCBdHW1haTJ0+OpUuXxqFDhyIi4v77749CoRBNTU39a/369RdlaACGr/MOz5dffhk33XRTzJs3L/bv3x9dXV1x+PDhuOuuuyIiYseOHbF+/fo4fvx4/7rnnnsu2uAADFPpPH3yySdp4cKF6eTJk/3bXnvttdTS0pK++uqrNHbs2PTRRx+d79OdplgspoiwLMuyhvkqFotn/X5/3lc8V155Zbz55ptxySWX9G/btGlTzJkzJzo7O+PEiROxYsWKmDRpUsycOTNWrVoVfX19Az5fuVyOUqlUsQAYBS7k6qSvry/95je/SePHj0+7du1Kb731VrrhhhvS1q1b09dff522b9+epkyZklavXj3gczz++OM1r7JlWZZV/XWuK55Bh6dYLKZbbrklXXbZZWnXrl0D7rd69eo0Z86cAR//6quvUrFY7F979+6t+cGyLMuy/vd1rvAM6h+Qdnd3x6JFi6K9vT127twZEydOjIiIzZs3x4EDB+Lee+/t37dcLse4ceMGfK5CoRCFQmEwLw/ASHC+VzpHjhxJ7e3tadmyZembb76peOzVV19N48aNS2+//Xbq6+tL27ZtSxMnTkwvvfTS+T69DxdYlmWNkFW1H7WtWbMmRURqaGhIjY2NFSullJ599tk0c+bM1NDQkGbMmJGeeeaZ846O8FiWZY2cda7w1KWUUgwBpVIpWltbaz0GAP+jYrEYLS0tAz7uljkAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQlPABkJTwAZCU8AGQ1ZMKTUqr1CABUwbm+nw+Z8PT09NR6BACq4Fzfz+vSELnU6Ovri3379kVzc3PU1dVVPFYqlWLatGmxd+/eaGlpqdGEtec4nOI4nOI4nOI4nDIUjkNKKXp6emLKlCkxZszA1zX1GWc6qzFjxsTUqVPPuk9LS8uoPrG+5Tic4jic4jic4jicUuvj0Nraes59hsyP2gAYHYQHgKyGRXgKhUI8/vjjUSgUaj1KTTkOpzgOpzgOpzgOpwyn4zBkPlwAwOgwLK54ABg5hAeArIQHgKyGfHi++OKLuPnmm2P8+PExceLEWL58eZw8ebLWY2W3cePGqK+vj6ampv51xx131HqsbA4ePBiXX355bN26tX/b9u3bY+7cudHU1BTTp0+PDRs21G7ATM50HO6///4oFAoV58b69etrN+RF1NnZGQsWLIi2traYPHlyLF26NA4dOhQRo+t8ONtxGBbnQxri5s+fn2677bb03//+N3V3d6errroqrV69utZjZffQQw+lZcuW1XqMmvjb3/6Wvv/976eISO+++25KKaUjR46ktra2tG7dunTixIn0zjvvpObm5rR9+/baDnsRnek4pJTSnDlz0gsvvFC7wTLp7e1N3/ve99KKFStSuVxOhw4dSosWLUo/+clPRtX5cLbjkNLwOB+GdHj27NmTIiJ9/vnn/dteeeWV1N7eXsOpauNHP/pRWrduXa3HyO6FF15I7e3t6ZVXXqn4hvvcc8+lK664omLf++67Ly1durQGU158Ax2Hr776Ko0dOzZ99NFHtR0wg08++SQtXLgwnTx5sn/ba6+9llpaWkbV+XC24zBczoch/aO2rq6uaGtriylTpvRvmzVrVnz66adx7Nix2g2WWV9fX3z44YfxxhtvxGWXXRZTp06Ne+65J44ePVrr0S66G2+8Mbq7u2PJkiUV27u6umL27NkV22bNmhWdnZ05x8tmoOPQ2dkZJ06ciBUrVsSkSZNi5syZsWrVqujr66vRpBfPlVdeGW+++WZccskl/ds2bdoUc+bMGVXnw9mOw3A5H4Z0eHp6eqKxsbFiW0NDQ0REHD9+vBYj1cTBgwfj6quvjsWLF8fHH38c27Ztiz179sTtt99e69EuusmTJ0d9/em3FBzo3Bip58VAx6FYLMb8+fPjgQceiM8++yxefvnlePrpp2PNmjU1mDKflFI89thj8frrr8dTTz016s6Hb333OAyX82HI3CT0TBobG6O3t7di27dfNzc312Kkmpg0aVK89957/V+3t7fH6tWrY+7cudHT0zOqjsW3GhsbT7vq7e3tHXXHYsGCBbFgwYL+r3/wgx/E8uXLY+PGjfHwww/XcLKLp1QqxV133RX/+Mc/4r333ovZs2ePyvPhTMdh9uzZw+J8GNJXPB0dHXH48OE4cOBA/7bdu3fH1KlTz+sOqCPFrl274pFHHqn4z5XK5XKMGTMmxo4dW8PJaqejoyO6uroqtu3evTs6OjpqNFFtbN68Of74xz9WbCuXyzFu3LgaTXRxdXd3x7XXXhulUil27tzZ/+O10XY+DHQchs35UOP3mM7p+uuvTz/72c9SqVRK//rXv9JVV12VHn/88VqPldXevXtTY2NjWrVqVTpx4kT6z3/+k374wx+mu+++u9ajZRX/5031Q4cOpfHjx6e1a9emr7/+Om3ZsiU1NzenLVu21HbIDP7vcXj11VfTuHHj0ttvv536+vrStm3b0sSJE9NLL71U2yEvgiNHjqT29va0bNmy9M0331Q8NprOh7Mdh+FyPgz58Ozfvz8tXrw4TZgwIV166aXpoYceqvg0x2ixdevWdN1116Xm5uZ06aWXpl//+tfpyy+/rPVYWcV3Pka8Y8eONG/evNTc3JxmzJiRnn/++ZrNltN3j8Ozzz6bZs6cmRoaGtKMGTPSM888U7vhLqI1a9akiEgNDQ2psbGxYqU0es6Hcx2H4XA+uEkoAFkN6fd4ABh5hAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKyEB4CshAeArIQHgKz+H3mvpO0hLrgVAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = train_ds['image'][0]\n",
    "ll = train_ds['label'][0]\n",
    "\n",
    "plt.imshow(im, cmap = 'gray')\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': Array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]], dtype=float32, weak_type=True), 'label': Array([0, 1, 0, ..., 8, 2, 0], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "perm = jax.random.permutation(jax.random.PRNGKey(0), len(train_ds['image']))\n",
    "batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "\n",
    "print(batch)\n",
    "del batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(*, logits, labels):\n",
    "  loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "# create train state\n",
    "def create_train_state(rng, learning_rate, batch_size, momentum = 0.9):\n",
    "  \"\"\"\n",
    "  Create a training state for Flax\n",
    "  \"\"\"\n",
    "\n",
    "  eic_model = EICNet()\n",
    "  params = eic_model.init(rng, jnp.ones((1, 16*16)))['params']\n",
    "  # opt = optax.sgd(learning_rate = learning_rate, momentum= momentum)\n",
    "  opt = optax.adam(learning_rate=learning_rate)\n",
    "  return train_state.TrainState.create(\n",
    "    apply_fn=eic_model.apply, params=params, tx=opt\n",
    "  )\n",
    "\n",
    "# create train state for conv net\n",
    "def create_train_state_conv(rng, learning_rate, batch_size, momentum = 0.9):\n",
    "  \"\"\"\n",
    "  Create a training state for Flax\n",
    "  \"\"\"\n",
    "\n",
    "  eic_model = EICConvNet()\n",
    "  params = eic_model.init(rng, jnp.ones([1, 28, 28, 1]))['params']\n",
    "  # opt = optax.sgd(learning_rate = learning_rate, momentum= momentum)\n",
    "  opt = optax.adam(learning_rate=learning_rate)\n",
    "  return train_state.TrainState.create(\n",
    "    apply_fn=eic_model.apply, params=params, tx=opt\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training step\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"\n",
    "    Train for single step\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = EICNet().apply({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits = logits, labels = batch['label'])\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux = True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    # print(jnp.linalg.norm(grads))\n",
    "\n",
    "    # for k, v in grads.items():\n",
    "    #     layer_norm = jax.tree_util.tree_map(lambda g: jnp.linalg.norm(g), v)\n",
    "    #     print(f\"Layer {k}, Gradient Norm: {layer_norm}\")\n",
    "\n",
    "\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits = logits, labels = batch['label'])\n",
    "    return state, metrics\n",
    "\n",
    "# create a training step\n",
    "@jax.jit\n",
    "def train_step_conv(state, batch):\n",
    "    \"\"\"\n",
    "    Train for single step\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = EICConvNet().apply({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits = logits, labels = batch['label'])\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux = True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    # print(jnp.linalg.norm(grads))\n",
    "\n",
    "    # for k, v in grads.items():\n",
    "    #     layer_norm = jax.tree_util.tree_map(lambda g: jnp.linalg.norm(g), v)\n",
    "    #     print(f\"Layer {k}, Gradient Norm: {layer_norm}\")\n",
    "\n",
    "\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits = logits, labels = batch['label'])\n",
    "    return state, metrics\n",
    "\n",
    "\n",
    "# evaluation step\n",
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "\n",
    "    logits = EICNet().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits = logits, labels = batch['label'])\n",
    "\n",
    "# evaluation step conv\n",
    "@jax.jit\n",
    "def eval_step_conv(params, batch):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "\n",
    "    logits = EICConvNet().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits = logits, labels = batch['label'])\n",
    "\n",
    "# evaluation step binary\n",
    "@jax.jit\n",
    "def eval_step_binary(params, batch):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "\n",
    "    logits = EICNetBinary().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits = logits, labels = batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \"\"\"\n",
    "    Trains for a single epoch\n",
    "    \"\"\"\n",
    "\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size] # drop incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    batch_metrics = []\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = {k : v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean across the batch\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]}\n",
    "\n",
    "    print(f\"Train epoch: {epoch}\")\n",
    "    print(f\"Training Accuracy: {epoch_metrics_np['accuracy']*100 :.02f} %, Loss: {epoch_metrics_np['loss'] :.02f}\")\n",
    "    # print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (\n",
    "    #     epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "    return state\n",
    "\n",
    "# train function conv\n",
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \"\"\"\n",
    "    Trains for a single epoch\n",
    "    \"\"\"\n",
    "\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size] # drop incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    batch_metrics = []\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = {k : v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step_conv(state, batch)\n",
    "\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean across the batch\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]}\n",
    "\n",
    "    print(f\"Train epoch: {epoch}\")\n",
    "    print(f\"Training Accuracy: {epoch_metrics_np['accuracy']*100 :.02f} %, Loss: {epoch_metrics_np['loss'] :.02f}\")\n",
    "    # print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (\n",
    "    #     epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval function\n",
    "def eval_model(params, val_ds):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "    metrics = eval_step(params, val_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree.map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']\n",
    "\n",
    "# eval function conv\n",
    "def eval_model_conv(params, val_ds):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "    metrics = eval_step_conv(params, val_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree.map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']\n",
    "\n",
    "# test accutacy\n",
    "def eval_test_conv(params, test_ds):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test set\n",
    "    \"\"\"\n",
    "    metrics = eval_step_conv(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree.map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']\n",
    "\n",
    "# test accutacy\n",
    "def eval_test(params, test_ds):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test set\n",
    "    \"\"\"\n",
    "    metrics = eval_step(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree.map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']\n",
    "\n",
    "# test accutacy\n",
    "def eval_test_binary(params, test_ds):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test set\n",
    "    \"\"\"\n",
    "    metrics = eval_step_binary(params, test_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree.map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "state = create_train_state(init_rng, learning_rate, batch_size)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10\n",
    "# batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38cc6061c18e4f7487d2038fdb1f07fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 256)\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "(64, 256)\n",
      "Train epoch: 1\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 2\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 3\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 4\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 5\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 6\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 7\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 8\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 9\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "Train epoch: 10\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "  # Use a separate PRNG key to permute image data during shuffling\n",
    "  rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "   # Evaluate on the test set after each training epoch \n",
    "  val_loss, val_accuracy = eval_model(state.params, val_ds)\n",
    "  print(f\"Validation Accuracy: {val_accuracy*100 :.02f} %, Loss: {val_loss :.02f}\")\n",
    "  \n",
    "  \n",
    "  # Run an optimization step over a training batch\n",
    "  state = train_epoch(state, train_ds, batch_size, epoch, input_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test set\n",
      "Accuracy: 9.800 %, Loss: 2.303\n"
     ]
    }
   ],
   "source": [
    "# using same relu network for evaluation\n",
    "test_loss, test_accuracy = eval_test(state.params, test_ds)\n",
    "\n",
    "print(f\"Test set\")\n",
    "print(f\"Accuracy: {test_accuracy*100 :.03f} %, Loss: {test_loss:.03f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScopeParamNotFoundError",
     "evalue": "Could not find parameter named \"weights\" in scope \"/ac1\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamNotFoundError)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mScopeParamNotFoundError\u001b[0m                   Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# use the binary network for evaluation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m test_loss, test_accuracy \u001b[38;5;241m=\u001b[39m \u001b[43meval_test_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAccuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.03f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.03f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[19], line 26\u001b[0m, in \u001b[0;36meval_test_binary\u001b[0;34m(params, test_ds)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_test_binary\u001b[39m(params, test_ds):\n\u001b[1;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;124;03m    Evaluate the model on test set\u001b[39;00m\n\u001b[1;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43meval_step_binary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_ds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mdevice_get(metrics)\n\u001b[1;32m     28\u001b[0m     summary \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mtree\u001b[38;5;241m.\u001b[39mmap(\u001b[38;5;28;01mlambda\u001b[39;00m x: x\u001b[38;5;241m.\u001b[39mitem(), metrics)\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[17], line 44\u001b[0m, in \u001b[0;36meval_step_binary\u001b[0;34m(params, batch)\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21meval_step_binary\u001b[39m(params, batch):\n\u001b[1;32m     40\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;124;03m    Evaluate the model\u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mEICNetBinary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mimage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m compute_metrics(logits \u001b[38;5;241m=\u001b[39m logits, labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "    \u001b[0;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[10], line 24\u001b[0m, in \u001b[0;36mEICNetBinary.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     22\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n\u001b[1;32m     23\u001b[0m x \u001b[38;5;241m=\u001b[39m custom_binary_gradient(x, threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, noise_sd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, key \u001b[38;5;241m=\u001b[39m subkey)\n\u001b[0;32m---> 24\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mac1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     25\u001b[0m key, subkey \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39msplit(key)\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m custom_binary_gradient(x, threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m, noise_sd \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.1\u001b[39m, key \u001b[38;5;241m=\u001b[39m subkey)\n",
      "    \u001b[0;31m[... skipping hidden 5 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[4], line 22\u001b[0m, in \u001b[0;36mAccumulator.setup\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msetup\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m    Set up the weights for the accumulator\u001b[39;00m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 22\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mW \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitializers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mxavier_normal\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_block_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_env/lib/python3.12/site-packages/flax/core/scope.py:974\u001b[0m, in \u001b[0;36mScope.param\u001b[0;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[1;32m    972\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_collection_empty(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    973\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeCollectionNotFound(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[0;32m--> 974\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamNotFoundError(name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text)\n\u001b[1;32m    975\u001b[0m value \u001b[38;5;241m=\u001b[39m init_fn(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_rng(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m), \u001b[38;5;241m*\u001b[39minit_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39minit_kwargs)\n\u001b[1;32m    976\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mput_variable(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m, name, value)\n",
      "\u001b[0;31mScopeParamNotFoundError\u001b[0m: Could not find parameter named \"weights\" in scope \"/ac1\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamNotFoundError)"
     ]
    }
   ],
   "source": [
    "# use the binary network for evaluation\n",
    "test_loss, test_accuracy = eval_test_binary(state.params, test_ds)\n",
    "\n",
    "print(f\"Test set\")\n",
    "print(f\"Accuracy: {test_accuracy*100 :.03f} %, Loss: {test_loss:.03f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv net test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 256)\n",
      "(1, 256)\n"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "batch_size = 64\n",
    "state = create_train_state_conv(init_rng, learning_rate, batch_size)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "008e4b54b4bb4003b45bc433d399d64b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 9.59 %, Loss: 2.30\n",
      "(64, 256)\n",
      "(64, 256)\n",
      "Train epoch: 1\n",
      "Training Accuracy: 9.93 %, Loss: 2.30\n",
      "Validation Accuracy: 9.59 %, Loss: 2.30\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[93], line 11\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mValidation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m100\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.02f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m %, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_loss\u001b[38;5;250m \u001b[39m\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.02f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Run an optimization step over a training batch\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_ds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_rng\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[77], line 52\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(state, train_ds, batch_size, epoch, rng)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m perm \u001b[38;5;129;01min\u001b[39;00m perms:\n\u001b[1;32m     51\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {k : v[perm, \u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m train_ds\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m---> 52\u001b[0m     state, metrics \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step_conv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     54\u001b[0m     batch_metrics\u001b[38;5;241m.\u001b[39mappend(metrics)\n\u001b[1;32m     56\u001b[0m \u001b[38;5;66;03m# compute mean across the batch\u001b[39;00m\n",
      "File \u001b[0;32m<string>:1\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(_cls, count, mu, nu)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "  # Use a separate PRNG key to permute image data during shuffling\n",
    "  rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "   # Evaluate on the test set after each training epoch \n",
    "  val_loss, val_accuracy = eval_model_conv(state.params, val_ds)\n",
    "  print(f\"Validation Accuracy: {val_accuracy*100 :.02f} %, Loss: {val_loss :.02f}\")\n",
    "  \n",
    "  \n",
    "  # Run an optimization step over a training batch\n",
    "  state = train_epoch(state, train_ds, batch_size, epoch, input_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from flax import linen as nn\n",
    "from EICDense import EICDense\n",
    "from ShuffleBlocks import ShuffleBlocks\n",
    "from Accumulator import Accumulator\n",
    "\n",
    "from mnist_dataloader import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EICDense(nn.Module):\n",
    "    in_size: int\n",
    "    out_size: int\n",
    "    threshold: float\n",
    "    noise_sd: float\n",
    "    activation: callable\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up dependent parameters.\n",
    "        \"\"\"\n",
    "        # Ensure valid in_size and out_size\n",
    "        assert self.in_size > 0, f\"Invalid in_size: {self.in_size}\"\n",
    "        assert self.out_size > 0, f\"Invalid out_size: {self.out_size}\"\n",
    "\n",
    "        # Calculate block counts with a minimum of 1\n",
    "        self.in_blocks = max(self.in_size // 256, 1)\n",
    "        self.out_blocks = max(self.out_size // 256, 1)\n",
    "\n",
    "        # Total cores required\n",
    "        self.num_cores = self.out_blocks * self.in_blocks\n",
    "\n",
    "        # Initialize weights\n",
    "        self.W = self.param(\n",
    "            \"weights\",\n",
    "            lambda key, shape: nn.initializers.xavier_normal()(key, shape),\n",
    "            (self.out_blocks, self.in_blocks, 256, 256),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \n",
    "        assert x.shape[-1] == self.in_size, f\"Input shape is incorrect. Got {x.shape}, expected (batch_size, {self.in_size}).\"\n",
    "\n",
    "        # Reshape input into blocks\n",
    "        x_reshaped = x.reshape((x.shape[0], self.in_blocks, 256))\n",
    "\n",
    "        # Apply weights\n",
    "        W_pos = jax.nn.silu(self.W)\n",
    "        #Block-wise matrix multiplication followed by summation over the input block index\n",
    "        y = jnp.einsum(\"ijkl,bjl->bik\", W_pos, x_reshaped)\n",
    "\n",
    "        # Apply activation\n",
    "        return self.activation(y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Accumulator(nn.Module):\n",
    "    in_block_size: int\n",
    "    threshold: float\n",
    "    noise_sd: float\n",
    "    activation: callable = None\n",
    "\n",
    "    def setup(self):\n",
    "        self.W = self.param(\n",
    "            \"weights\",\n",
    "            nn.initializers.xavier_normal(),\n",
    "            (256, 256),  \n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        #print(f\"x.shape in Accumulator: {x.shape}, in_block_size: {self.in_block_size}\")\n",
    "\n",
    "        # Ensure weights are positive\n",
    "        W_pos = jax.nn.silu(self.W)\n",
    "\n",
    "        \n",
    "        #Matrix multiplication between 256 dimension vectors & weights\n",
    "        y = jnp.einsum(\"bij,jk->bik\", x, W_pos)\n",
    "        #print(f\"Shape after einsum in Accumulator: {y.shape}\")\n",
    "\n",
    "        # Flatten the output\n",
    "        y = y.reshape((y.shape[0], -1))  # Combine blocks\n",
    "        #print(f\"Shape after flattening in Accumulator: {y.shape}\")\n",
    "\n",
    "        return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ShuffleBlocks(nn.Module):\n",
    "    subvector_len: int\n",
    "    slot_len: int\n",
    "    key: jax.random.PRNGKey\n",
    "\n",
    "    def __call__(self, x):\n",
    "        #print(f\"Input shape to ShuffleBlocks: {x.shape}\")\n",
    "\n",
    "        batch_size = x.shape[0]\n",
    "        feature_size = x.shape[1]\n",
    "\n",
    "        # Calculate number of subvectors\n",
    "        num_subvectors = feature_size // self.subvector_len\n",
    "        if num_subvectors == 0:\n",
    "            pad_size = self.subvector_len - feature_size\n",
    "            #print(f\"Padding input by {pad_size} to match subvector_len.\")\n",
    "            x = jnp.pad(x, ((0, 0), (0, pad_size)))\n",
    "            num_subvectors = 1\n",
    "\n",
    "        slots_per_input = self.subvector_len // self.slot_len\n",
    "\n",
    "        # Reshape input into blocks\n",
    "        x_reshaped = x.reshape((batch_size, num_subvectors, slots_per_input, self.slot_len))\n",
    "        #print(f\"x_reshaped shape: {x_reshaped.shape}\")\n",
    "\n",
    "        # Shuffle blocks\n",
    "        key, subkey = jax.random.split(self.key)\n",
    "        keys = jax.random.split(key, num_subvectors)\n",
    "\n",
    "        shuffled_blocks = [\n",
    "            x_reshaped[:, i, jax.random.permutation(keys[i], slots_per_input, independent=True)]\n",
    "            for i in range(num_subvectors)\n",
    "        ]\n",
    "\n",
    "        x_shuffled = jnp.concatenate([blocks.reshape(batch_size, -1) for blocks in shuffled_blocks], axis=1)\n",
    "\n",
    "        # Ensure output matches the original feature size\n",
    "        if x_shuffled.shape[1] != feature_size:\n",
    "            x_shuffled = x_shuffled[:, :feature_size]  \n",
    "        #print(f\"x_shuffled shape: {x_shuffled.shape}\")\n",
    "\n",
    "        return x_shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PseudoFFNet(nn.Module):\n",
    "    activation_fn: callable\n",
    "\n",
    "    def setup(self):\n",
    "        self.fc1 = EICDense(in_size=1024, out_size=2048, threshold=0.0, noise_sd=0.1, activation=self.activation_fn)\n",
    "        self.ac1 = Accumulator(in_block_size=8, threshold=0.0, noise_sd=0.1, activation=self.activation_fn)\n",
    "        self.shuffle1 = ShuffleBlocks(subvector_len=256, slot_len=64, key=jax.random.PRNGKey(0))\n",
    "\n",
    "        self.fc2 = EICDense(in_size=2048, out_size=256, threshold=0.0, noise_sd=0.1, activation=self.activation_fn)\n",
    "        self.ac2 = Accumulator(in_block_size=1, threshold=0.0, noise_sd=0.1, activation=self.activation_fn)\n",
    "        self.shuffle2 = ShuffleBlocks(subvector_len=256, slot_len=64, key=jax.random.PRNGKey(1))\n",
    "\n",
    "        self.fc3 = EICDense(in_size=256, out_size=10, threshold=0.0, noise_sd=0.1, activation=self.activation_fn)\n",
    "        self.ac3 = Accumulator(in_block_size=1, threshold=0.0, noise_sd=0.1, activation=None)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        #print(f\"Input shape: {x.shape}\")\n",
    "\n",
    "        # Layer 1\n",
    "        x = self.fc1(x)\n",
    "        #print(f\"Shape after fc1: {x.shape}\")\n",
    "        x = x.reshape((x.shape[0], -1, 256))\n",
    "        x = self.ac1(x)\n",
    "        #print(f\"Shape after ac1: {x.shape}\")\n",
    "        x = self.shuffle1(x)\n",
    "        #print(f\"Shape after shuffle1: {x.shape}\")\n",
    "\n",
    "        # Layer 2\n",
    "        x = x.reshape((x.shape[0], 2048))  \n",
    "        x = self.fc2(x)\n",
    "        #print(f\"Shape after fc2: {x.shape}\")\n",
    "        x = x.reshape((x.shape[0], -1, 256))\n",
    "        x = self.ac2(x)\n",
    "        #print(f\"Shape after ac2: {x.shape}\")\n",
    "        x = self.shuffle2(x)\n",
    "        #print(f\"Shape after shuffle2: {x.shape}\")\n",
    "\n",
    "        # Layer 3\n",
    "        x = self.fc3(x)\n",
    "        #print(f\"Shape after fc3: {x.shape}\")\n",
    "        x = self.ac3(x)\n",
    "        #print(f\"Final output shape: {x.shape}\")\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "# Initialize model parameters and optimizer\n",
    "image_size =(32,32)\n",
    "activation_fn = jax.nn.silu\n",
    "data_path = '/Users/rairo/trident data'\n",
    "\n",
    "# Load MNIST data\n",
    "print(\"Preparing the MNIST dataset...\")\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_and_process_mnist(data_path, binarize=True, input_size=image_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 782batch [03:34,  3.65batch/s, Batch Loss=0.077030614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Validation Accuracy: 94.23%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 782batch [03:53,  3.36batch/s, Batch Loss=0.0044169524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2 completed. Validation Accuracy: 96.15%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 782batch [03:50,  3.40batch/s, Batch Loss=0.22260378]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 completed. Validation Accuracy: 96.55%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 782batch [03:59,  3.26batch/s, Batch Loss=0.00039530403]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4 completed. Validation Accuracy: 96.66%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 782batch [02:54,  4.49batch/s, Batch Loss=0.0014377253] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 completed. Validation Accuracy: 96.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 782batch [02:28,  5.28batch/s, Batch Loss=6.0852253e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 completed. Validation Accuracy: 96.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 782batch [02:33,  5.10batch/s, Batch Loss=1.9750762e-05]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7 completed. Validation Accuracy: 97.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 782batch [02:58,  4.38batch/s, Batch Loss=0.0036040063] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 completed. Validation Accuracy: 97.37%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 782batch [02:56,  4.42batch/s, Batch Loss=0.00031952775]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9 completed. Validation Accuracy: 96.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 782batch [02:28,  5.25batch/s, Batch Loss=0.00052177964] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 completed. Validation Accuracy: 97.15%\n",
      "Final Test Accuracy: 97.44%\n",
      "Test Accuracy after Quantization: 97.47%\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def prune_weights(params, threshold=0.01):\n",
    "    return jax.tree_map(lambda p: jnp.where(jnp.abs(p) > threshold, p, 0), params)\n",
    "\n",
    "# Training state\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "# Create training state\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    params = model.init(rng, jnp.ones([64, input_size]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "# Training step\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch[\"images\"])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[\"labels\"]).mean()\n",
    "        return loss\n",
    "\n",
    "    # Compute gradients and loss\n",
    "    loss, grads = jax.value_and_grad(loss_fn)(state.params)\n",
    "\n",
    "    # Apply gradients to update state\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state, loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(state, images, labels):\n",
    "    logits = state.apply_fn({'params': state.params}, images)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == labels)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Training loop \n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "train_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "# Initialize model, RNG, and training state\n",
    "rng = jax.random.PRNGKey(0)\n",
    "input_size = 1024\n",
    "model = PseudoFFNet(activation_fn=jax.nn.silu)\n",
    "state = create_train_state(rng, model, learning_rate=0.001)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    epoch_loss = 0\n",
    "    num_batches = 0\n",
    "\n",
    "    # Progress bar for each epoch\n",
    "    with tqdm(get_train_batches(train_images, train_labels, batch_size), desc=f\"Epoch {epoch+1}/{num_epochs}\", unit=\"batch\") as pbar:\n",
    "        for images, labels in pbar:\n",
    "            batch = {\"images\": images, \"labels\": labels}\n",
    "            state, loss = train_step(state, batch)\n",
    "            epoch_loss += loss\n",
    "            num_batches += 1\n",
    "            pbar.set_postfix({\"Batch Loss\": loss})\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    epoch_loss /= num_batches\n",
    "    train_losses.append(epoch_loss)\n",
    "\n",
    "    # Validation accuracy\n",
    "    val_accuracy = evaluate(state, val_images, val_labels)\n",
    "    val_accuracies.append(val_accuracy)\n",
    "    print(f\"Epoch {epoch + 1} completed. Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test evaluation after training\n",
    "test_accuracy = evaluate(state, test_images, test_labels)\n",
    "print(f\"Final Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Post-training quantization\n",
    "def quantize_params(params, num_bits=8):\n",
    "    scale = 2 ** (num_bits - 1) - 1\n",
    "    quantized_params = jax.tree_map(lambda p: jnp.round(p * scale) / scale, params)\n",
    "    return quantized_params\n",
    "\n",
    "quantized_params = quantize_params(state.params)\n",
    "test_accuracy_quantized = evaluate(state.replace(params=quantized_params), test_images, test_labels)\n",
    "print(f\"Test Accuracy after Quantization: {test_accuracy_quantized:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* EICDense 1: For each (i,j) block pair, matrix multiplication between the 256×256 weight block W_pos[i,j,::] and the corresponding input blockx_reshaped[:,j,:] & sum the results across all input blocks (j) for each output block (i).\n",
    "* for Accumulator 1: For each input block (i),matrix multiplication between x[:,i,:] (256-dimensional vector) and W_pos (256×256). ==> new 256-dimensional output for each block\n",
    "* EICDense 2: Matrix multiplication between each block of W_pos and corresponding input blocks & sum over the input block index (j)\n",
    "* Accumulator 2: Multiplyx[:,0,:] (256-dimensional) with W_pos\n",
    "* EICDense 3: Matrix multiplication for the single block of weights W_pos[0,0,:,:] & the input x[:,0,:].\n",
    "* Accumulator 3: Multiply x[:,0,:] with W_pos, accumulating the final output. \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ep1cs1m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

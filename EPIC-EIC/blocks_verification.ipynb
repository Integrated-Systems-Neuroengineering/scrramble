{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verify the working of the blocks\n",
    "- Copy over all the blocks.\n",
    "- Simulate them imdependently.\n",
    "- Vefity `vmap`\n",
    "- Verify activation.\n",
    "- Implement quantization.\n",
    "- "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "# from EICDense import *\n",
    "# from ShuffleBlock import *\n",
    "# from Accumulator import *\n",
    "# from PseudoFFNet import *\n",
    "# from EICNet import *\n",
    "from HelperFunctions.binary_trident_helper_functions import *\n",
    "from HelperFunctions.binary_mnist_dataloader import *\n",
    "from HelperFunctions.metric_functions import *\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weights': Array([ 1.000e-03,  2.345e-01,  2.100e-01, -9.000e-01,  1.320e+00,\n",
      "       -7.234e+00], dtype=float32)}\n",
      "{'weights': Array([ 0.        ,  0.23622048,  0.21259843, -0.8976378 ,  1.3228346 ,\n",
      "       -7.2362204 ], dtype=float32)}\n"
     ]
    }
   ],
   "source": [
    "def quantize_params(params, bits = 8):\n",
    "    \"\"\"\n",
    "    Quantizes the parameters of the model to given number of bits.\n",
    "    Args:\n",
    "        params: flax model parameters\n",
    "        bits: number of bits to quantize to\n",
    "    Returns:\n",
    "        quantized_params: quantized flax model parameters\n",
    "    \"\"\"\n",
    "\n",
    "    scale = 2**(bits - 1) - 1\n",
    "    params = jax.tree.map(\n",
    "        lambda p: jnp.round(p * scale) / scale, params\n",
    "    )\n",
    "\n",
    "    return params\n",
    "\n",
    "## testing\n",
    "params = {\n",
    "    'weights': jnp.array([0.001, 0.2345, 0.21, -0.9, 1.32, -7.234])\n",
    "}\n",
    "print(params)\n",
    "quantized_params = quantize_params(params, bits = 8)\n",
    "print(quantized_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EICDense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EICDense(nn.Module):\n",
    "    \"\"\"\n",
    "    Pseudo-dense layer using EIC Cores.\n",
    "    Args:\n",
    "    in_size: int, number of input neurons\n",
    "    out_size: int, number of output neurons\n",
    "    threshold: float, threshold for binary activation\n",
    "    noise_sd: flaat, standard deviation of noise for binary activation\n",
    "    key: jax.random.PRNGKey, random key\n",
    "\n",
    "    Returns:\n",
    "    x: jnp.ndarray, output of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    in_size: int\n",
    "    out_size: int\n",
    "    threshold: float\n",
    "    noise_sd: float\n",
    "    # activation: callable\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up dependent parameters\n",
    "        \"\"\"\n",
    "        self.out_blocks = max(self.out_size//256, 1) # number of blocks required at the output \n",
    "        self.in_blocks = max(self.in_size//256, 1) # number of bloacks required at the input\n",
    "\n",
    "\n",
    "        self.num_cores = self.out_blocks * self.in_blocks # number of cores required\n",
    "        self.W = self.param(\n",
    "            \"weights\",\n",
    "            lambda key, shape: nn.initializers.xavier_normal()(key, shape),\n",
    "            (self.out_blocks, self.in_blocks, 256, 256)\n",
    "        )\n",
    "\n",
    "\n",
    "    # @staticmethod\n",
    "    # def linear_map(x, threshold = 0., noise_sd = 0.1, key = None):\n",
    "    #     \"\"\"\n",
    "    #     Linear map\n",
    "    #     \"\"\"\n",
    "    #     return x\n",
    "\n",
    "    # @staticmethod\n",
    "    # def sigmoid_fn(x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0)):\n",
    "    #     \"\"\"\n",
    "    #     Simple sigmoid.\n",
    "    #     \"\"\"\n",
    "    #     return jax.nn.sigmoid(x)\n",
    "    \n",
    "    # def relu_fn(self, x, threshold = 0.0, noise_sd = 0.1, key = jax.random.key(0)):\n",
    "    #     \"\"\"\n",
    "    #     Simple ReLU.\n",
    "    #     \"\"\"\n",
    "    #     return jax.nn.relu(x)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the layer\n",
    "        Args:\n",
    "        x: jnp.ndarray (batch_size, in_size), input to the layer\n",
    "        \n",
    "        Returns:\n",
    "        x: jnp.ndarray, output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[-1] == self.in_size, f\"Input shape is incorrect. Got {x.shape[-1]}, expected {self.in_size}\"\n",
    "\n",
    "        x_reshaped = x.reshape(x.shape[0], self.in_blocks, 256) # organize x into blocks of 256 for every batch\n",
    "\n",
    "        # make sure that the weights are positive\n",
    "        W_pos= jax.nn.softplus(self.W)\n",
    "\n",
    "        # quantize weights\n",
    "        W_pos = quantize_params(W_pos, bits = 8)\n",
    "\n",
    "        y = jnp.einsum(\"ijkl,bjl->bijk\", W_pos, x_reshaped)\n",
    "\n",
    "        # activation_fn = self.activation if self.activation is not None else self.linear_map\n",
    " \n",
    "        # key = self.make_rng(\"activation\")\n",
    "        # y = activation_fn(y, threshold = self.threshold, noise_sd = self.noise_sd, key = key)\n",
    "\n",
    "        return y\n",
    "    \n",
    "    \n",
    "# testing...\n",
    "\n",
    "# rng = jax.random.key(0)\n",
    "# key, subkey = jax.random.split(rng)\n",
    "# x = jax.random.normal(key, (1024,))\n",
    "# eic = EICDense(in_size = 1024, out_size = 2048, threshold=0., activation = custom_binary_gradient, noise_sd = 0.1)\n",
    "# params_eic = eic.init(key, x)\n",
    "# print(\"Initialized EICDense parameters\")\n",
    "# print(f\"Params: {params_eic}\")\n",
    "# print(f\"Params shape: {params_eic['params']['weights'].shape}\")\n",
    "# y = eic.apply(params_eic, x, rngs = {\"activation\": subkey})\n",
    "# print(f\"Output: {y}\")\n",
    "# print(f\"Output shape: {y.shape}\")\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accumulator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the accumulator module\n",
    "class Accumulator(nn.Module):\n",
    "    \"\"\"\n",
    "    Accumulating the EICDense outputs. \n",
    "    Since the EICDense generates pseudo-feedforward outputs, we use a learnable accumulation matrix that minimizes error\n",
    "    between the true feedforward output and the EIC output.\n",
    "\n",
    "    Args:\n",
    "        in_block_size: int, number of 256-sized blocks. This should be the .shape[0] of the EICDense output\n",
    "    \"\"\"\n",
    "\n",
    "    in_block_size: int\n",
    "    threshold: float\n",
    "    noise_sd: float\n",
    "    # activation: callable = None\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up the weights for the accumulator\n",
    "        \"\"\"\n",
    "\n",
    "        self.W = self.param(\n",
    "            \"weights\",\n",
    "            nn.initializers.xavier_normal(),\n",
    "            (self.in_block_size, 256, 256)\n",
    "        )\n",
    "\n",
    "    @staticmethod\n",
    "    def linear_map(x, threshold = 0., noise_sd = 0.1, key = None):\n",
    "        \"\"\"\n",
    "        Linear map\n",
    "        \"\"\"\n",
    "        return x\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the accumulator\n",
    "        Args:\n",
    "        x: jnp.ndarray, input to the accumulator\n",
    "        \n",
    "        Returns:\n",
    "        x: jnp.ndarray, output of the accumulator\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[1] == self.in_block_size, \"Input shape is incorrect\"\n",
    "        # assert x.shape[1] == self.out_block_size, \"Input shape is incorrect\"\n",
    "\n",
    "        # ensure positive \n",
    "        W_pos = jax.nn.softplus(self.W)\n",
    "        W_pos = quantize_params(W_pos, bits = 8)\n",
    "        \n",
    "        x = jnp.einsum(\"bijk->bik\", x)\n",
    "        y = jnp.einsum(\"ijk,bik->bik\", W_pos, x) \n",
    "\n",
    "\n",
    "        # y = jnp.einsum('ijk,imk->ij', W_pos, x) #jnp.einsum('imk,ijk->im', W_pos, x) #\n",
    "        # key = self.make_rng(\"activation\")\n",
    "\n",
    "        # activation_fn = self.activation if self.activation is not None else self.linear_map\n",
    "        # y = activation_fn(y, threshold = self.threshold, noise_sd = self.noise_sd, key = key)\n",
    "\n",
    "        # flatten y before returning\n",
    "        y = y.reshape((y.shape[0], -1)) # (batch_size, out_size)\n",
    "\n",
    "        return y\n",
    "\n",
    "# testing...\n",
    "# def __main__():\n",
    "#     rng = jax.random.key(42)\n",
    "#     x = jax.random.normal(rng, (8, 4, 256))\n",
    "#     acc = Accumulator(\n",
    "#         in_block_size = x.shape[0],\n",
    "#         threshold = 0.0,\n",
    "#         noise_sd = 1.0,\n",
    "#         activation = custom_binary_gradient\n",
    "#     )\n",
    "\n",
    "#     params = acc.init(rng, x)\n",
    "#     print(\"Initialized accumulator parameters\")\n",
    "#     print(\"----------------------------------------\")\n",
    "#     print(f\"Output shape: {params[\"params\"][\"weights\"].shape}\")\n",
    "#     y = acc.apply(params, x, rngs = {\"activation\": rng})\n",
    "#     print(\"----------------------------------------\")\n",
    "#     print(f\"Accumulator output {y}, \\n Output shape: {y.shape}\")\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     __main__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffle Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output shape: (10, 256)\n",
      "Output: [[ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " ...\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]\n",
      " [ 1.1920929e-07  4.1723251e-07  5.9604645e-08 ...  1.1920929e-07\n",
      "  -2.9802322e-07 -5.9604645e-08]]\n"
     ]
    }
   ],
   "source": [
    "## define a module to split the input and shuffle it in blocks of 64\n",
    "\n",
    "class ShuffleBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Defines a trainable permutation matrix with a temperature parameter.\n",
    "    This is a proxy for the lookup table. In some sense it describes an idealized LUT.\n",
    "    Temperature parameter can be very loosely interpreted as \"release probability\"\n",
    "\n",
    "    Args:\n",
    "        input_size: int, size of the input (e.g. 2048)\n",
    "    Defines:\n",
    "        A: jnp.ndarray, trainable permutation matrix\n",
    "        tau: float, temperature parameter\n",
    "    Returns:\n",
    "        y: jnp.ndarray, shuffled input. Basically, y = Ax\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    tau: float \n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up trainable permutation matrix\n",
    "        \"\"\"\n",
    "\n",
    "        self.Zpos = self.param(\n",
    "            'Zpos',\n",
    "            nn.initializers.xavier_normal(),\n",
    "            (self.input_size, self.input_size)\n",
    "        )\n",
    "\n",
    "        self.Zneg = self.param(\n",
    "            'Zneg',\n",
    "            nn.initializers.xavier_normal(),\n",
    "            (self.input_size, self.input_size)\n",
    "        )\n",
    "\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Soft shuffle the input\n",
    "        \"\"\"\n",
    "\n",
    "        Ppos = jax.nn.softmax(self.Zpos/self.tau, axis = -1)\n",
    "        Pneg = jax.nn.softmax(self.Zneg/self.tau, axis = -1)\n",
    "\n",
    "        xpos = jnp.einsum('ij,bj->bi', Ppos, x)\n",
    "        xneg = jnp.einsum('ij,bj->bi', Pneg, x)\n",
    "\n",
    "        # P = jax.nn.softmax(self.Z/self.tau, axis = -1) * jnp.sign(self.Z)\n",
    "        y = xpos - xneg\n",
    "\n",
    "        return y\n",
    "    \n",
    "# testing...\n",
    "sh1 = ShuffleBlock(input_size = 256, tau = 0.5)\n",
    "key = jax.random.key(1)\n",
    "x = jnp.ones((10, 256))\n",
    "params = sh1.init(key, x)\n",
    "out = sh1.apply(params, x)\n",
    "\n",
    "print(f\"Output shape: {out.shape}\")\n",
    "print(f\"Output: {out}\")\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EICNet(nn.Module):\n",
    "    \"\"\"\n",
    "    EICNetwork -> Might supersede PseudoFFNet... in progress\n",
    "    1024 - 2048 - 256 - 10\n",
    "    \"\"\"\n",
    "    def setup(self):\n",
    "        nsd = 0.1\n",
    "        temp = 0.9\n",
    "        self.sh1 = ShuffleBlock(input_size = 28*28, tau = temp)\n",
    "        self.fc1 = EICDense(in_size = 28*28, out_size = 2048, threshold = 0.0, noise_sd = nsd) # (8, 4, 256)\n",
    "        self.ac1 = Accumulator(in_block_size = 2048//256, threshold = 0., noise_sd = nsd) # (2048,)\n",
    "        self.sh2 = ShuffleBlock(input_size = 2048, tau = temp)\n",
    "        self.fc2 = EICDense(in_size = 2048, out_size = 256, threshold = 0.0, noise_sd = nsd) # (8, 4, 256)\n",
    "        self.ac2 = Accumulator(in_block_size = 256//256, threshold = 0., noise_sd = nsd) # (2048,)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape((x.shape[0], 256))\n",
    "        # x = self.sh1(x)\n",
    "        x = self.fc1(x)\n",
    "        x = jax.nn.relu(x)#custom_binary_gradient(x, threshold = 0.0, noise_sd = 1.0, key = jax.random.key(0))\n",
    "        x = self.ac1(x)\n",
    "        x = jax.nn.relu(x)#custom_binary_gradient(x, threshold = 0.0, noise_sd = 1.0, key = jax.random.key(1))\n",
    "        x = self.sh2(x)\n",
    "        x = self.fc2(x)\n",
    "        x = jax.nn.relu(x)#custom_binary_gradient(x, threshold = 0.0, noise_sd = 1.0, key = jax.random.key(2))\n",
    "        x = self.ac2(x)\n",
    "        # x = x[:, :10]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.690805   -0.48744103 -1.155789    0.12108463 -0.19598432 -0.5078766\n",
      "  0.91568655  1.70968    -0.36749417  0.14315689] 7\n",
      "2.5878873\n"
     ]
    }
   ],
   "source": [
    "# some additional functions\n",
    "def cross_entropy_loss(*, logits, labels):\n",
    "    one_hot_labels = jax.nn.one_hot(labels, num_classes = 10)\n",
    "    loss = optax.softmax_cross_entropy(logits = logits, labels = one_hot_labels).mean()\n",
    "    return loss\n",
    "\n",
    "# testing...\n",
    "key = jax.random.key(1)\n",
    "logits = jax.random.normal(key, (10,))\n",
    "print(logits, jnp.argmax(logits))\n",
    "label = 9\n",
    "loss = cross_entropy_loss(logits = logits, labels = label)\n",
    "\n",
    "print(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "DATA_PATH = f\"/Users/vikrantjaltare/OneDrive - UC San Diego/Datasets/\"\n",
    "# (train_inputs, train_labels), (val_inputs, val_labels), (test_inputs, test_labels) = load_and_process_mnist(data_path=DATA_PATH, binarize = True)\n",
    "# train_inputs[0].shape\n",
    "\n",
    "def get_datasets():\n",
    "    \"\"\"Load MNIST train, validation, and test datasets into memory.\"\"\"\n",
    "    # Load MNIST dataset\n",
    "    ds_builder = tfds.builder('mnist', data_dir=DATA_PATH)\n",
    "    ds_builder.download_and_prepare()\n",
    "    \n",
    "    # Load full datasets as numpy arrays\n",
    "    full_train_ds = tfds.as_numpy(ds_builder.as_dataset(split='train', batch_size=-1))\n",
    "    full_test_ds = tfds.as_numpy(ds_builder.as_dataset(split='test', batch_size=-1))\n",
    "\n",
    "    # Normalize images\n",
    "    full_train_ds['image'] = jnp.float32(full_train_ds['image']) / 255.\n",
    "    full_test_ds['image'] = jnp.float32(full_test_ds['image']) / 255.\n",
    "\n",
    "    # Concatenate train and test datasets for splitting\n",
    "    all_images = jnp.concatenate([full_train_ds['image'], full_test_ds['image']], axis=0)\n",
    "    all_labels = jnp.concatenate([full_train_ds['label'], full_test_ds['label']], axis=0)\n",
    "\n",
    "    # Train-validation-test split: 50k-10k-10k\n",
    "    train_images, val_images, test_images = all_images[:50000], all_images[50000:60000], all_images[60000:]\n",
    "    train_labels, val_labels, test_labels = all_labels[:50000], all_labels[50000:60000], all_labels[60000:]\n",
    "\n",
    "    # Resize images to 16x16\n",
    "    def resize_images(images, new_size):\n",
    "        resized = tf.image.resize(images, new_size, method='bilinear')\n",
    "        return jnp.array(resized)\n",
    "\n",
    "    # train_images = resize_images(train_images, (16, 16))\n",
    "    # val_images = resize_images(val_images, (16, 16))\n",
    "    # test_images = resize_images(test_images, (16, 16))\n",
    "\n",
    "    # Binarize images\n",
    "    def binarize_images(images, threshold=0.5):\n",
    "        return jnp.where(images < threshold, 0.0, 1.0)\n",
    "\n",
    "    train_images = binarize_images(train_images)\n",
    "    val_images = binarize_images(val_images)\n",
    "    test_images = binarize_images(test_images)\n",
    "\n",
    "    # Prepare datasets in similar structure as original\n",
    "    train_ds = {'image': train_images, 'label': train_labels}\n",
    "    val_ds = {'image': val_images, 'label': val_labels}\n",
    "    test_ds = {'image': test_images, 'label': test_labels}\n",
    "\n",
    "    return train_ds, val_ds, test_ds\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 50000\n",
      "Validation dataset size: 10000\n",
      "Test dataset size: 10000\n",
      "Image shape: (28, 28, 1)\n"
     ]
    }
   ],
   "source": [
    "train_ds, val_ds, test_ds = get_datasets()\n",
    "\n",
    "print(f\"Train dataset size: {len(train_ds['image'])}\")\n",
    "print(f\"Validation dataset size: {len(val_ds['image'])}\")\n",
    "print(f\"Test dataset size: {len(test_ds['image'])}\")\n",
    "print(f\"Image shape: {train_ds['image'][0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "im = train_ds['image'][0]\n",
    "ll = train_ds['label'][0]\n",
    "\n",
    "plt.imshow(im, cmap = 'gray')\n",
    "print(ll)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image': Array([[[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       ...,\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]],\n",
      "\n",
      "\n",
      "       [[[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]],\n",
      "\n",
      "        [[0.],\n",
      "         [0.],\n",
      "         [0.],\n",
      "         ...,\n",
      "         [0.],\n",
      "         [0.],\n",
      "         [0.]]]], dtype=float32, weak_type=True), 'label': Array([0, 1, 0, ..., 8, 2, 0], dtype=int32)}\n"
     ]
    }
   ],
   "source": [
    "perm = jax.random.permutation(jax.random.PRNGKey(0), len(train_ds['image']))\n",
    "batch = {k: v[perm, ...] for k, v in train_ds.items()}\n",
    "\n",
    "print(batch)\n",
    "del batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(*, logits, labels):\n",
    "  loss = cross_entropy_loss(logits=logits, labels=labels)\n",
    "  accuracy = jnp.mean(jnp.argmax(logits, -1) == labels)\n",
    "  metrics = {\n",
    "      'loss': loss,\n",
    "      'accuracy': accuracy,\n",
    "  }\n",
    "  return metrics\n",
    "\n",
    "# create train state\n",
    "def create_train_state(rng, learning_rate, batch_size, momentum = 0.9):\n",
    "  \"\"\"\n",
    "  Create a training state for Flax\n",
    "  \"\"\"\n",
    "\n",
    "  eic_model = EICNet()\n",
    "  params = eic_model.init(rng, jnp.ones((batch_size, 28*28)))['params']\n",
    "  opt = optax.sgd(learning_rate = learning_rate, momentum= momentum)\n",
    "  return train_state.TrainState.create(\n",
    "    apply_fn=eic_model.apply, params=params, tx=opt\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a training step\n",
    "@jax.jit\n",
    "def train_step(state, batch):\n",
    "    \"\"\"\n",
    "    Train for single step\n",
    "    \"\"\"\n",
    "\n",
    "    def loss_fn(params):\n",
    "        logits = EICNet().apply({'params': params}, batch['image'])\n",
    "        loss = cross_entropy_loss(logits = logits, labels = batch['label'])\n",
    "        return loss, logits\n",
    "    \n",
    "    grad_fn = jax.value_and_grad(loss_fn, has_aux = True)\n",
    "    (_, logits), grads = grad_fn(state.params)\n",
    "    # print(jnp.linalg.norm(grads))\n",
    "\n",
    "    # for k, v in grads.items():\n",
    "    #     layer_norm = jax.tree_util.tree_map(lambda g: jnp.linalg.norm(g), v)\n",
    "    #     print(f\"Layer {k}, Gradient Norm: {layer_norm}\")\n",
    "\n",
    "\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    metrics = compute_metrics(logits = logits, labels = batch['label'])\n",
    "    return state, metrics\n",
    "\n",
    "\n",
    "# evaluation step\n",
    "@jax.jit\n",
    "def eval_step(params, batch):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "\n",
    "    logits = EICNet().apply({'params': params}, batch['image'])\n",
    "    return compute_metrics(logits = logits, labels = batch['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train function\n",
    "def train_epoch(state, train_ds, batch_size, epoch, rng):\n",
    "    \"\"\"\n",
    "    Trains for a single epoch\n",
    "    \"\"\"\n",
    "\n",
    "    train_ds_size = len(train_ds['image'])\n",
    "    steps_per_epoch = train_ds_size // batch_size\n",
    "\n",
    "    perms = jax.random.permutation(rng, train_ds_size)\n",
    "    perms = perms[:steps_per_epoch * batch_size] # drop incomplete batch\n",
    "    perms = perms.reshape((steps_per_epoch, batch_size))\n",
    "\n",
    "    batch_metrics = []\n",
    "\n",
    "    for perm in perms:\n",
    "        batch = {k : v[perm, ...] for k, v in train_ds.items()}\n",
    "        state, metrics = train_step(state, batch)\n",
    "\n",
    "        batch_metrics.append(metrics)\n",
    "\n",
    "    # compute mean across the batch\n",
    "    batch_metrics_np = jax.device_get(batch_metrics)\n",
    "    epoch_metrics_np = {\n",
    "      k: np.mean([metrics[k] for metrics in batch_metrics_np])\n",
    "      for k in batch_metrics_np[0]}\n",
    "\n",
    "    print('train epoch: %d, loss: %.4f, accuracy: %.2f' % (\n",
    "        epoch, epoch_metrics_np['loss'], epoch_metrics_np['accuracy'] * 100))\n",
    "\n",
    "    return state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval function\n",
    "def eval_model(params, val_ds):\n",
    "    \"\"\"\n",
    "    Evaluate the model\n",
    "    \"\"\"\n",
    "\n",
    "    metrics = eval_step(params, val_ds)\n",
    "    metrics = jax.device_get(metrics)\n",
    "    summary = jax.tree.map(lambda x: x.item(), metrics)\n",
    "    return summary['loss'], summary['accuracy']\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "cannot reshape array of shape (128, 784) (size 100352) into shape (128, 256) (size 32768)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m momentum \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.9\u001b[39m\n\u001b[1;32m      5\u001b[0m batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m128\u001b[39m\n\u001b[0;32m----> 6\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_rng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mdel\u001b[39;00m init_rng  \u001b[38;5;66;03m# Must not be used anymore.\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[69], line 17\u001b[0m, in \u001b[0;36mcreate_train_state\u001b[0;34m(rng, learning_rate, batch_size, momentum)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03mCreate a training state for Flax\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     16\u001b[0m eic_model \u001b[38;5;241m=\u001b[39m EICNet()\n\u001b[0;32m---> 17\u001b[0m params \u001b[38;5;241m=\u001b[39m \u001b[43meic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m28\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m     18\u001b[0m opt \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39msgd(learning_rate \u001b[38;5;241m=\u001b[39m learning_rate, momentum\u001b[38;5;241m=\u001b[39m momentum)\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_state\u001b[38;5;241m.\u001b[39mTrainState\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[1;32m     20\u001b[0m   apply_fn\u001b[38;5;241m=\u001b[39meic_model\u001b[38;5;241m.\u001b[39mapply, params\u001b[38;5;241m=\u001b[39mparams, tx\u001b[38;5;241m=\u001b[39mopt\n\u001b[1;32m     21\u001b[0m )\n",
      "    \u001b[0;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[74], line 17\u001b[0m, in \u001b[0;36mEICNet.__call__\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m---> 17\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;66;03m# x = self.sh1(x)\u001b[39;00m\n\u001b[1;32m     19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc1(x)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_env/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:144\u001b[0m, in \u001b[0;36m_compute_newshape\u001b[0;34m(a, newshape)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    142\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mall\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(d, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mshape(a), \u001b[38;5;241m*\u001b[39mnewshape)) \u001b[38;5;129;01mand\u001b[39;00m\n\u001b[1;32m    143\u001b[0m       np\u001b[38;5;241m.\u001b[39msize(a) \u001b[38;5;241m!=\u001b[39m math\u001b[38;5;241m.\u001b[39mprod(newshape)):\n\u001b[0;32m--> 144\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcannot reshape array of shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39msize(a)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m) \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    145\u001b[0m                     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minto shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00morig_newshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m (size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmath\u001b[38;5;241m.\u001b[39mprod(newshape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(\u001b[38;5;241m-\u001b[39mcore\u001b[38;5;241m.\u001b[39mdivide_shape_sizes(np\u001b[38;5;241m.\u001b[39mshape(a), newshape)\n\u001b[1;32m    147\u001b[0m              \u001b[38;5;28;01mif\u001b[39;00m core\u001b[38;5;241m.\u001b[39mdefinitely_equal(d, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m d \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m newshape)\n",
      "\u001b[0;31mTypeError\u001b[0m: cannot reshape array of shape (128, 784) (size 100352) into shape (128, 256) (size 32768)"
     ]
    }
   ],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "rng, init_rng = jax.random.split(rng)\n",
    "learning_rate = 1e-4\n",
    "momentum = 0.9\n",
    "batch_size = 128\n",
    "state = create_train_state(init_rng, learning_rate, batch_size)\n",
    "del init_rng  # Must not be used anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 3\n",
    "# batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "184b53685c4547ce8b337e0fd77949c8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " val epoch: 1, loss: 3.89, accuracy: 9.99\n",
      "Layer ac1, Gradient Norm: {'weights': Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "Layer ac2, Gradient Norm: {'weights': Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "Layer fc1, Gradient Norm: {'weights': Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "Layer fc2, Gradient Norm: {'weights': Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "Layer sh2, Gradient Norm: {'Zneg': Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>, 'Zpos': Traced<ShapedArray(float32[])>with<DynamicJaxprTrace(level=1/0)>}\n",
      "train epoch: 1, loss: 3.9053, accuracy: 9.92\n",
      " val epoch: 2, loss: 3.89, accuracy: 9.99\n",
      "train epoch: 2, loss: 3.9052, accuracy: 9.91\n",
      " val epoch: 3, loss: 3.89, accuracy: 9.99\n",
      "train epoch: 3, loss: 3.9056, accuracy: 9.92\n"
     ]
    }
   ],
   "source": [
    "for epoch in tqdm(range(1, num_epochs + 1)):\n",
    "  # Use a separate PRNG key to permute image data during shuffling\n",
    "  rng, input_rng = jax.random.split(rng)\n",
    "\n",
    "   # Evaluate on the test set after each training epoch \n",
    "  test_loss, test_accuracy = eval_model(state.params, val_ds)\n",
    "  print(' val epoch: %d, loss: %.2f, accuracy: %.2f' % (\n",
    "      epoch, test_loss, test_accuracy * 100))\n",
    "  \n",
    "  # Run an optimization step over a training batch\n",
    "  state = train_epoch(state, train_ds, batch_size, epoch, input_rng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Array(2.3025851, dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_entropy_loss(logits = jnp.ones(10), labels = 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

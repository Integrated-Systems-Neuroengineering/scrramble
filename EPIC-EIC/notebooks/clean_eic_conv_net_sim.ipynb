{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic simulation of EIC core-based simulation on MNIST \n",
    "- The code simulates a pseudo-feedforward network with constraints of a RRAM-based CIM chip.\n",
    "- The goal is to achieve (near) SOTA on MNIST\n",
    "- The most notable constraint is that the chip can only apply non-linearity to blocks of 256-bit wide vectors at a time. This poses a challenge: we cannot implement any arbitrary-sized layers.\n",
    "- Most of the MNIST benchmark studies contain multiple hidden layers of often >1000 neurons.\n",
    "- In order to tackle this issue, we define a set of cores with learnable parameters that can accumulate the individual 256-bit long vectors. \n",
    "- We call these modules `EICDense` and `Accumulator`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.notebook import tqdm\n",
    "from flax import linen as nn\n",
    "from flax.linen import summary\n",
    "from flax.training import train_state\n",
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions\n",
    "\n",
    "- These functions implement noisy binary activation with straight through estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import vmap, jit\n",
    "\n",
    "# print(\"Modified custom grad to STE\")\n",
    "\n",
    "## define binary thresholding function: states [-1, 1]\n",
    "def binary_activation(x, threshold, noise_sd, key):\n",
    "    \"\"\"\n",
    "    Binary activation function\n",
    "    \"\"\"\n",
    "    # key, key2 = jax.random.split(key, 2)\n",
    "\n",
    "    # generate noise\n",
    "    noise = jax.random.normal(key, shape = x.shape) * noise_sd\n",
    "\n",
    "    # inject noise\n",
    "    x = x + noise\n",
    "\n",
    "    s = jnp.where(\n",
    "        x < threshold, 0.0, 1.0\n",
    "    )\n",
    "\n",
    "    return s\n",
    "\n",
    "## helper function\n",
    "@jax.jit\n",
    "def gaussian_cdf(x, mu, sigma):\n",
    "    return jax.scipy.stats.norm.cdf(x, loc = mu, scale = sigma)\n",
    "\n",
    "@jax.jit\n",
    "def gaussian_pdf(x, mu, sigma):\n",
    "    return jax.scipy.stats.norm.pdf(x, loc = mu, scale = sigma)\n",
    "\n",
    "@jax.jit\n",
    "def bin_expected_state(x, threshold, noise_sd):\n",
    "    e = gaussian_cdf(x = x - threshold, mu = 0, sigma = noise_sd)\n",
    "    return e\n",
    "\n",
    "@jax.custom_vjp\n",
    "def custom_binary_gradient(x, threshold, noise_sd, key):\n",
    "    return binary_activation(x = x, threshold = threshold, noise_sd = noise_sd, key = key)\n",
    "\n",
    "def custom_binary_gradient_fwd(x, threshold, noise_sd, key):\n",
    "    return custom_binary_gradient(x, threshold, noise_sd, key), (x, threshold, noise_sd)\n",
    "\n",
    "def custom_binary_gradient_bwd(residuals, gradients):\n",
    "    x, threshold, noise_sd = residuals\n",
    "    key, subkey = jax.random.split(jax.random.key(0))\n",
    "    grad = jnp.where(jnp.abs(x) <= 1.0, 1.0, 0.0) #gaussian_pdf(x = x - threshold, mu = 0, sigma = noise_sd*10)\n",
    "    return (grad*gradients, None, None, None)\n",
    "\n",
    "custom_binary_gradient.defvjp(custom_binary_gradient_fwd, custom_binary_gradient_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EICDense(nn.Module):\n",
    "    \"\"\"\n",
    "    Pseudo-dense layer using EIC Cores.\n",
    "    Args:\n",
    "    in_size: int, number of input neurons\n",
    "    out_size: int, number of output neurons\n",
    "    threshold: float, threshold for binary activation\n",
    "    noise_sd: flaat, standard deviation of noise for binary activation\n",
    "    key: jax.random.PRNGKey, random key\n",
    "\n",
    "    Returns:\n",
    "    x: jnp.ndarray, output of the layer\n",
    "    \"\"\"\n",
    "\n",
    "    in_size: int\n",
    "    out_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up dependent parameters\n",
    "        \"\"\"\n",
    "        self.out_blocks = max(self.out_size//256, 1) # number of blocks required at the output \n",
    "        self.in_blocks = max(self.in_size//256, 1) # number of bloacks required at the input\n",
    "\n",
    "\n",
    "        self.num_cores = self.out_blocks * self.in_blocks # number of cores required\n",
    "        self.W = self.param(\n",
    "            \"weights\",\n",
    "            lambda key, shape: nn.initializers.xavier_normal()(key, shape),\n",
    "            (self.out_blocks, self.in_blocks, 256, 256)\n",
    "        )\n",
    "\n",
    "\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the layer\n",
    "        Args:\n",
    "        x: jnp.ndarray (batch_size, in_size), input to the layer\n",
    "        \n",
    "        Returns:\n",
    "        x: jnp.ndarray, output of the layer\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[-1] == self.in_size, f\"Input shape is incorrect. Got {x.shape[-1]}, expected {self.in_size}\"\n",
    "\n",
    "        x_reshaped = x.reshape(x.shape[0], self.in_blocks, 256) # organize x into blocks of 256 for every batch\n",
    "\n",
    "        # make sure that the weights are positive\n",
    "        W_pos= self.W #jax.nn.softplus(self.W)\n",
    "\n",
    "        # quantize weights\n",
    "        # W_pos = quantize_params(W_pos, bits = 8)\n",
    "\n",
    "        y = jnp.einsum(\"ijkl,bjl->bijk\", W_pos, x_reshaped)\n",
    "\n",
    "\n",
    "        return y\n",
    "    \n",
    "\n",
    "# define the accumulator module\n",
    "class Accumulator(nn.Module):\n",
    "    \"\"\"\n",
    "    Accumulating the EICDense outputs. \n",
    "    Since the EICDense generates pseudo-feedforward outputs, we use a learnable accumulation matrix that minimizes error\n",
    "    between the true feedforward output and the EIC output.\n",
    "\n",
    "    Args:\n",
    "        in_block_size: int, number of 256-sized blocks. This should be the .shape[0] of the EICDense output\n",
    "    \"\"\"\n",
    "\n",
    "    in_block_size: int\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up the weights for the accumulator\n",
    "        \"\"\"\n",
    "\n",
    "        self.W = self.param(\n",
    "            \"weights\",\n",
    "            nn.initializers.xavier_normal(),\n",
    "            (self.in_block_size, 256, 256)\n",
    "        )\n",
    "\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Forward pass of the accumulator\n",
    "        Args:\n",
    "        x: jnp.ndarray, input to the accumulator\n",
    "        \n",
    "        Returns:\n",
    "        x: jnp.ndarray, output of the accumulator\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[1] == self.in_block_size, \"Input shape is incorrect\"\n",
    "        # assert x.shape[1] == self.out_block_size, \"Input shape is incorrect\"\n",
    "\n",
    "        # ensure positive \n",
    "        W_pos = self.W #jax.nn.softplus(self.W)\n",
    "        # W_pos = quantize_params(W_pos, bits = 8)\n",
    "        \n",
    "        x = jnp.einsum(\"bijk->bik\", x)\n",
    "        y = jnp.einsum(\"ijk,bik->bik\", W_pos, x) \n",
    "\n",
    "        # flatten y before returning\n",
    "        y = y.reshape((y.shape[0], -1)) # (batch_size, out_size)\n",
    "\n",
    "        return y\n",
    "\n",
    "class PermuteBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Contains two fixed permutation matrices (pos and neg) to shuffle the input block-wise.\n",
    "    \"\"\"\n",
    "\n",
    "    input_size: int\n",
    "    permute_block_size: int = 16 # previously 64\n",
    "    core_input_size: int = 256\n",
    "\n",
    "    def setup(self):\n",
    "        \"\"\"\n",
    "        Set up permutation matrices\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        self.num_slots = self.core_input_size // self.permute_block_size # should be 16 in the latest iteration\n",
    "        self.num_subvectors = self.input_size // self.core_input_size # for input_size = 1024, should be 256\n",
    "\n",
    "        self.tau = self.param(\n",
    "            'tau',\n",
    "            nn.initializers.constant(10),\n",
    "            ()\n",
    "        ) # temperature paramter\n",
    "\n",
    "        # generate two independent permutation sequences\n",
    "        key = jax.random.key(1245)\n",
    "        key1, key2 = jax.random.split(key)\n",
    "        p1 = jax.random.permutation(key1, self.num_slots)\n",
    "        p2 = jax.random.permutation(key2, self.num_slots) # jnp.roll(p1, shift = 1) #\n",
    "\n",
    "        # generate permutation matrices\n",
    "        m1 = jnp.eye(self.num_slots)*self.tau\n",
    "        m2 = jnp.eye(self.num_slots)*self.tau\n",
    "\n",
    "        # generate the permutation matrices\n",
    "        self.Ppos = m1[p1]\n",
    "        self.Ppos = jax.nn.softmax(self.Ppos, axis = -1)\n",
    "        self.Pneg = m2[p2]\n",
    "        self.Pneg = jax.nn.softmax(self.Pneg, axis = -1)\n",
    "\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        \"\"\"\n",
    "        Apply permutations and return (xpos - xneg)\n",
    "        Args:\n",
    "        x: jnp.ndarray, input vector. Shape: (batch_size, input_size) e.g. (32, 2048)\n",
    "        Returns:\n",
    "        xpos - xneg: jnp.ndarray, difference of permuted inputs. Shape: (batch_size, input_size)\n",
    "        \"\"\"\n",
    "\n",
    "        assert x.shape[-1] == self.input_size, f\"Input shape is incorrect. Got {x.shape[-1]}, expected {self.input_size}\"\n",
    "        assert self.num_subvectors * self.num_slots * self.permute_block_size == self.input_size, f\"Inconsistent metrics!\"\n",
    "\n",
    "        x = x.reshape(x.shape[0], self.num_subvectors, self.num_slots, self.permute_block_size) # first dimension must be the batch size\n",
    "\n",
    "        xpos = jnp.einsum('ij,bsjp->bsip', self.Ppos, x)\n",
    "        xneg = jnp.einsum('ij,bsjp->bsip', self.Pneg, x)\n",
    "\n",
    "        xout = xpos - xneg\n",
    "\n",
    "        xout = xout.reshape((x.shape[0], self.input_size))\n",
    "\n",
    "        return xout\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### relu/silu/quantization post-training\n",
    "* post-training quantization  \n",
    "* next steps: implement further steps towards hardware/jax block compatibility **quantization-aware training** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import optax\n",
    "import flax\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "from flax import linen as nn\n",
    "from EICDense import *\n",
    "from ShuffleBlocks import *\n",
    "from Accumulator import *\n",
    "#from PseudoFFNet import *\n",
    "#from EICNet import *\n",
    "#from HelperFunctions.activations import *\n",
    "from mnist_dataloader import *\n",
    "#from HelperFunctions.metric_functions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the MNIST dataset...\n",
      "Epoch 1 completed. Validation Accuracy: 96.68%\n",
      "Epoch 2 completed. Validation Accuracy: 97.72%\n",
      "Epoch 3 completed. Validation Accuracy: 97.68%\n",
      "Epoch 4 completed. Validation Accuracy: 98.10%\n",
      "Epoch 5 completed. Validation Accuracy: 97.74%\n",
      "Epoch 6 completed. Validation Accuracy: 97.82%\n",
      "Epoch 7 completed. Validation Accuracy: 98.14%\n",
      "Epoch 8 completed. Validation Accuracy: 97.82%\n",
      "Epoch 9 completed. Validation Accuracy: 97.90%\n",
      "Epoch 10 completed. Validation Accuracy: 98.26%\n",
      "Test Accuracy: 97.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rairo\\AppData\\Local\\Temp\\ipykernel_18756\\3232692158.py:92: DeprecationWarning: jax.tree_map is deprecated: use jax.tree.map (jax v0.4.25 or newer) or jax.tree_util.tree_map (any JAX version).\n",
      "  quantized_params = jax.tree_map(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy after Quantization: 97.12%\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_path = '/Users/rairo/trident data'\n",
    "\n",
    "# Load MNIST data\n",
    "print(\"Preparing the MNIST dataset...\")\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_and_process_mnist(data_path, binarize=True)\n",
    "\n",
    "# Define the ANN model\n",
    "class silu_net(nn.Module):\n",
    "    activation_fn: callable\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten input\n",
    "        x = self.noisy_dense(x, 2048)\n",
    "        x = self.noisy_dense(x, 256)\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "\n",
    "    def noisy_dense(self, x, features):\n",
    "        dense = nn.Dense(features)(x)\n",
    "        # Add beta noise\n",
    "        noise = jax.random.beta(jax.random.PRNGKey(0), 2, 5, shape=dense.shape)\n",
    "        noisy_output = dense + noise\n",
    "        return self.activation_fn(noisy_output)\n",
    "\n",
    "# Initialize model parameters and optimizer\n",
    "input_size = 28 * 28\n",
    "activation_fn = jax.nn.silu\n",
    "\n",
    "model = silu_net(activation_fn=activation_fn)\n",
    "\n",
    "# Training state\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    params = model.init(rng, jnp.ones([1, input_size]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(rng, model, learning_rate=0.001)\n",
    "\n",
    "# Training step\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch[\"images\"])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[\"labels\"]).mean()\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(state, images, labels):\n",
    "    logits = state.apply_fn({'params': state.params}, images)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == labels)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in get_train_batches(train_images, train_labels, batch_size):\n",
    "        batch = {\n",
    "            \"images\": images,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    val_accuracy = evaluate(state, val_images, val_labels)\n",
    "    print(f\"Epoch {epoch + 1} completed. Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test evaluation\n",
    "test_accuracy = evaluate(state, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Post-training quantization\n",
    "def quantize_params(params, num_bits=4):\n",
    "    scale = 2 ** (num_bits - 1) - 1\n",
    "    quantized_params = jax.tree_map(\n",
    "        lambda p: jnp.round(p * scale) / scale, params\n",
    "    )\n",
    "    return quantized_params\n",
    "\n",
    "# Quantize and evaluate\n",
    "quantized_params = quantize_params(state.params)\n",
    "test_accuracy_quantized = evaluate(state.replace(params=quantized_params), test_images, test_labels)\n",
    "print(f\"Test Accuracy after Quantization: {test_accuracy_quantized:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ScopeParamShapeError",
     "evalue": "Initializer expected to generate shape (256, 512) but got shape (784, 512) instead for parameter \"kernel\" in \"/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mScopeParamShapeError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 80\u001b[0m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m images, labels \u001b[38;5;129;01min\u001b[39;00m get_train_batches(train_images, train_labels, batch_size):\n\u001b[0;32m     76\u001b[0m     batch \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m     77\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m\"\u001b[39m: images,\n\u001b[0;32m     78\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m: labels\n\u001b[0;32m     79\u001b[0m     }\n\u001b[1;32m---> 80\u001b[0m     state \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     82\u001b[0m val_accuracy \u001b[38;5;241m=\u001b[39m evaluate(state, val_images, val_labels)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m completed. Validation Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mval_accuracy\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[25], line 60\u001b[0m, in \u001b[0;36mtrain_step\u001b[1;34m(state, batch)\u001b[0m\n\u001b[0;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy_with_integer_labels(logits, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n\u001b[1;32m---> 60\u001b[0m grads \u001b[38;5;241m=\u001b[39m \u001b[43mjax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_fn\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m state \u001b[38;5;241m=\u001b[39m state\u001b[38;5;241m.\u001b[39mapply_gradients(grads\u001b[38;5;241m=\u001b[39mgrads)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m state\n",
      "    \u001b[1;31m[... skipping hidden 17 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[25], line 56\u001b[0m, in \u001b[0;36mtrain_step.<locals>.loss_fn\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mloss_fn\u001b[39m(params):\n\u001b[1;32m---> 56\u001b[0m     logits \u001b[38;5;241m=\u001b[39m \u001b[43mstate\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mparams\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     57\u001b[0m     loss \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39msoftmax_cross_entropy_with_integer_labels(logits, batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "    \u001b[1;31m[... skipping hidden 6 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[25], line 23\u001b[0m, in \u001b[0;36mrelu_net.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;129m@nn\u001b[39m\u001b[38;5;241m.\u001b[39mcompact\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     22\u001b[0m     x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Flatten input (now 256 dimensions for 16x16 images)\u001b[39;00m\n\u001b[1;32m---> 23\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnoisy_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m512\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Adjusted dimensions to match smaller input size\u001b[39;00m\n\u001b[0;32m     24\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnoisy_dense(x, \u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m     25\u001b[0m     x \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDense(\u001b[38;5;241m10\u001b[39m)(x)  \u001b[38;5;66;03m# Output layer for 10 classes\u001b[39;00m\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[25], line 29\u001b[0m, in \u001b[0;36mrelu_net.noisy_dense\u001b[1;34m(self, x, features)\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mnoisy_dense\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, features):\n\u001b[1;32m---> 29\u001b[0m     dense \u001b[38;5;241m=\u001b[39m \u001b[43mnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Add beta noise\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     noise \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mbeta(jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m5\u001b[39m, shape\u001b[38;5;241m=\u001b[39mdense\u001b[38;5;241m.\u001b[39mshape)\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rairo\\miniconda3\\envs\\ep1cs1m\\lib\\site-packages\\flax\\linen\\linear.py:251\u001b[0m, in \u001b[0;36mDense.__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    241\u001b[0m \u001b[38;5;129m@compact\u001b[39m\n\u001b[0;32m    242\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, inputs: Array) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[0;32m    243\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Applies a linear transformation to the inputs along the last dimension.\u001b[39;00m\n\u001b[0;32m    244\u001b[0m \n\u001b[0;32m    245\u001b[0m \u001b[38;5;124;03m  Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;124;03m    The transformed input.\u001b[39;00m\n\u001b[0;32m    250\u001b[0m \u001b[38;5;124;03m  \"\"\"\u001b[39;00m\n\u001b[1;32m--> 251\u001b[0m   kernel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mkernel\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_init\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    256\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    257\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39muse_bias:\n\u001b[0;32m    258\u001b[0m     bias \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam(\n\u001b[0;32m    259\u001b[0m       \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbias\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias_init, (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures,), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparam_dtype\n\u001b[0;32m    260\u001b[0m     )\n",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rairo\\miniconda3\\envs\\ep1cs1m\\lib\\site-packages\\flax\\core\\scope.py:960\u001b[0m, in \u001b[0;36mScope.param\u001b[1;34m(self, name, init_fn, unbox, *init_args, **init_kwargs)\u001b[0m\n\u001b[0;32m    955\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m val, abs_val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(value_flat, abs_value_flat):\n\u001b[0;32m    956\u001b[0m     \u001b[38;5;66;03m# NOTE: We could check dtype consistency here as well but it's\u001b[39;00m\n\u001b[0;32m    957\u001b[0m     \u001b[38;5;66;03m# usefuleness is less obvious. We might intentionally change the dtype\u001b[39;00m\n\u001b[0;32m    958\u001b[0m     \u001b[38;5;66;03m# for inference to a half float type for example.\u001b[39;00m\n\u001b[0;32m    959\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m jnp\u001b[38;5;241m.\u001b[39mshape(val) \u001b[38;5;241m!=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mshape(abs_val):\n\u001b[1;32m--> 960\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mScopeParamShapeError(\n\u001b[0;32m    961\u001b[0m         name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpath_text, jnp\u001b[38;5;241m.\u001b[39mshape(abs_val), jnp\u001b[38;5;241m.\u001b[39mshape(val)\n\u001b[0;32m    962\u001b[0m       )\n\u001b[0;32m    963\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    964\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_mutable_collection(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m):\n",
      "\u001b[1;31mScopeParamShapeError\u001b[0m: Initializer expected to generate shape (256, 512) but got shape (784, 512) instead for parameter \"kernel\" in \"/Dense_0\". (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.ScopeParamShapeError)"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_path = '/Users/rairo/trident data'\n",
    "\n",
    "# Load MNIST data\n",
    "\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_and_process_mnist(data_path, binarize=True)\n",
    "\n",
    "# Define the ANN model\n",
    "class relu_net(nn.Module):\n",
    "    activation_fn: callable\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten input (now 256 dimensions for 16x16 images)\n",
    "        x = self.noisy_dense(x, 512)  # Adjusted dimensions to match smaller input size\n",
    "        x = self.noisy_dense(x, 256)\n",
    "        x = nn.Dense(10)(x)  # Output layer for 10 classes\n",
    "        return x\n",
    "\n",
    "    def noisy_dense(self, x, features):\n",
    "        dense = nn.Dense(features)(x)\n",
    "        # Add beta noise\n",
    "        noise = jax.random.beta(jax.random.PRNGKey(0), 2, 5, shape=dense.shape)\n",
    "        noisy_output = dense + noise\n",
    "        return self.activation_fn(noisy_output)\n",
    "\n",
    "# Initialize model parameters and optimizer\n",
    "input_size = 16 * 16  # Updated input size for 16x16 images\n",
    "activation_fn = jax.nn.relu\n",
    "\n",
    "model = relu_net(activation_fn=activation_fn)\n",
    "\n",
    "# Training state\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    params = model.init(rng, jnp.ones([1, input_size]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(rng, model, learning_rate=0.001)\n",
    "\n",
    "# Training step\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch[\"images\"])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[\"labels\"]).mean()\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(state, images, labels):\n",
    "    logits = state.apply_fn({'params': state.params}, images)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == labels)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in get_train_batches(train_images, train_labels, batch_size):\n",
    "        batch = {\n",
    "            \"images\": images,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    val_accuracy = evaluate(state, val_images, val_labels)\n",
    "    print(f\"Epoch {epoch + 1} completed. Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test evaluation\n",
    "test_accuracy = evaluate(state, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Post-training quantization\n",
    "def quantize_params(params, num_bits=4):\n",
    "    scale = 2 ** (num_bits - 1) - 1\n",
    "    quantized_params = jax.tree_map(\n",
    "        lambda p: jnp.round(p * scale) / scale, params\n",
    "    )\n",
    "    return quantized_params\n",
    "\n",
    "# Quantize and evaluate\n",
    "quantized_params = quantize_params(state.params)\n",
    "test_accuracy_quantized = evaluate(state.replace(params=quantized_params), test_images, test_labels)\n",
    "print(f\"Test Accuracy after Quantization: {test_accuracy_quantized:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the MNIST dataset...\n",
      "Epoch 1 completed. Validation Accuracy: 96.46%\n",
      "Epoch 2 completed. Validation Accuracy: 97.56%\n",
      "Epoch 3 completed. Validation Accuracy: 98.18%\n",
      "Epoch 4 completed. Validation Accuracy: 97.68%\n",
      "Epoch 5 completed. Validation Accuracy: 98.24%\n",
      "Epoch 6 completed. Validation Accuracy: 98.12%\n",
      "Epoch 7 completed. Validation Accuracy: 97.96%\n",
      "Epoch 8 completed. Validation Accuracy: 97.92%\n",
      "Epoch 9 completed. Validation Accuracy: 97.68%\n",
      "Epoch 10 completed. Validation Accuracy: 97.94%\n",
      "Test Accuracy: 97.81%\n",
      "Test Accuracy after Quantization: 97.84%\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_path = '/Users/rairo/trident data'\n",
    "\n",
    "# Load MNIST data\n",
    "print(\"Preparing the MNIST dataset...\")\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_and_process_mnist(data_path, binarize=True)\n",
    "\n",
    "# Define the ANN model\n",
    "class silu_net(nn.Module):\n",
    "    activation_fn: callable\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten input\n",
    "        x = self.noisy_dense(x, 2048)\n",
    "        x = self.noisy_dense(x, 256)\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "\n",
    "    def noisy_dense(self, x, features):\n",
    "        dense = nn.Dense(features)(x)\n",
    "        # Add beta noise\n",
    "        noise = jax.random.beta(jax.random.PRNGKey(0), 2, 5, shape=dense.shape)\n",
    "        noisy_output = dense + noise\n",
    "        return self.activation_fn(noisy_output)\n",
    "\n",
    "# Initialize model parameters and optimizer\n",
    "input_size = 28 * 28\n",
    "activation_fn = jax.nn.silu\n",
    "\n",
    "model = silu_net(activation_fn=activation_fn)\n",
    "\n",
    "# Training state\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    params = model.init(rng, jnp.ones([1, input_size]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(rng, model, learning_rate=0.001)\n",
    "\n",
    "# Training step\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch[\"images\"])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[\"labels\"]).mean()\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(state, images, labels):\n",
    "    logits = state.apply_fn({'params': state.params}, images)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == labels)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in get_train_batches(train_images, train_labels, batch_size):\n",
    "        batch = {\n",
    "            \"images\": images,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    val_accuracy = evaluate(state, val_images, val_labels)\n",
    "    print(f\"Epoch {epoch + 1} completed. Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test evaluation\n",
    "test_accuracy = evaluate(state, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Post-training quantization\n",
    "def quantize_params(params, num_bits=8):\n",
    "    scale = 2 ** (num_bits - 1) - 1\n",
    "    quantized_params = jax.tree_map(\n",
    "        lambda p: jnp.round(p * scale) / scale, params\n",
    "    )\n",
    "    return quantized_params\n",
    "\n",
    "# Quantize and evaluate\n",
    "quantized_params = quantize_params(state.params)\n",
    "test_accuracy_quantized = evaluate(state.replace(params=quantized_params), test_images, test_labels)\n",
    "print(f\"Test Accuracy after Quantization: {test_accuracy_quantized:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jax blocks compatible version of silu-net? \n",
    "* reshaping & dense layers to approximate the tensor block level operations of jnp.einsum \n",
    "* silu \n",
    "* dense layer operation to aggregate outputs \n",
    "* permutations across features (static)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "data_path = '/Users/rairo/trident data'\n",
    "\n",
    "# Load MNIST data\n",
    "print(\"Preparing the MNIST dataset...\")\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_and_process_mnist(data_path, binarize=True)\n",
    "\n",
    "# Define the ANN model\n",
    "class silu_net(nn.Module):\n",
    "    activation_fn: callable\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten input\n",
    "        x = self.noisy_dense(x, 2048)\n",
    "        x = self.noisy_dense(x, 256)\n",
    "        x = nn.Dense(10)(x)\n",
    "        return x\n",
    "\n",
    "    def noisy_dense(self, x, features):\n",
    "        dense = nn.Dense(features)(x)\n",
    "        # Add beta noise\n",
    "        noise = jax.random.beta(jax.random.PRNGKey(0), 2, 5, shape=dense.shape)\n",
    "        noisy_output = dense + noise\n",
    "        return self.activation_fn(noisy_output)\n",
    "\n",
    "# Initialize model parameters and optimizer\n",
    "input_size = 28 * 28\n",
    "activation_fn = jax.nn.silu\n",
    "\n",
    "model = silu_net(activation_fn=activation_fn)\n",
    "\n",
    "# Training state\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    params = model.init(rng, jnp.ones([1, input_size]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(rng, model, learning_rate=0.001)\n",
    "\n",
    "# Training step\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch[\"images\"])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[\"labels\"]).mean()\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(state, images, labels):\n",
    "    logits = state.apply_fn({'params': state.params}, images)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == labels)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in get_train_batches(train_images, train_labels, batch_size):\n",
    "        batch = {\n",
    "            \"images\": images,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    val_accuracy = evaluate(state, val_images, val_labels)\n",
    "    print(f\"Epoch {epoch + 1} completed. Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test evaluation\n",
    "test_accuracy = evaluate(state, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Post-training quantization\n",
    "def quantize_params(params, num_bits=8):\n",
    "    scale = 2 ** (num_bits - 1) - 1\n",
    "    quantized_params = jax.tree_map(\n",
    "        lambda p: jnp.round(p * scale) / scale, params\n",
    "    )\n",
    "    return quantized_params\n",
    "\n",
    "# Quantize and evaluate\n",
    "quantized_params = quantize_params(state.params)\n",
    "test_accuracy_quantized = evaluate(state.replace(params=quantized_params), test_images, test_labels)\n",
    "print(f\"Test Accuracy after Quantization: {test_accuracy_quantized:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reshape_to_blocks(x, block_size=256):\n",
    "    # Calculate the required padding\n",
    "    total_size = x.shape[1]\n",
    "    pad_size = block_size - (total_size % block_size) if total_size % block_size != 0 else 0\n",
    "    \n",
    "    # Apply padding\n",
    "    x_padded = jnp.pad(x, ((0, 0), (0, pad_size)))  # Pad along the feature dimension\n",
    "\n",
    "    # Reshape to blocks\n",
    "    num_blocks = x_padded.shape[1] // block_size\n",
    "    return x_padded.reshape(x.shape[0], num_blocks, block_size)\n",
    "\n",
    "\n",
    "def constrained_dense(x, features):\n",
    "    dense = nn.Dense(features)(x)\n",
    "    constrained_weights = jax.nn.relu(dense)\n",
    "    return constrained_weights\n",
    "\n",
    "class Shuffle(nn.Module):\n",
    "    def __call__(self, x):\n",
    "        # Random permutation of features\n",
    "        perm = jax.random.permutation(jax.random.PRNGKey(0), x.shape[1])\n",
    "        return x[:, perm]\n",
    "\n",
    "def accumulate(x, features):\n",
    "    # Simple accumulation block\n",
    "    dense = nn.Dense(features)(x)\n",
    "    return nn.relu(dense)  # Simulates accumulation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class silu_net_blocks(nn.Module):\n",
    "    activation_fn: callable\n",
    "\n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        x = x.reshape(x.shape[0], -1)  # Flatten input\n",
    "\n",
    "        # Reshape to blocks with padding\n",
    "        x = reshape_to_blocks(x)\n",
    "\n",
    "        # First noisy dense layer\n",
    "        x = self.noisy_dense(x, 2048)\n",
    "\n",
    "        # Shuffle layer\n",
    "        x = Shuffle()(x)\n",
    "\n",
    "        # Accumulate outputs\n",
    "        x = accumulate(x, 256)\n",
    "\n",
    "        # Second dense layer\n",
    "        x = self.noisy_dense(x, 256)\n",
    "\n",
    "        # Final dense layer and reshape logits\n",
    "        x = nn.Dense(10)(x)\n",
    "        x = x.reshape(x.shape[0], -1)  # Remove extra dimensions\n",
    "\n",
    "        return x\n",
    "\n",
    "    def noisy_dense(self, x, features):\n",
    "        dense = constrained_dense(x, features)\n",
    "        noise = jax.random.beta(jax.random.PRNGKey(0), 2, 5, shape=dense.shape)\n",
    "        return self.activation_fn(dense + noise)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the MNIST dataset...\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from flax import linen as nn\n",
    "from flax.training import train_state\n",
    "import optax\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from HelperFunctions.mnist_dataloader import * \n",
    "data_path = '/Users/rairo/trident data'\n",
    "\n",
    "# Load MNIST data\n",
    "print(\"Preparing the MNIST dataset...\")\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_and_process_mnist(data_path, binarize=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 completed. Validation Accuracy: 93.10%\n",
      "Epoch 2 completed. Validation Accuracy: 95.06%\n",
      "Epoch 3 completed. Validation Accuracy: 95.24%\n",
      "Epoch 4 completed. Validation Accuracy: 95.36%\n",
      "Epoch 5 completed. Validation Accuracy: 95.54%\n",
      "Epoch 6 completed. Validation Accuracy: 96.27%\n",
      "Epoch 7 completed. Validation Accuracy: 95.72%\n",
      "Epoch 8 completed. Validation Accuracy: 96.45%\n",
      "Epoch 9 completed. Validation Accuracy: 96.04%\n",
      "Epoch 10 completed. Validation Accuracy: 96.33%\n",
      "Test Accuracy: 96.47%\n",
      "Test Accuracy after Quantization: 96.40%\n"
     ]
    }
   ],
   "source": [
    "# Initialize model parameters and optimizer\n",
    "input_size = 28 * 28\n",
    "activation_fn = jax.nn.silu\n",
    "\n",
    "model = silu_net_blocks(activation_fn=activation_fn)\n",
    "\n",
    "# Training state\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    params = model.init(rng, jnp.ones([1, input_size]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(rng, model, learning_rate=0.001)\n",
    "\n",
    "# Training step\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch[\"images\"])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[\"labels\"]).mean()\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(state, images, labels):\n",
    "    logits = state.apply_fn({'params': state.params}, images)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == labels)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in get_train_batches(train_images, train_labels, batch_size):\n",
    "        batch = {\n",
    "            \"images\": images,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    val_accuracy = evaluate(state, val_images, val_labels)\n",
    "    print(f\"Epoch {epoch + 1} completed. Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test evaluation\n",
    "test_accuracy = evaluate(state, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Post-training quantization\n",
    "def quantize_params(params, num_bits=8):\n",
    "    scale = 2 ** (num_bits - 1) - 1\n",
    "    quantized_params = jax.tree_map(\n",
    "        lambda p: jnp.round(p * scale) / scale, params\n",
    "    )\n",
    "    return quantized_params\n",
    "\n",
    "# Quantize and evaluate\n",
    "quantized_params = quantize_params(state.params)\n",
    "test_accuracy_quantized = evaluate(state.replace(params=quantized_params), test_images, test_labels)\n",
    "print(f\"Test Accuracy after Quantization: {test_accuracy_quantized:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from EICDense import *\n",
    "from ShuffleBlocks import *\n",
    "from Accumulator import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BlockSiluNet(nn.Module):\n",
    "    activation_fn: callable\n",
    "\n",
    "    def setup(self):\n",
    "        # Layer 1: EICDense \n",
    "        self.fc1 = EICDense(\n",
    "            in_size=784, \n",
    "            out_size=2048, \n",
    "            threshold=0.0, \n",
    "            noise_sd=0.05,  \n",
    "            activation=self.activation_fn\n",
    "        )\n",
    "        self.ac1 = Accumulator(\n",
    "            in_block_size=2048 // 256, \n",
    "            threshold=0.0, \n",
    "            noise_sd=0.05, \n",
    "            activation=self.activation_fn\n",
    "        )\n",
    "        # Shuffle block \n",
    "        self.shuffle1 = ShuffleBlocks(subvector_len=256, slot_len=64, key=jax.random.PRNGKey(0))\n",
    "        \n",
    "        # Layer 2: EICDense \n",
    "        self.fc2 = EICDense(\n",
    "            in_size=2048, \n",
    "            out_size=512,  \n",
    "            threshold=0.0, \n",
    "            noise_sd=0.05, \n",
    "            activation=self.activation_fn\n",
    "        )\n",
    "        self.ac2 = Accumulator(\n",
    "            in_block_size=512 // 256, \n",
    "            threshold=0.0, \n",
    "            noise_sd=0.05, \n",
    "            activation=self.activation_fn\n",
    "        )\n",
    "        self.shuffle2 = ShuffleBlocks(subvector_len=256, slot_len=64, key=jax.random.PRNGKey(1))\n",
    "        \n",
    "        # Layer 3: EICDense with reduced output size\n",
    "        self.fc3 = EICDense(\n",
    "            in_size=512, \n",
    "            out_size=10, \n",
    "            threshold=0.0, \n",
    "            noise_sd=0.05, \n",
    "            activation=self.activation_fn\n",
    "        )\n",
    "        self.ac3 = Accumulator(\n",
    "            in_block_size=10 // 256, \n",
    "            threshold=0.0, \n",
    "            noise_sd=0.05, \n",
    "            activation=self.activation_fn\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        # Flatten input\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "\n",
    "        # First layer: EICDense -> Accumulator -> Shuffle\n",
    "        x = self.fc1(x)\n",
    "        x = self.ac1(x)\n",
    "        x = self.shuffle1(x)\n",
    "\n",
    "        # Second layer: EICDense -> Accumulator -> Shuffle\n",
    "        x = self.fc2(x)\n",
    "        x = self.ac2(x)\n",
    "        x = self.shuffle2(x)\n",
    "\n",
    "        # Third layer: EICDense -> Accumulator (final output)\n",
    "        x = self.fc3(x)\n",
    "        x = self.ac3(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing the MNIST dataset...\n"
     ]
    },
    {
     "ename": "SetAttributeFrozenModuleError",
     "evalue": "Can't set in_blocks=1 for Module of type EICDense: Module instance is frozen outside of setup method. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.SetAttributeFrozenModuleError)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mSetAttributeFrozenModuleError\u001b[0m             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 25\u001b[0m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TrainState\u001b[38;5;241m.\u001b[39mcreate(apply_fn\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply, params\u001b[38;5;241m=\u001b[39mparams, tx\u001b[38;5;241m=\u001b[39mtx)\n\u001b[0;32m     24\u001b[0m rng \u001b[38;5;241m=\u001b[39m jax\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mPRNGKey(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m---> 25\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_train_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.001\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Post-training pruning\u001b[39;00m\n\u001b[0;32m     28\u001b[0m pruned_params \u001b[38;5;241m=\u001b[39m prune_weights(state\u001b[38;5;241m.\u001b[39mparams, threshold\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m)\n",
      "Cell \u001b[1;32mIn[24], line 20\u001b[0m, in \u001b[0;36mcreate_train_state\u001b[1;34m(rng, model, learning_rate)\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcreate_train_state\u001b[39m(rng, model, learning_rate):\n\u001b[1;32m---> 20\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrng\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mones\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43minput_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     21\u001b[0m     tx \u001b[38;5;241m=\u001b[39m optax\u001b[38;5;241m.\u001b[39madam(learning_rate)\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m TrainState\u001b[38;5;241m.\u001b[39mcreate(apply_fn\u001b[38;5;241m=\u001b[39mmodel\u001b[38;5;241m.\u001b[39mapply, params\u001b[38;5;241m=\u001b[39mparams, tx\u001b[38;5;241m=\u001b[39mtx)\n",
      "    \u001b[1;31m[... skipping hidden 9 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[20], line 58\u001b[0m, in \u001b[0;36mBlockSiluNet.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     55\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m# First layer: EICDense -> Accumulator -> Shuffle\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mac1(x)\n\u001b[0;32m     60\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshuffle1(x)\n",
      "    \u001b[1;31m[... skipping hidden 2 frame]\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\rairo\\Desktop\\Blocks\\EICDense.py:83\u001b[0m, in \u001b[0;36mEICDense.__call__\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     80\u001b[0m     pad_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m-\u001b[39m (x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m256\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m256\u001b[39m \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     81\u001b[0m     x \u001b[38;5;241m=\u001b[39m jnp\u001b[38;5;241m.\u001b[39mpad(x, ((\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m), (\u001b[38;5;241m0\u001b[39m, pad_size)))  \u001b[38;5;66;03m# Pad along feature dimension\u001b[39;00m\n\u001b[1;32m---> 83\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_blocks\u001b[49m \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39m \u001b[38;5;241m256\u001b[39m  \u001b[38;5;66;03m# Recalculate in_blocks after padding\u001b[39;00m\n\u001b[0;32m     85\u001b[0m \u001b[38;5;66;03m# Reshape into blocks\u001b[39;00m\n\u001b[0;32m     86\u001b[0m x_reshaped \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_blocks, \u001b[38;5;241m256\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\rairo\\miniconda3\\envs\\ep1cs1m\\lib\\site-packages\\flax\\linen\\module.py:1279\u001b[0m, in \u001b[0;36mModule.__setattr__\u001b[1;34m(self, name, val)\u001b[0m\n\u001b[0;32m   1275\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m   1276\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1277\u001b[0m     \u001b[38;5;66;03m# We're past all initialization and setup logic:\u001b[39;00m\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;66;03m# Raises a TypeError just like frozen python dataclasses.\u001b[39;00m\n\u001b[1;32m-> 1279\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m errors\u001b[38;5;241m.\u001b[39mSetAttributeFrozenModuleError(\n\u001b[0;32m   1280\u001b[0m       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, name, val\n\u001b[0;32m   1281\u001b[0m     )\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# We're inside the setup() method:\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_dataclass_attr:\n\u001b[0;32m   1285\u001b[0m   \u001b[38;5;66;03m# These names are specified as dataclass fields. They should not be\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m   \u001b[38;5;66;03m# initialized within the setup() method, but can be modified freely\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m   \u001b[38;5;66;03m# before it.\u001b[39;00m\n",
      "\u001b[1;31mSetAttributeFrozenModuleError\u001b[0m: Can't set in_blocks=1 for Module of type EICDense: Module instance is frozen outside of setup method. (https://flax.readthedocs.io/en/latest/api_reference/flax.errors.html#flax.errors.SetAttributeFrozenModuleError)"
     ]
    }
   ],
   "source": [
    "# Initialize model parameters and optimizer\n",
    "input_size = 28 * 28\n",
    "activation_fn = jax.nn.silu\n",
    "data_path = '/Users/rairo/trident data'\n",
    "\n",
    "# Load MNIST data\n",
    "print(\"Preparing the MNIST dataset...\")\n",
    "(train_images, train_labels), (val_images, val_labels), (test_images, test_labels) = load_and_process_mnist(data_path, binarize=True)\n",
    "\n",
    "model = BlockSiluNet(activation_fn=activation_fn)\n",
    "\n",
    "def prune_weights(params, threshold=0.01):\n",
    "    return jax.tree_map(lambda p: jnp.where(jnp.abs(p) > threshold, p, 0), params)\n",
    "\n",
    "# Training state\n",
    "class TrainState(train_state.TrainState):\n",
    "    pass\n",
    "\n",
    "def create_train_state(rng, model, learning_rate):\n",
    "    params = model.init(rng, jnp.ones([input_size]))['params']\n",
    "    tx = optax.adam(learning_rate)\n",
    "    return TrainState.create(apply_fn=model.apply, params=params, tx=tx)\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "state = create_train_state(rng, model, learning_rate=0.001)\n",
    "\n",
    "# Post-training pruning\n",
    "pruned_params = prune_weights(state.params, threshold=0.01)\n",
    "state = state.replace(params=pruned_params)\n",
    "\n",
    "# Training step\n",
    "def train_step(state, batch):\n",
    "    def loss_fn(params):\n",
    "        logits = state.apply_fn({'params': params}, batch[\"images\"])\n",
    "        loss = optax.softmax_cross_entropy_with_integer_labels(logits, batch[\"labels\"]).mean()\n",
    "        return loss\n",
    "\n",
    "    grads = jax.grad(loss_fn)(state.params)\n",
    "    state = state.apply_gradients(grads=grads)\n",
    "    return state\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(state, images, labels):\n",
    "    logits = state.apply_fn({'params': state.params}, images)\n",
    "    predictions = jnp.argmax(logits, axis=-1)\n",
    "    accuracy = jnp.mean(predictions == labels)\n",
    "    return accuracy * 100\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "batch_size = 64\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in get_train_batches(train_images, train_labels, batch_size):\n",
    "        batch = {\n",
    "            \"images\": images,\n",
    "            \"labels\": labels\n",
    "        }\n",
    "        state = train_step(state, batch)\n",
    "\n",
    "    val_accuracy = evaluate(state, val_images, val_labels)\n",
    "    print(f\"Epoch {epoch + 1} completed. Validation Accuracy: {val_accuracy:.2f}%\")\n",
    "\n",
    "# Test evaluation\n",
    "test_accuracy = evaluate(state, test_images, test_labels)\n",
    "print(f\"Test Accuracy: {test_accuracy:.2f}%\")\n",
    "\n",
    "# Post-training quantization\n",
    "def quantize_params(params, num_bits=8):\n",
    "    scale = 2 ** (num_bits - 1) - 1\n",
    "    quantized_params = jax.tree_map(\n",
    "        lambda p: jnp.round(p * scale) / scale, params\n",
    "    )\n",
    "    return quantized_params\n",
    "\n",
    "# Quantize and evaluate\n",
    "quantized_params = quantize_params(state.params)\n",
    "test_accuracy_quantized = evaluate(state.replace(params=quantized_params), test_images, test_labels)\n",
    "print(f\"Test Accuracy after Quantization: {test_accuracy_quantized:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ep1cs1m",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tutorial for implementing custom gradients in JAX\n",
    "\n",
    "- Goal of this notebook is to better understand how to leverage acceleration libraries to implement custom derivatives.\n",
    "- Although, most of the libraries like JAX, PyTorch, TensorFlow etc. can easily implement automatic differentiation, there is always a need to implement activation functions that may not be differentiable:\n",
    "    - Spiking neural networks typically use LIF neurons\n",
    "    - It is efficient to use simplified backward pass or truncated gradients.\n",
    "    - We are now working with ternary neurons, which also have discontinuous activation.\n",
    "\n",
    "- For these type of models, the backward pass in backpropagation or a similar algorithm has to have custom defined gradients.\n",
    "- As a starting point, this notebook will go through <a href = \"https://jax.readthedocs.io/en/latest/notebooks/Custom_derivative_rules_for_Python_code.html\">\"Custom Derivative Rules\" </a> tutorial on JAX documentation page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Goals of the notebook\n",
    "- Get familirized with flax.\n",
    "- Be able to instantiate modules with activations given by a non-linearity\n",
    "- Make a simple hierarchical parameter data structure whcih can be ported to MEC conectivity.\n",
    "\n",
    "## Iteration 1:\n",
    "1. Consider two cores and a lookup table.\n",
    "2. The cores each have one fully connected layer.\n",
    "3. The lookup table provides connectivity between cores.\n",
    "4. Neurons in each core are binary stochastic.\n",
    "5. Try to train MNIST on these cores.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax\n",
    "from flax import linen as nn\n",
    "from flax import nnx\n",
    "import optax\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'jax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m s\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m## helper function\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgaussian_cdf\u001b[39m(x, mu, sigma):\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m jax\u001b[38;5;241m.\u001b[39mscipy\u001b[38;5;241m.\u001b[39mstats\u001b[38;5;241m.\u001b[39mnorm\u001b[38;5;241m.\u001b[39mcdf(x, loc \u001b[38;5;241m=\u001b[39m mu, scale \u001b[38;5;241m=\u001b[39m sigma)\n\u001b[1;32m     25\u001b[0m \u001b[38;5;129m@jax\u001b[39m\u001b[38;5;241m.\u001b[39mjit\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgaussian_pdf\u001b[39m(x, mu, sigma):\n",
      "\u001b[0;31mNameError\u001b[0m: name 'jax' is not defined"
     ]
    }
   ],
   "source": [
    "## define binary thresholding function: states [0, 1]\n",
    "def binary_activation(x, threshold, noise_sd, key):\n",
    "    \"\"\"\n",
    "    Binary activation function\n",
    "    \"\"\"\n",
    "    # key, key2 = jax.random.split(key, 2)\n",
    "\n",
    "    # generate noise\n",
    "    noise = jax.random.normal(key, shape = x.shape) * noise_sd\n",
    "\n",
    "    # inject noise\n",
    "    x = x + noise\n",
    "\n",
    "    s = jnp.where(\n",
    "        x < threshold, 0.0, 1.0\n",
    "    )\n",
    "\n",
    "    return s\n",
    "\n",
    "## helper function\n",
    "@jax.jit\n",
    "def gaussian_cdf(x, mu, sigma):\n",
    "    return jax.scipy.stats.norm.cdf(x, loc = mu, scale = sigma)\n",
    "\n",
    "@jax.jit\n",
    "def gaussian_pdf(x, mu, sigma):\n",
    "    return jax.scipy.stats.norm.pdf(x, loc = mu, scale = sigma)\n",
    "\n",
    "@jax.jit\n",
    "def expected_state(x, thresholds, noise_sd):\n",
    "    t1, t2 = thresholds\n",
    "    e = ((1 - gaussian_cdf(x = t2 - x, mu = 0, sigma = noise_sd)) - gaussian_cdf(x = t1 - x, mu = 0, sigma = noise_sd))\n",
    "    return e\n",
    "\n",
    "\n",
    "## cuatom gradient for binary activation\n",
    "@jax.custom_vjp\n",
    "def custom_binary_gradient(x, threshold, noise_sd, key):\n",
    "    return binary_activation(x = x, threshold = threshold, noise_sd = noise_sd, key = key)\n",
    "\n",
    "def custom_binary_gradient_fwd(x, threshold, noise_sd, key):\n",
    "    return custom_binary_gradient(x, threshold, noise_sd, key), (x, threshold, noise_sd)\n",
    "\n",
    "def custom_binary_gradient_bwd(residuals, gradients):\n",
    "    x, threshold, noise_sd = residuals\n",
    "    grad = gaussian_pdf(x = x - threshold, mu = 0, sigma = noise_sd)\n",
    "    return (grad*gradients, None, None, None)\n",
    "\n",
    "custom_binary_gradient.defvjp(custom_binary_gradient_fwd, custom_binary_gradient_bwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.78476596  0.85644484]\n",
      "[[ 1.2841669  -0.5337844 ]\n",
      " [ 0.24033786 -0.3781857 ]]\n",
      "[1. 1.]\n"
     ]
    }
   ],
   "source": [
    "## define the dense layer module\n",
    "class Dense(nnx.Module):\n",
    "    \"\"\"\n",
    "    Define a single core.\n",
    "    For a start this is one layer.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs: int,\n",
    "                num_outputs: int, \n",
    "                key: jax.random.key):\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_outputs = num_outputs\n",
    "        self.key = key\n",
    "\n",
    "        ## define the weights and biases\n",
    "        self.weights = nnx.Param(jax.random.normal(key, (num_inputs, num_outputs)) * jnp.sqrt(2/(num_inputs + num_outputs)))\n",
    "\n",
    "        self.biases = nnx.Param(jnp.zeros((num_outputs,)))\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray):\n",
    "        # assert x.shape[-1] == self.num_inputs, \"Input shape does not match the number of inputs\"\n",
    "        return x @ self.weights + self.biases\n",
    "    \n",
    "\n",
    "class Core(nnx.Module):\n",
    "    \"\"\"\n",
    "    Define a single core. One layer with binary activation\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs: int,\n",
    "                 num_neurons: int,\n",
    "                 threshold: float,\n",
    "                 noise_sd: float,\n",
    "                key: jax.random.key\n",
    "                 ):\n",
    "        \n",
    "        self.num_inputs = num_inputs\n",
    "        self.num_neurons = num_neurons\n",
    "        self.threshold = threshold\n",
    "        self.noise_sd = noise_sd\n",
    "        self.key = key\n",
    "        self.dense = Dense(self.num_inputs, self.num_neurons, self.key)\n",
    "\n",
    "\n",
    "    def __call__(self, \n",
    "                 x: jnp.array,\n",
    "                 ):\n",
    "        \n",
    "        ## pass through the dense layer\n",
    "        x = self.dense(x)\n",
    "        self.key, self.subkey = jax.random.split(self.key, 2)\n",
    "        x = custom_binary_gradient(x, self.threshold, self.noise_sd, self.subkey)\n",
    "        return x\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "## TESTING..\n",
    "    \n",
    "# ## test if the dense layer works\n",
    "# key = jax.random.PRNGKey(0)\n",
    "# d1 = Dense(2, 2, key)\n",
    "# x = jnp.ones((2,))#jax.random.normal(key, (2,))\n",
    "# print(d1.weights.value)\n",
    "# print(x)\n",
    "# print(d1(x))\n",
    "\n",
    "# plt.figure()\n",
    "# plt.imshow(d1.weights.value, cmap = 'coolwarm')\n",
    "# plt.colorbar()\n",
    "\n",
    "# plt.figure()\n",
    "# plt.plot(d1.biases.value)\n",
    "\n",
    "## core\n",
    "key = jax.random.PRNGKey(0)\n",
    "c1 = Core(num_inputs=2, num_neurons=2, threshold=-1, noise_sd=0.1, key=key)\n",
    "x = jax.random.normal(key, (2,))\n",
    "print(x)\n",
    "print(c1.dense.weights.value)\n",
    "print(c1(x))\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

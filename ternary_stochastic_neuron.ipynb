{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ternary stochastic neuron\n",
    "\n",
    "## goal of the notebook\n",
    "- To develop a model of ternary stochastic neuron in `JAX`\n",
    "- Show the transitions between the states as a function of the noise-free input.\n",
    "- Subject the model to gaussian additive noise.\n",
    "- One of the later aims could also be to characterize the gradient. Comparing the empirical and the calculated values.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = f\"/Users/vikrantjaltare/OneDrive - UC San Diego/Datasets/\"\n",
    "FIGURES_PATH = f\"/Users/vikrantjaltare/OneDrive - UC San Diego/Figures/onr_figures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x33ae07b30>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## Try: generating random numbers\n",
    "key1 = jax.random.key(1)\n",
    "mu1 = 1.0\n",
    "std1 = 0.5\n",
    "zeta_arr1 = mu1 + jax.random.normal(key1, (1000,))*std1\n",
    "zeta_arr2 = mu1 + jax.random.normal(key1, (1000,))*std1*5\n",
    "plt.hist(zeta_arr1, alpha =.5, label = f\"std = {std1}\")\n",
    "plt.hist(zeta_arr2, alpha =.5, label=f\"std = {std1*5}\")\n",
    "\n",
    "plt.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "## create a neuron model\n",
    "@jax.jit\n",
    "def state_function(x, thresholds, std, key):\n",
    "    \"\"\"\n",
    "    Implements ternary state updates for vector inputs.\n",
    "    x: Input vector\n",
    "    thresholds: Two-element array of thresholds [theta1, theta2]\n",
    "    \"\"\"\n",
    "\n",
    "    theta1, theta2 = thresholds\n",
    "\n",
    "    noise = jax.random.normal(key, x.shape) * std\n",
    "    x = x + noise\n",
    "\n",
    "\n",
    "    # Apply element-wise thresholding using jax.numpy.where (which supports vectorization)\n",
    "    state = jnp.where(x < theta1, -1, \n",
    "                      jnp.where(x > theta2, 1, 0))\n",
    "\n",
    "    return state\n",
    "\n",
    "\n",
    "\n",
    "# @jax.jit\n",
    "# def activation(W, b, x):\n",
    "#     return jnp.dot(W, x) + b\n",
    "\n",
    "# @jax.jit\n",
    "# def generate_keys(init_key = 101, num_keys = 1000):\n",
    "#     key = jax.random.key(init_key)\n",
    "#     return jax.random.split(key, num = num_keys)\n",
    "\n",
    "# @jax.jit\n",
    "# def generate_gaussian_noise(key, mean, std, size):\n",
    "#     \"\"\"\n",
    "#     Generate random samples from a Gaussian distribution of a given size\n",
    "#     \"\"\"\n",
    "#     return mean + jax.random.normal(key, (size,)) * std\n",
    "\n",
    "@jax.jit\n",
    "def gaussian_erf(x, mu, sigma):\n",
    "    \"\"\"\n",
    "    Gaussian error function. \n",
    "    To compute survivor function simply use 1 - erf\n",
    "    \"\"\"\n",
    "    return 0.5 * (1 + jax.lax.erf((x - mu) / (sigma * jnp.sqrt(2))))\n",
    "\n",
    "@jax.jit\n",
    "def expected_state(y_tilde, threshold, std):\n",
    "    \"\"\"\"\n",
    "    Calculate the expected state.\n",
    "    Assume threshold to be a two element array\n",
    "    \"\"\"\n",
    "\n",
    "    # unpack the thresholds\n",
    "    theta1, theta2 = threshold\n",
    "\n",
    "    # calculate the expected state\n",
    "    E =  (1/(jnp.sqrt(2 * jnp.pi) * std)) * ((1 - gaussian_erf(x = theta2 - y_tilde, mu = 0, sigma = std)) - gaussian_erf(x = theta1 - y_tilde, mu = 0, sigma = std)) # # jax.nn.sigmoid(y_tilde)\n",
    "    return E\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.9785075 -1.3780545 -1.3985887]\n",
      "[ 0 -1 -1]\n"
     ]
    }
   ],
   "source": [
    "## testing state function\n",
    "W = jax.random.normal(key1, (3, 3))\n",
    "x = jax.random.normal(key1, (3,))\n",
    "print(jnp.dot(x, W.T))\n",
    "print(state_function(jnp.dot(x, W.T), [-1, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining state function with custom gradients: UNDER CONSTRUCTION\n",
    "@jax.custom_vjp\n",
    "def state_fn_with_custom_grad(x, threshold, std, key):\n",
    "    \"\"\"\n",
    "    Implements ternary state updates.\n",
    "    Uses state_function in the forward pass but overrides the gradient\n",
    "    with expected state in the backward pass\n",
    "    \"\"\"\n",
    "\n",
    "    return state_function(x, threshold, std, key)\n",
    "\n",
    "## define the forward pass\n",
    "def state_fwd(x, threshold, std, key):\n",
    "    return state_function(x, threshold, std, key), (x, threshold, std) # store x, threshold and std for the backward pass\n",
    "\n",
    "## define the backward pass\n",
    "def state_bwd(res, g):\n",
    "    # unpack the stored values\n",
    "    x, threshold, std = res\n",
    "\n",
    "    # use expected_state with passed std value\n",
    "    grad_x = expected_state(y_tilde = x, threshold = threshold, std = std)\n",
    "\n",
    "    return (grad_x * g, None, None) # None for threshold and std as they are not differentiable\n",
    "\n",
    "# Attach custom forward and backward to state_fn_with_custom_grad\n",
    "state_fn_with_custom_grad.defvjp(state_fwd, state_bwd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining the architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Helper function to initialize the weights \n",
    "def random_layer_params(input_size, output_size, key):\n",
    "    \"\"\"\n",
    "    Initialize the weights and biases for a single layer\n",
    "    \"\"\"\n",
    "    scale = jnp.sqrt(2/(input_size + output_size))\n",
    "    w_key, b_key = jax.random.split(key)\n",
    "    W = jax.random.normal(w_key, (output_size, input_size)) * scale\n",
    "    b = jax.random.normal(b_key, (output_size, )) * scale\n",
    "\n",
    "    return W, b\n",
    "\n",
    "## Initialize all the layers for a fully-connected neural network with sizes \"sizes\"\n",
    "def init_network_params(sizes, key):\n",
    "    \"\"\"\n",
    "    Initialize the weights and biases for all the layers\n",
    "    \"\"\"\n",
    "\n",
    "    keys = jax.random.split(key, len(sizes))\n",
    "    params = [random_layer_params(n_in, n_out, k) for n_in, n_out, k in zip(sizes[:-1], sizes[1:], keys)]\n",
    "    return params\n",
    "\n",
    "## Initialize Adam parameters\n",
    "def init_adam_params(params):\n",
    "    \"\"\"\n",
    "    Initialize the Adam parameters\n",
    "    \"\"\"\n",
    "\n",
    "    # 1st order term\n",
    "    m = [(jnp.zeros_like(w), jnp.zeros_like(b)) for w, b in params]\n",
    "    # 2nd order term\n",
    "    v = [(jnp.zeros_like(w), jnp.zeros_like(b)) for w, b in params]\n",
    "    # initialize time\n",
    "    t = 1\n",
    "\n",
    "    return m, v, t\n",
    "\n",
    "\n",
    "## Forward pass through the network\n",
    "def predict(params, image, thresholds, key, noise_sd):\n",
    "    \"\"\"\n",
    "    Forward pass through the network\n",
    "    \"\"\"\n",
    "\n",
    "    # generate a key\n",
    "    key = jax.random.key(key)\n",
    "\n",
    "    # Unpack the image\n",
    "    activations = image\n",
    "\n",
    "    # print(f\"Activations.shape inside predict {activations.shape}\")\n",
    "\n",
    "    for i, (W, b) in enumerate(params[:-1]): # iterate through all but final layer\n",
    "        \n",
    "        # split the key\n",
    "        key, noise_key = jax.random.split(key, 2)\n",
    "\n",
    "        # comput noise-free input\n",
    "        y_tilde = jnp.dot(activations, W.T) + b\n",
    "\n",
    "        # # compute the expected state\n",
    "        # activations = expected_state(y_tilde=y_tilde, threshold = thresholds, std = noise_sd)\n",
    "\n",
    "        # # add noise to y_tilde: y = y_tilde + noise\n",
    "        # noise = noise_sd * jax.random.normal(noise_key, activations.shape)\n",
    "        # outputs = y_tilde + noise\n",
    "\n",
    "        # Apply thresholding function here\n",
    "        activations = state_fn_with_custom_grad(y_tilde, thresholds, noise_sd, noise_key)\n",
    "\n",
    "\n",
    "        # activations = state_function(outputs, thresholds)\n",
    "        # print(activations)\n",
    "\n",
    "    # for final layer no thresholding!\n",
    "    out_W, out_b = params[-1]\n",
    "    logits = jnp.dot(activations, out_W.T) + out_b\n",
    "\n",
    "    return logits # this is because we are taking log_softmax in the loss calculation function\n",
    "\n",
    "## dealing with batches using vmap\n",
    "batched_predict = jax.vmap(predict, in_axes = (None, 0, None, None, None)) # params, inputs, thresholds, key, noise_sd\n",
    "\n",
    "## define a one hot encoding function\n",
    "def one_hot(x, k, dtype = jnp.float32):\n",
    "    \"\"\"\n",
    "    One hot encoding\n",
    "    \"\"\"\n",
    "    return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
    "\n",
    "## define an accuracy function\n",
    "def accuracy(params, images, targets, thresholds, key, noise_sd):\n",
    "    \"\"\"\n",
    "    Calculate the accuracy of the model\n",
    "    \"\"\"\n",
    "    target_class = jnp.argmax(targets, axis = 1)\n",
    "    predicted_class = jnp.argmax(batched_predict(params, images, thresholds, key, noise_sd), axis = 1)\n",
    "    mean_acc = jnp.mean(predicted_class == target_class)\n",
    "\n",
    "    return mean_acc\n",
    "\n",
    "# def loss(params, images, targets):\n",
    "#   preds = batched_predict(params, images)\n",
    "#   return -jnp.mean(preds * targets)\n",
    "\n",
    "def ce_loss(params, images, targets, thresholds, key, noise_sd):\n",
    "    \"\"\"\n",
    "    Calculate the cross entropy loss: Rewrite this to incorporate batch size!!\n",
    "    \"\"\"\n",
    "\n",
    "    # make predictions\n",
    "    logits = batched_predict(params, images, thresholds, key, noise_sd)\n",
    "\n",
    "    # compute the cross-entropy loss\n",
    "    log_softmax = jax.nn.log_softmax(logits)\n",
    "\n",
    "    # compute the cross-entropy loss\n",
    "    loss = -jnp.sum(log_softmax * targets)\n",
    "\n",
    "    # add L2 penmalty\n",
    "    l2_penalty = 0 #0.5 * sum(jnp.sum(jnp.square(w)) for w, _ in params)\n",
    "\n",
    "    return jnp.mean(loss) + 1e-4*l2_penalty\n",
    "\n",
    "## define two update functions: one for SGD and one for Adam\n",
    "\n",
    "@jax.jit\n",
    "def update_with_sgd(params, x, y, thresholds, key, noise_sd, lr):\n",
    "    \"\"\"\n",
    "    Update parameters usinf SGD\n",
    "    \"\"\"\n",
    "    \n",
    "    # partially apply the non-differentiable arguments to ce_loss\n",
    "    loss_fn = partial(ce_loss, thresholds = thresholds, key = key, noise_sd = noise_sd)\n",
    "\n",
    "    # comput the gradients w.r.t. the parameters\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "\n",
    "    # update the parameters\n",
    "    return [(w - lr * dw, b - lr * db) for (w, b), (dw, db) in zip(params, grads)]\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def update_with_adam(params, x, y, thresholds, key, noise_sd, optim_state, lr_adam = 1e-3, beta1 = 0.9, beta2 = 0.999, epsi = 1e-8):\n",
    "    \"\"\"\n",
    "    Update the parameters using Adam\n",
    "    \"\"\"\n",
    "\n",
    "    # partially apply the non-differentiable arguments to ce_loss\n",
    "    loss_fn = partial(ce_loss, thresholds = thresholds, key = key, noise_sd = noise_sd)\n",
    "\n",
    "    # comput the gradients w.r.t. the parameters\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "\n",
    "    # extract parameters from the optimizer state\n",
    "    m, v, t = optim_state # for JAX: storing internal states explicitly is requried\n",
    "\n",
    "    new_params = []\n",
    "    new_m = []\n",
    "    new_v = []\n",
    "\n",
    "    for (w, b), (dw, db), (m_w, m_b), (v_w, v_b) in zip(params, grads, m, v):\n",
    "        # update the first order moment\n",
    "        m_w = beta1 * m_w + (1 - beta1) * dw\n",
    "        m_b = beta1 * m_b + (1 - beta1) * db\n",
    "\n",
    "        # update the second order moments\n",
    "        v_w = beta2 * v_w + (1 - beta2) * dw**2\n",
    "        v_b = beta2 * v_b + (1 - beta2) * db**2\n",
    "\n",
    "        # compute the bias correct moment estimates\n",
    "        m_w_hat = m_w/(1 - beta1**t)\n",
    "        m_b_hat = m_b/(1 - beta1**t)\n",
    "        v_w_hat = v_w/(1 - beta2**t)\n",
    "        v_b_hat = v_b/(1 - beta2**t)\n",
    "\n",
    "        # update the parameters\n",
    "        w_new = w - lr_adam * m_w_hat/(jnp.sqrt(v_w_hat) + epsi)\n",
    "        b_new = b - lr_adam * m_b_hat/(jnp.sqrt(v_b_hat) + epsi)\n",
    "\n",
    "        # append updated params \n",
    "        new_params.append((w_new, b_new))\n",
    "        new_m.append((m_w, m_b))\n",
    "        new_v.append((v_w, v_b))\n",
    "\n",
    "\n",
    "\n",
    "    # increment time\n",
    "    t = t + 1\n",
    "\n",
    "    # return updated parameters and updated Adam states\n",
    "\n",
    "    return new_params, (new_m, new_v, t)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## define a param update function\n",
    "@jax.jit\n",
    "def update(params, x, y, thresholds, key, noise_sd, lr, opt_state, optimizer = 0):\n",
    "    \"\"\"\n",
    "    Update the model parameters.\n",
    "    Update the function to incorporate Adam.\n",
    "    As strings are not JAX compatible, we will use 0 for SGD and 1 for Adam\n",
    "    \"\"\"\n",
    "\n",
    "    # partially apply the non-differentiable arguments to ce_loss\n",
    "    loss_fn = partial(ce_loss, thresholds = thresholds, key = key, noise_sd = noise_sd)\n",
    "\n",
    "    # comput the gradients w.r.t. the parameters\n",
    "    grads = jax.grad(loss_fn)(params, x, y)\n",
    "\n",
    "    def sgd_update(args):\n",
    "        params, grads, lr, _ = args\n",
    "        new_params = [(w - lr * dw, b - lr * db) for (w, b), (dw, db) in zip(params, grads)]\n",
    "        return new_params, None\n",
    "    \n",
    "    def adam_update(args):\n",
    "        params, grads, opt_state, lr = args\n",
    "        # declare the hypterparameters as recommended in the original paper\n",
    "        [beta1, beta2, epsi, lr_adam] = [0.9, 0.999, 1e-8, 1e-3]\n",
    "        m, v, t = opt_state # for JAX: storing internal states explicitly is requried\n",
    "        # extract parameters from the optimizer state\n",
    "        m, v, t = opt_state # for JAX: storing internal states explicitly is requried\n",
    "\n",
    "        new_params = []\n",
    "        new_m = []\n",
    "        new_v = []\n",
    "\n",
    "        for (w, b), (dw, db), (m_w, m_b), (v_w, v_b) in zip(params, grads, m, v):\n",
    "            # update the first order moment\n",
    "            m_w = beta1 * m_w + (1 - beta1) * dw\n",
    "            m_b = beta1 * m_b + (1 - beta1) * db\n",
    "\n",
    "            # update the second order moments\n",
    "            v_w = beta2 * v_w + (1 - beta2) * dw**2\n",
    "            v_b = beta2 * v_b + (1 - beta2) * db**2\n",
    "\n",
    "            # compute the bias correct moment estimates\n",
    "            m_w_hat = m_w/(1 - beta1**t)\n",
    "            m_b_hat = m_b/(1 - beta1**t)\n",
    "            v_w_hat = v_w/(1 - beta2**t)\n",
    "            v_b_hat = v_b/(1 - beta2**t)\n",
    "\n",
    "            # update the parameters\n",
    "            w_new = w - lr_adam * m_w_hat/(jnp.sqrt(v_w_hat) + epsi)\n",
    "            b_new = b - lr_adam * m_b_hat/(jnp.sqrt(v_b_hat) + epsi)\n",
    "\n",
    "            # append updated params \n",
    "            new_params.append((w_new, b_new))\n",
    "            new_m.append((m_w, m_b))\n",
    "            new_v.append((v_w, v_b))\n",
    "\n",
    "\n",
    "\n",
    "        # increment time\n",
    "        t = t + 1\n",
    "\n",
    "        # return updated parameters and updated Adam states\n",
    "\n",
    "        return new_params, (new_m, new_v, t)\n",
    "    \n",
    "    return jax.lax.cond(\n",
    "        optimizer == 0,\n",
    "        sgd_update, \n",
    "        adam_update,\n",
    "        (params, grads, lr), \n",
    "        (params, grads, opt_state, lr)\n",
    "    )\n",
    "\n",
    "\n",
    "    # # SGD\n",
    "    # if optimizer == 0:\n",
    "    #     return [(w - lr * dw, b - lr * db) for (w, b), (dw, db) in zip(params, grads)]\n",
    "    \n",
    "    # elif optimizer == 1:\n",
    "    #     ## TODO\n",
    "\n",
    "        \n",
    "    #     [beta1, beta2, epsi, lr_adam] = [0.9, 0.999, 1e-8, 1e-3]\n",
    "\n",
    "    #     # extract parameters from the optimizer state\n",
    "    #     m, v, t = opt_state # for JAX: storing internal states explicitly is requried\n",
    "\n",
    "    #     new_params = []\n",
    "    #     new_m = []\n",
    "    #     new_v = []\n",
    "\n",
    "    #     for (w, b), (dw, db), (m_w, m_b), (v_w, v_b) in zip(params, grads, m, v):\n",
    "    #         # update the first order moment\n",
    "    #         m_w = beta1 * m_w + (1 - beta1) * dw\n",
    "    #         m_b = beta1 * m_b + (1 - beta1) * db\n",
    "\n",
    "    #         # update the second order moments\n",
    "    #         v_w = beta2 * v_w + (1 - beta2) * dw**2\n",
    "    #         v_b = beta2 * v_b + (1 - beta2) * db**2\n",
    "\n",
    "    #         # compute the bias correct moment estimates\n",
    "    #         m_w_hat = m_w/(1 - beta1**t)\n",
    "    #         m_b_hat = m_b/(1 - beta1**t)\n",
    "    #         v_w_hat = v_w/(1 - beta2**t)\n",
    "    #         v_b_hat = v_b/(1 - beta2**t)\n",
    "\n",
    "    #         # update the parameters\n",
    "    #         w_new = w - lr_adam * m_w_hat/(jnp.sqrt(v_w_hat) + epsi)\n",
    "    #         b_new = b - lr_adam * m_b_hat/(jnp.sqrt(v_b_hat) + epsi)\n",
    "\n",
    "    #         # append updated params \n",
    "    #         new_params.append((w_new, b_new))\n",
    "    #         new_m.append((m_w, m_b))\n",
    "    #         new_v.append((v_w, v_b))\n",
    "\n",
    "\n",
    "\n",
    "    #     # increment time\n",
    "    #     t = t + 1\n",
    "\n",
    "    #     # return updated parameters and updated Adam states\n",
    "\n",
    "    #     return new_params, (new_m, new_v, t)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load MNIST using tensorflow\n",
    "temp_data_dir = DATA_PATH\n",
    "# Ensure TF does not see GPU and grab all GPU memory.\n",
    "tf.config.set_visible_devices([], device_type='GPU')\n",
    "\n",
    "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=temp_data_dir, with_info=True)\n",
    "mnist_data = tfds.as_numpy(mnist_data)\n",
    "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
    "num_labels = info.features['label'].num_classes\n",
    "h, w, c = info.features['image'].shape\n",
    "num_pixels = h * w * c\n",
    "\n",
    "# Full train set\n",
    "train_images, train_labels = train_data['image'], train_data['label']\n",
    "train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
    "train_labels = one_hot(train_labels, num_labels)\n",
    "\n",
    "# Full test set\n",
    "test_images, test_labels = test_data['image'], test_data['label']\n",
    "test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
    "test_labels = one_hot(test_labels, num_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (60000, 784) (60000, 10)\n",
      "Test: (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "print('Train:', train_images.shape, train_labels.shape)\n",
    "print('Test:', test_images.shape, test_labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "## initializing the network: in the future this should be a dictionary\n",
    "threshold = [-2, 1]\n",
    "layer_size = [784, 2098, 10]\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "net_params = init_network_params(layer_size, jax.random.key(0))\n",
    "\n",
    "opti_dict = {\n",
    "    'SGD': 0,\n",
    "    'Adam': 1\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "# batched_predictions = batched_predict(net_params, flattened_images)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c48f0674b80b4620ab1a20a340510e14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noise SD sim: 10\n",
      "std Traced<ShapedArray(int32[], weak_type=True)>with<DynamicJaxprTrace(level=2/1)>, noise = Traced<ShapedArray(float32[3])>with<DynamicJaxprTrace(level=2/1)>\n",
      "Training set accuracy 0.09266667068004608\n",
      "Test set accuracy 0.09129999577999115\n",
      "Noise SD sim: 10\n",
      "Training set accuracy 0.9419000148773193\n",
      "Test set accuracy 0.9341999888420105\n",
      "Noise SD sim: 10\n",
      "Training set accuracy 0.9601333737373352\n",
      "Test set accuracy 0.9431999921798706\n",
      "Noise SD sim: 10\n",
      "Training set accuracy 0.9681666493415833\n",
      "Test set accuracy 0.9456999897956848\n",
      "Noise SD sim: 10\n",
      "Training set accuracy 0.9728000164031982\n",
      "Test set accuracy 0.946899950504303\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def get_train_batches():\n",
    "  \n",
    "  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
    "  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir = DATA_PATH)\n",
    "  # You can build up an arbitrary tf.data input pipeline\n",
    "  ds = ds.batch(batch_size).prefetch(1)\n",
    "  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
    "  return tfds.as_numpy(ds)\n",
    "\n",
    "train_acc_list = []\n",
    "test_acc_list = []\n",
    "noise_sd_sim = 10\n",
    "noise_sd_inf = 10\n",
    "lr_sim = 1e-4\n",
    "(m, v, t) = init_adam_params(net_params)\n",
    "opti = 1\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "  # Compute accuracy at the start of the epoch\n",
    "  print(f\"Noise SD sim: {noise_sd_sim}\")\n",
    "  train_acc = accuracy(params=net_params, images=train_images, targets=train_labels, thresholds = threshold, key = 0, noise_sd = noise_sd_inf)\n",
    "  test_acc = accuracy(params=net_params, images=test_images, targets=test_labels, thresholds = threshold, key = 0, noise_sd = noise_sd_inf)\n",
    "  train_acc_list.append(train_acc)\n",
    "  test_acc_list.append(test_acc)\n",
    "  print(f\"Training set accuracy {train_acc}\")\n",
    "  print(f\"Test set accuracy {test_acc}\")\n",
    "\n",
    "  for x, y in get_train_batches():\n",
    "    x = jnp.reshape(x, (len(x), num_pixels))\n",
    "    # print(x.shape)\n",
    "    # break\n",
    "    y = one_hot(y, num_labels)\n",
    "    net_params, (m, v, t) = update_with_adam(net_params, x ,y, threshold, key = 0, noise_sd = noise_sd_sim, optim_state = (m, v, t))\n",
    "    # if opti == 0:\n",
    "    #   net_params, _ = update(params = net_params, x = x, y = y, thresholds = threshold, key = 0, noise_sd = noise_sd_sim, lr = lr_sim, optimizer = 0, opt_state = None)\n",
    "    # else:\n",
    "    #   net_params, (m, v, t) = update(params = net_params, x = x, y = y, thresholds = threshold, key = 0, noise_sd = noise_sd_sim, lr = lr_sim, optimizer = 1, opt_state = (m, v, t))\n",
    "\n",
    "#   train_acc = accuracy(params=net_params, images=train_images, targets=train_labels, thresholds = threshold, key = 0, noise_sd = noise_sd_inf)\n",
    "#   test_acc = accuracy(params=net_params, images=test_images, targets=test_labels, thresholds = threshold, key = 0, noise_sd = noise_sd_inf)\n",
    "#   train_acc_list.append(train_acc)\n",
    "#   test_acc_list.append(test_acc)\n",
    "#   print(\"Training set accuracy {}\".format(train_acc))\n",
    "#   print(\"Test set accuracy {}\".format(test_acc))\n",
    "\n",
    "#   # annealing??\n",
    "  # noise_sd_sim = noise_sd_sim * 0.7\n",
    "  # noise_sd_inf = noise_sd_inf * 0.7\n",
    "\n",
    "\n",
    "## Plotting the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(train_acc_list, label = \"Training accuracy\")\n",
    "ax.plot(test_acc_list, label = \"Test accuracy\")\n",
    "ax.set_xlabel(\"Epochs\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend()\n",
    "ax.set_xlim([-0.5, num_epochs])\n",
    "ax.set_ylim([0, 1])\n",
    "\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the accuracy as a function of noise standard deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[9.9999997e-06 5.0118757e-05 2.5118855e-04 1.2589259e-03 6.3095726e-03\n",
      " 3.1622775e-02 1.5848938e-01 7.9432833e-01 3.9810724e+00 1.9952631e+01\n",
      " 1.0000000e+02]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bc3b40f79774b73b4545a6f80233cb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running simulation for noise_sd = 9.999999747378752e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-24 22:49:06.881812: W tensorflow/core/kernels/data/cache_dataset_ops.cc:913] The calling iterator did not fully read the dataset being cached. In order to avoid unexpected truncation of the dataset, the partially cached contents of the dataset  will be discarded. This can happen if you have an input pipeline similar to `dataset.cache().take(k).repeat()`. You should use `dataset.take(k).cache().repeat()` instead.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "add got incompatible shapes for broadcasting: (2098, 784), (2048, 784).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[129], line 37\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;66;03m# break\u001b[39;00m\n\u001b[1;32m     36\u001b[0m         y \u001b[38;5;241m=\u001b[39m one_hot(y, num_labels)\n\u001b[0;32m---> 37\u001b[0m         net_params \u001b[38;5;241m=\u001b[39m \u001b[43mupdate_with_adam\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet_params\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreshold\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnoise_sd\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mnoise_sd_sim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptim_state\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m final_train_acc\u001b[38;5;241m.\u001b[39mappend(train_acc_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m     40\u001b[0m final_test_acc\u001b[38;5;241m.\u001b[39mappend(test_acc_list[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "Cell \u001b[0;32mIn[128], line 164\u001b[0m, in \u001b[0;36mupdate_with_adam\u001b[0;34m(params, x, y, thresholds, key, noise_sd, optim_state, lr_adam, beta1, beta2, epsi)\u001b[0m\n\u001b[1;32m    160\u001b[0m new_v \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m    162\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (w, b), (dw, db), (m_w, m_b), (v_w, v_b) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(params, grads, m, v):\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# update the first order moment\u001b[39;00m\n\u001b[0;32m--> 164\u001b[0m     m_w \u001b[38;5;241m=\u001b[39m \u001b[43mbeta1\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mm_w\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdw\u001b[49m\n\u001b[1;32m    165\u001b[0m     m_b \u001b[38;5;241m=\u001b[39m beta1 \u001b[38;5;241m*\u001b[39m m_b \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m-\u001b[39m beta1) \u001b[38;5;241m*\u001b[39m db\n\u001b[1;32m    167\u001b[0m     \u001b[38;5;66;03m# update the second order moments\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_env/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:747\u001b[0m, in \u001b[0;36m_forward_operator_to_aval.<locals>.op\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mop\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs):\n\u001b[0;32m--> 747\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43maval\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mname\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_env/lib/python3.12/site-packages/jax/_src/numpy/array_methods.py:272\u001b[0m, in \u001b[0;36m_defer_to_unrecognized_arg.<locals>.deferring_binary_op\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    270\u001b[0m args \u001b[38;5;241m=\u001b[39m (other, \u001b[38;5;28mself\u001b[39m) \u001b[38;5;28;01mif\u001b[39;00m swap \u001b[38;5;28;01melse\u001b[39;00m (\u001b[38;5;28mself\u001b[39m, other)\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(other, _accepted_binop_types):\n\u001b[0;32m--> 272\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbinary_op\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    273\u001b[0m \u001b[38;5;66;03m# Note: don't use isinstance here, because we don't want to raise for\u001b[39;00m\n\u001b[1;32m    274\u001b[0m \u001b[38;5;66;03m# subclasses, e.g. NamedTuple objects that may override operators.\u001b[39;00m\n\u001b[1;32m    275\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(other) \u001b[38;5;129;01min\u001b[39;00m _rejected_binop_types:\n",
      "    \u001b[0;31m[... skipping hidden 11 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_env/lib/python3.12/site-packages/jax/_src/numpy/ufuncs.py:206\u001b[0m, in \u001b[0;36madd\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[38;5;129m@implements\u001b[39m(np\u001b[38;5;241m.\u001b[39madd, module\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    203\u001b[0m \u001b[38;5;129m@partial\u001b[39m(jit, inline\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    204\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd\u001b[39m(x: ArrayLike, y: ArrayLike, \u001b[38;5;241m/\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Array:\n\u001b[1;32m    205\u001b[0m   x, y \u001b[38;5;241m=\u001b[39m promote_args(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124madd\u001b[39m\u001b[38;5;124m\"\u001b[39m, x, y)\n\u001b[0;32m--> 206\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlax\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m x\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mbool\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m lax\u001b[38;5;241m.\u001b[39mbitwise_or(x, y)\n",
      "    \u001b[0;31m[... skipping hidden 7 frame]\u001b[0m\n",
      "File \u001b[0;32m~/miniconda3/envs/jax_env/lib/python3.12/site-packages/jax/_src/lax/lax.py:1725\u001b[0m, in \u001b[0;36mbroadcasting_shape_rule\u001b[0;34m(name, *avals)\u001b[0m\n\u001b[1;32m   1723\u001b[0m       result_shape\u001b[38;5;241m.\u001b[39mappend(non_1s[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m   1724\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1725\u001b[0m       \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m got incompatible shapes for broadcasting: \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1726\u001b[0m                       \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mstr\u001b[39m,\u001b[38;5;250m \u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mtuple\u001b[39m,\u001b[38;5;250m \u001b[39mshapes)))\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m   1728\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mtuple\u001b[39m(result_shape)\n",
      "\u001b[0;31mTypeError\u001b[0m: add got incompatible shapes for broadcasting: (2098, 784), (2048, 784)."
     ]
    }
   ],
   "source": [
    "## initialize noise sd to sweep\n",
    "noise_sd_arr = jnp.logspace(-5, 2, 11)\n",
    "print(noise_sd_arr)\n",
    "\n",
    "## initializing the network: in the future this should be a dictionary\n",
    "threshold = [-1, 1]\n",
    "layer_size = [784, 2048, 10]\n",
    "num_epochs = 5\n",
    "batch_size = 128\n",
    "n_targets = 10\n",
    "\n",
    "## initialize the finaly accuracies\n",
    "final_train_acc = []\n",
    "final_test_acc = []\n",
    "\n",
    "for noise_sd in tqdm(noise_sd_arr):\n",
    "    print(f\"Running simulation for noise_sd = {noise_sd}\")\n",
    "\n",
    "    # initialize the weights\n",
    "    net_params = init_network_params(layer_size, jax.random.key(0))\n",
    "\n",
    "    train_acc_list = []\n",
    "    test_acc_list = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        # Compute accuracy at the start of the epoch\n",
    "        train_acc = accuracy(params=net_params, images=train_images, targets=train_labels, thresholds = threshold, key = 0, noise_sd = noise_sd)\n",
    "        test_acc = accuracy(params=net_params, images=test_images, targets=test_labels, thresholds = threshold, key = 0, noise_sd = noise_sd)\n",
    "        train_acc_list.append(train_acc)\n",
    "        test_acc_list.append(test_acc)\n",
    "\n",
    "        for x, y in get_train_batches():\n",
    "            x = jnp.reshape(x, (len(x), num_pixels))\n",
    "            # print(x.shape)\n",
    "            # break\n",
    "            y = one_hot(y, num_labels)\n",
    "            net_params = update_with_adam(net_params, x ,y, threshold, key = 0, noise_sd = noise_sd_sim, optim_state = (m, v, t))\n",
    "\n",
    "    final_train_acc.append(train_acc_list[-1])\n",
    "    final_test_acc.append(test_acc_list[-1])\n",
    "\n",
    "\n",
    "## Plotting the results\n",
    "fig, ax = plt.subplots()\n",
    "ax.semilogx(noise_sd_arr, final_train_acc, label = \"Training accuracy\")\n",
    "ax.semilogx(noise_sd_arr, final_test_acc, label = \"Test accuracy\")\n",
    "ax.set_xlabel(\"Noise SD\")\n",
    "ax.set_ylabel(\"Accuracy\")\n",
    "ax.legend()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x, y in get_train_batches():\n",
    "#     x = jnp.reshape(x, (len(x), num_pixels))  # Flatten input to (batch_size, 784)\n",
    "#     y = one_hot(y, num_labels)\n",
    "\n",
    "#     # Check the shape before applying batched_predict\n",
    "#     print(f\"Before batched_predict: {x.shape}\")  # Should be (batch_size, 784)\n",
    "\n",
    "#     logits = batched_predict(net_params, x, threshold, 0, 0.0001)\n",
    "\n",
    "#     # Check the shape after batched_predict\n",
    "#     print(f\"After batched_predict: {logits.shape}\")\n",
    "    \n",
    "#     net_params = update(params=net_params, x=x, y=y, thresholds=threshold, key=0, noise_sd=0.1, lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.imshow(net_params[1][0], cmap = \"viridis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## load MNIST using Pytorch\n",
    "# def jax_transform(x):\n",
    "#     \"\"\"Flatten and convert to JAX array.\"\"\"\n",
    "#     return jnp.ravel(jnp.array(x, dtype=jnp.float32)) / 255.0  # Normalize to [0, 1]\n",
    "\n",
    "# ## Custom collate function for batching\n",
    "# def jax_custom_collate(batch):\n",
    "#     \"\"\"Collate function to prepare data and labels for JAX.\"\"\"\n",
    "#     transposed_data = list(zip(*batch))\n",
    "    \n",
    "#     # Labels as JAX arrays (int32)\n",
    "#     labels = jnp.array(transposed_data[1], dtype=jnp.int32)\n",
    "    \n",
    "#     # Data as stacked JAX arrays\n",
    "#     data = jnp.stack(transposed_data[0], axis=0)\n",
    "    \n",
    "#     return data, labels\n",
    "\n",
    "# # ## Data loading parameters\n",
    "# # data_params = {\n",
    "# #     \"batch_size\": 128,\n",
    "# #     \"num_workers\": 0,\n",
    "# #     \"pin_memory\": False,\n",
    "# #     \"shuffle\": True\n",
    "# # }\n",
    "\n",
    "# ## Load MNIST datasets using PyTorch's torchvision\n",
    "# train_dataset = datasets.MNIST(root=DATA_PATH, train=True, download=False, transform=jax_transform)\n",
    "# test_dataset = datasets.MNIST(root=DATA_PATH, train=False, download=False, transform=jax_transform)\n",
    "\n",
    "# ## Create DataLoader for training and test datasets\n",
    "# train_loader = DataLoader(train_dataset, collate_fn=jax_custom_collate, drop_last=True, batch_size=batch_size)\n",
    "# test_loader = DataLoader(test_dataset, collate_fn=jax_custom_collate, drop_last=True, batch_size = batch_size)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set images shape: (128, 784)\n",
      "Train set labels shape: (128,)\n",
      "Test set images shape: (128, 784)\n",
      "Test set labels shape: (128,)\n",
      "Total number of training samples: 60000\n",
      "Total number of testing samples: 10000\n"
     ]
    }
   ],
   "source": [
    "# # Get the first batch from the training loader and print the shapes\n",
    "# train_images, train_labels = next(iter(train_loader))\n",
    "# print(f\"Train set images shape: {train_images.shape}\")\n",
    "# print(f\"Train set labels shape: {train_labels.shape}\")\n",
    "\n",
    "# # Get the first batch from the test loader and print the shapes\n",
    "# test_images, test_labels = next(iter(test_loader))\n",
    "# print(f\"Test set images shape: {test_images.shape}\")\n",
    "# print(f\"Test set labels shape: {test_labels.shape}\")\n",
    "\n",
    "# print(f\"Total number of training samples: {len(train_dataset)}\")\n",
    "# print(f\"Total number of testing samples: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## training loop\n",
    "# num_pixels = 784\n",
    "# num_labels = 10\n",
    "# key_ = jax.random.key(0)\n",
    "# test_accuracy = []\n",
    "# train_accuracy = []\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "\n",
    "#     for x_batch, y_batch in train_loader:\n",
    "#         # Reshape the input images to (batch_size, num_pixels)\n",
    "#         x_batch = jnp.reshape(x_batch, (len(x_batch), num_pixels))\n",
    "        \n",
    "#         # One-hot encode the labels\n",
    "#         y_batch = one_hot(y_batch, num_labels)\n",
    "        \n",
    "#         # Update parameters using the training batch\n",
    "#         net_params = update(net_params, x_batch, y_batch, thresholds = threshold, key = key_, noise_sd = 0.01, lr = 1e-3)\n",
    "\n",
    "#     # Calculate accuracy on the training set\n",
    "#     train_images, train_labels = next(iter(train_loader))\n",
    "#     train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
    "#     train_labels = one_hot(train_labels, num_labels)\n",
    "#     train_acc = accuracy(net_params, train_images, train_labels)\n",
    "#     train_accuracy.append(train_acc)\n",
    "\n",
    "#     # Calculate accuracy on the test set\n",
    "#     test_images, test_labels = next(iter(test_loader))\n",
    "#     test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
    "#     test_labels = one_hot(test_labels, num_labels)\n",
    "#     test_acc = accuracy(net_params, test_images, test_labels)\n",
    "\n",
    "#     print(f\"Training set accuracy {train_acc}\")\n",
    "#     print(f\"Test set accuracy {test_acc}\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jax_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
